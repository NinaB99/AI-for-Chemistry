{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### total import statements\n",
    "! pip install chemprop\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "! pip install pytorch-lightning wandb rdkit ogb deepchem\n",
    "import torch\n",
    "VERSION = torch.__version__\n",
    "! pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-{VERSION}.html\n",
    "! pip install torch-geometric\n",
    "! mkdir data/\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "IPythonConsole.ipython_useSVG = True  # < use SVGs instead of PNGs\n",
    "IPythonConsole.drawOptions.addAtomIndices = True  # adding indices for atoms\n",
    "IPythonConsole.drawOptions.addBondIndices = False  # not adding indices for bonds\n",
    "IPythonConsole.molSize = 200, 200\n",
    "# Random Seeds and Reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "! pip install deepchem.data\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import GRU\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import NNConv, MLP, global_add_pool\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    download_url,\n",
    ")\n",
    "from ogb.utils import smiles2graph\n",
    "from deepchem.splits import RandomSplitter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_add_pool\n",
    "from deepchem.feat import RDKitDescriptors\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import wandb\n",
    "! pip install mordred\n",
    "from mordred import Calculator, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing data and normalising target\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/NinaB99/AI-for-Chemistry/main/Data/Bioavailibility.csv')\n",
    "smiles = df['SMILES'].values.tolist()\n",
    "category = df['Category'].values.tolist()\n",
    "y = df['logK(%F)'].values\n",
    "# Here, we removed all zero-variance features, i.e. features that have the same value in all samples.\n",
    "\n",
    "splitter = RandomSplitter()\n",
    "\n",
    "# Normalize target to mean = 0 and std = 1.\n",
    "mean = y.mean()\n",
    "std = y.std()\n",
    "y = (y - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using the Mordred calculator and preprocessing the data\n",
    "calc = Calculator(descriptors, ignore_3D=True)\n",
    "#extracting the SMILES from the imported data and calculating\n",
    "mols = [MolFromSmiles(smi) for smi in smiles]\n",
    "Mordred_features_df =  calc.pandas(mols)\n",
    "Mordred_features = Mordred_features_df.to_numpy()\n",
    "# print(type(Mordred_features)) # use for debugging\n",
    "Mordred_features = Mordred_features.astype(float)\n",
    "\n",
    "# removing invalid values\n",
    "Mordred_features = Mordred_features[:, ~pd.isnull(Mordred_features).any(axis=0)]\n",
    "print(f\"Number of molecular descriptors without invalid values: {Mordred_features.shape[1]}\")\n",
    "\n",
    "#Removing zero variance features\n",
    "selector = VarianceThreshold(threshold=0.0)\n",
    "Mordred_features = selector.fit_transform(Mordred_features)\n",
    "print(f\"Number of molecular descriptors after removing zero-variance features: {Mordred_features.shape[1]}\")\n",
    "print(Mordred_features[0,:])\n",
    "# convert features to tensor for processing\n",
    "Mordred_molecular_features = torch.tensor(Mordred_features, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### splitting and preprocessing\n",
    "X = Mordred_features\n",
    "# training data size : test data size = 0.8 : 0.2\n",
    "# fixed seed using the random_state parameter, so it always has the same split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, random_state=0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# save original X\n",
    "X_train_ori = X_train\n",
    "X_test_ori = X_test\n",
    "# transform data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### coding the model training\n",
    "def train_test_model(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Function that trains a model, and tests it.\n",
    "    Inputs: sklearn model, train_data, test_data\n",
    "    \"\"\"\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate RMSE on training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    model_train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    model_test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    model_train_rmse = model_train_mse ** 0.5\n",
    "    model_test_rmse = model_test_mse ** 0.5\n",
    "    print(f\"RMSE on train set: {model_train_rmse:.3f}, and test set: {model_test_rmse:.3f}.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### optimising the Random Forest and XGBoost model using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [85,90,95,100,150,200],\n",
    "    'max_depth': [5,9,13,17,21,25,29,33,37,41],\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# use 5-folds cross validation during grid searching\n",
    "grid_search_RF = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=0),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    # will print out how long each step takes\n",
    "    verbose = 2\n",
    ")\n",
    "grid_search_RF.fit(X_train, y_train)\n",
    "\n",
    "# re-train a model using best hyperparameters\n",
    "rf_gs = RandomForestRegressor(**grid_search_RF.best_params_, random_state=0)\n",
    "\n",
    "print('Best paramters(Random Forest): ', grid_search_RF.best_params_)\n",
    "print('Random forests performance after hyperparamter optimization:')\n",
    "train_test_model(rf_gs, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# use 5-folds cross validation during grid searching\n",
    "grid_search_XBG= GridSearchCV(\n",
    "    XGBRegressor(random_state=0),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    # will print out how long each step takes\n",
    "    verbose = 2\n",
    ")\n",
    "grid_search_XBG.fit(X_train, y_train)\n",
    "\n",
    "# re-train a model using best hyperparameters\n",
    "xgb_gs = XGBRegressor(**grid_search_XBG.best_params_, random_state=0)\n",
    "\n",
    "print('Best paramters(XGBoost): ', grid_search_XBG.best_params_)\n",
    "print('XGBoost performance after hyperparamter optimization:')\n",
    "train_test_model(xgb_gs, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating the models\n",
    "RF_predictions = rf_gs.predict(Mordred_molecular_features)\n",
    "\n",
    "XGB_predictions = xgb_gs.predict(Mordred_molecular_features)\n",
    "\n",
    "outputs_df = pd.DataFrame([RF_predictions,\n",
    "                          XGB_predictions,\n",
    "                          category])\n",
    "outputs_df = outputs_df.transpose()\n",
    "outputs_df = outputs_df.reset_index(drop=True)\n",
    "\n",
    "# count correct predictions\n",
    "count_total = len(outputs_df[0])\n",
    "\n",
    "# comparing predictions with real values\n",
    "RF_count_correct = 0\n",
    "for i in range(count_total):\n",
    "    if outputs_df[0][i] > 0.5 and outputs_df[2][i] == 1:\n",
    "        RF_count_correct += 1\n",
    "    elif outputs_df[0][i] < 0.5 and outputs_df[2][i] == 0:\n",
    "        RF_count_correct += 1\n",
    "print(RF_count_correct/count_total)\n",
    "\n",
    "XGB_count_correct = 0\n",
    "for i in range(count_total):\n",
    "    if outputs_df[1][i] > 0.5 and outputs_df[2][i] == 1:\n",
    "        XGB_count_correct += 1\n",
    "    elif outputs_df[1][i] < 0.5 and outputs_df[2][i] == 0:\n",
    "        XGB_count_correct += 1\n",
    "print(XGB_count_correct/count_total)\n",
    "\n",
    "# print(outputs_df) # sometimes useful\n",
    "\n",
    "# Calculating the True/False positives and negatives\n",
    "RF_TP = ((outputs_df[0] > 0.5) & (outputs_df[2] == 1)).sum()\n",
    "RF_FN = ((outputs_df[0] < 0.5) & (outputs_df[2] == 1)).sum()\n",
    "RF_TN = ((outputs_df[0] < 0.5) & (outputs_df[2] == 0)).sum()\n",
    "RF_FP = ((outputs_df[0] > 0.5) & (outputs_df[2] == 0)).sum()\n",
    "\n",
    "# evaluating the success of the model\n",
    "RF_sensitivity = RF_TP/(RF_TP+RF_FN)\n",
    "print(f\"Sensitivity of random forest model: {RF_sensitivity}\")\n",
    "\n",
    "RF_specificity = RF_TN/(RF_TN+RF_FP)\n",
    "print(f\"Specificity of random forest model: {RF_specificity}\")\n",
    "\n",
    "RF_CCR = ((RF_sensitivity+RF_specificity)/2)*100\n",
    "print(f\"Correct classfication rate of random forest model: {RF_CCR}\")\n",
    "\n",
    "# Calculating the True/False positives and negatives\n",
    "XGB_TP = ((outputs_df[1] > 0.5) & (outputs_df[2] == 1)).sum()\n",
    "XGB_FN = ((outputs_df[1] < 0.5) & (outputs_df[2] == 1)).sum()\n",
    "XGB_TN = ((outputs_df[1] < 0.5) & (outputs_df[2] == 0)).sum()\n",
    "XGB_FP = ((outputs_df[1] > 0.5) & (outputs_df[2] == 0)).sum()\n",
    "\n",
    "# evaluating the succes of the model\n",
    "XGB_sensitivity = XGB_TP/(XGB_TP+XGB_FN)\n",
    "print(f\"Sensitivity of XG Boost model: {XGB_sensitivity}\")\n",
    "\n",
    "XGB_specificity = XGB_TN/(XGB_TN+XGB_FP)\n",
    "print(f\"Specificity of XG Boost model: {XGB_specificity}\")\n",
    "\n",
    "XGB_CCR = ((XGB_sensitivity+XGB_specificity)/2)*100\n",
    "print(f\"Correct classfication rate of XG Boost model: {XGB_CCR}\")\n",
    "\n",
    "print(outputs_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
