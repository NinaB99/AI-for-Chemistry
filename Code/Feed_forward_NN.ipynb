{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article from Philippe:\n",
    "https://practicalcheminformatics.blogspot.com/2023/06/getting-real-with-molecular-property.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for running in colab:\n",
    "\n",
    "# !pip install rdkit\n",
    "# !pip install deepchem\n",
    "\n",
    "\n",
    "# !wget https://raw.githubusercontent.com/NinaB99/AI-for-Chemistry/main/Data/ADME_public_set_3521.csv\n",
    "# !wget https://raw.githubusercontent.com/NinaB99/AI-for-Chemistry/main/Data/11095_2013_1222_MOESM2_ESM.csv\n",
    "\n",
    "# load biogen data\n",
    "# import pandas as pd\n",
    "\n",
    "# biogen_data=pd.read_csv(\"ADME_public_set_3521.csv\")\n",
    "# print(biogen_data.columns)\n",
    "\n",
    "# #load bioavailabity data\n",
    "# bio_avail_data = pd.read_csv(\"11095_2013_1222_MOESM2_ESM.csv\",sep=\";\")\n",
    "# print(bio_avail_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Internal ID', 'Vendor ID', 'SMILES', 'CollectionName',\n",
      "       'LOG HLM_CLint (mL/min/kg)', 'LOG MDR1-MDCK ER (B-A/A-B)',\n",
      "       'LOG SOLUBILITY PH 6.8 (ug/mL)',\n",
      "       'LOG PLASMA PROTEIN BINDING (HUMAN) (% unbound)',\n",
      "       'LOG PLASMA PROTEIN BINDING (RAT) (% unbound)',\n",
      "       'LOG RLM_CLint (mL/min/kg)', 'MolW(Da)', 'NumHAcceptors', 'NumHDonors',\n",
      "       'LogP', 'Lipinski_rule'],\n",
      "      dtype='object')\n",
      "Index(['ID', 'Name', 'InChI', 'InChIKey', 'SMILES', 'Solubility', 'SD',\n",
      "       'Ocurrences', 'Group', 'MolWt', 'MolLogP', 'MolMR', 'HeavyAtomCount',\n",
      "       'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds',\n",
      "       'NumValenceElectrons', 'NumAromaticRings', 'NumSaturatedRings',\n",
      "       'NumAliphaticRings', 'RingCount', 'TPSA', 'LabuteASA', 'BalabanJ',\n",
      "       'BertzCT', 'MolW(Da)', 'LogP', 'Lipinski_rule'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# load biogen data\n",
    "import pandas as pd\n",
    "\n",
    "biogen_data=pd.read_csv(\"../Data/Biogen.csv\")\n",
    "print(biogen_data.columns)\n",
    "\n",
    "#new data\n",
    "curated_data = pd.read_csv(\"../Data/CuratedSol.csv\")\n",
    "print(curated_data.columns)\n",
    "\n",
    "#load bioavailabity data\n",
    "bio_avail_data = pd.read_csv(\"../Data/Bioavailibility.csv\")\n",
    "#print(bio_avail_data.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use feed-forward NN using pytorch.\n",
    "train on solubility from dataset 1 first.\n",
    "then optimize for bioavailability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "c:\\Users\\nina\\anaconda3\\envs\\ai_chem\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (c:\\Users\\nina\\anaconda3\\envs\\ai_chem\\Lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "from deepchem.feat import RDKitDescriptors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disable warnings from RDKit\n",
    "from rdkit import RDLogger \n",
    "RDLogger.DisableLog('rdApp.*') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize(Dataframe: pd.DataFrame):\n",
    "    \n",
    "    \"\"\"Canonicalizes the SMILES from Dataframe. A column called 'SMILES' is requiered\n",
    "\n",
    "    Args: Dataframe with 'SMILES' column contaning smiles. \n",
    "    \"\"\"\n",
    "    \n",
    "    Dataframe['SMILES'] = Dataframe['SMILES'].apply(lambda x: Chem.MolToSmiles(Chem.MolFromSmiles(x))) #canonicalize smiles from a Dataframe                                          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 SMILES  Log solubility\n",
      "0                   CCCCCCCCCCCCCCCCCC[N+](C)(C)C.[Br-]       -3.616127\n",
      "1                                  O=C1Nc2cccc3cccc1c23       -3.254767\n",
      "2                                       O=Cc1ccc(Cl)cc1       -2.177078\n",
      "3     CC(c1ccccc1)c1cc(C(=O)[O-])c(O)c(C(C)c2ccccc2)...       -3.924409\n",
      "4     c1cc(N(CC2CO2)CC2CO2)ccc1Cc1ccc(N(CC2CO2)CC2CO...       -4.662065\n",
      "...                                                 ...             ...\n",
      "3516            O=C(c1ccc2c(c1)CCCC2)N1CCOCC1c1ccn[nH]1             NaN\n",
      "3517           O=C(Nc1nc2ccccc2[nH]1)c1ccc(-n2cccc2)cc1             NaN\n",
      "3518         NC(=O)c1noc([C@@H](CCCC2CCCCC2)CC(=O)NO)n1             NaN\n",
      "3519         CCCCCCCCc1ccc(CC[C@](N)(CO)COP(=O)(O)O)cc1             NaN\n",
      "3520  Cc1cccc(/C=N/Nc2cc(N3CCOCC3)n3nc(-c4ccncc4)cc3...             NaN\n",
      "\n",
      "[13480 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# they are both log so I do not need to log-transform\n",
    "canonicalize(biogen_data)\n",
    "canonicalize(curated_data)\n",
    "\n",
    "#make sure to remove overlaps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrames (replace with your actual data)\n",
    "# biogen_data = pd.read_csv('biogen_data.csv')\n",
    "# curated_data = pd.read_csv('curated_data.csv')\n",
    "\n",
    "# Rename columns for consistency\n",
    "biogen_data_new = biogen_data.rename(columns={\"LOG SOLUBILITY PH 6.8 (ug/mL)\": \"Log solubility\"})\n",
    "curated_data_new = curated_data.rename(columns={\"Solubility\": \"Log solubility\"})\n",
    "\n",
    "# Select relevant columns\n",
    "biogen_data_new = biogen_data_new[[\"SMILES\", \"Log solubility\"]]\n",
    "curated_data_new = curated_data_new[[\"SMILES\", \"Log solubility\"]]\n",
    "\n",
    "# Combine the DataFrames, prioritizing the curated_data\n",
    "data = pd.concat([curated_data_new, biogen_data_new])\n",
    "\n",
    "# Drop duplicate SMILES, keeping the first occurrence (curated_data)\n",
    "data = data.drop_duplicates(subset=\"SMILES\", keep='first')\n",
    "\n",
    "# Reset index for the final combined DataFrame\n",
    "#combined_data = combined_data.reset_index(drop=True)\n",
    "\n",
    "# Print the combined DataFrame\n",
    "print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12147\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### DATA PREPARATION ####\n",
    "# try with new data\n",
    "\n",
    "# Function to generate features from SMILES strings using RDKit descriptors\n",
    "def generate_features(smiles_list):\n",
    "    featurizer = RDKitDescriptors()\n",
    "    features = featurizer.featurize(smiles_list)\n",
    "    # Drop features containing invalid values\n",
    "    used_features = ~np.isnan(features).any(axis=0)\n",
    "    features = features[:, ~np.isnan(features).any(axis=0)]\n",
    "    return features, used_features\n",
    "\n",
    "#remove nan values from data\n",
    "data = data.dropna(subset=['Log solubility'])\n",
    "\n",
    "#get x and y data (x is the molecular descriptors, y is the solubility)\n",
    "y_data = data[\"Log solubility\"]\n",
    "\n",
    "print(len(y_data))\n",
    "\n",
    "# Generate features from SMILES data (get smiles from df)\n",
    "smiles = data[\"SMILES\"]\n",
    "X_data, used_features = generate_features(smiles)\n",
    "\n",
    "#split data into training and validation using train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "# Convert y pandas Series to NumPy array\n",
    "y_train = y_train.values.astype(np.float32)\n",
    "y_test = y_test.values.astype(np.float32)\n",
    "\n",
    "#scale x values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#convert data to pytorch tensors (like numpy arrays but for pytorch)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Reshape the target tensor to match the shape of the output tensor\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "# Reshape the target tensor to match the shape of the output tensor\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6858044085631407"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best Hyperparameters: {'activation': 'relu', 'hidden_layer_sizes': (512, 512, 512), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "Test Score (R^2): 0.1923752586047588\n"
     ]
    }
   ],
   "source": [
    "#### CREATING MODEL ####\n",
    "\n",
    "#haven't tried it with the newest settings\n",
    "\n",
    "# Define the neural network architecture\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_data.shape[1]  # Number of molecular descriptors. 201 different ones\n",
    "hidden_dim = 256   #best according to grid search\n",
    "output_dim = 1\n",
    "learning_rate = 0.001 #gets overruled by the grid search\n",
    "num_epochs = 800\n",
    "batch_size = 32\n",
    "\n",
    "#create dataloader for batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#initialize model, loss function (nn.MSELoss) and optimizer\n",
    "model = SolubilityPredictor(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "##### Grid-search for best hyperparameters #####\n",
    "# Define your hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(256,256,256),(512,512,512)],  # Number of neurons in the hidden layer(s)\n",
    "    'activation': ['relu', 'tanh'],  # Activation function\n",
    "    'solver': ['adam', 'sgd'],  # Optimization algorithm\n",
    "    'learning_rate': ['constant', 'adaptive'],  # Learning rate schedule\n",
    "}\n",
    "\n",
    "# Create an MLPRegressor object\n",
    "mlp = MLPRegressor(max_iter=num_epochs, batch_size=batch_size)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_score = model.score(X_test, y_test)\n",
    "print(\"Test Score (R^2):\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.3812762971349799\n",
      "Test RMSE: 0.6174757461916864\n",
      "Test MAE: 0.4129440122100839\n",
      "Test R^2: 0.1923752586047588\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate additional metrics\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 943.4128410816193\n",
      "Epoch 2/1000, Loss: 571.640143930912\n",
      "Epoch 3/1000, Loss: 495.7608366012573\n",
      "Epoch 4/1000, Loss: 442.81039038300514\n",
      "Epoch 5/1000, Loss: 381.6474874615669\n",
      "Epoch 6/1000, Loss: 353.0723693370819\n",
      "Epoch 7/1000, Loss: 316.92699748277664\n",
      "Epoch 8/1000, Loss: 282.5618694126606\n",
      "Epoch 9/1000, Loss: 256.27133582532406\n",
      "Epoch 10/1000, Loss: 261.1899362206459\n",
      "Epoch 11/1000, Loss: 228.9344640672207\n",
      "Epoch 12/1000, Loss: 226.72700282931328\n",
      "Epoch 13/1000, Loss: 209.87524117529392\n",
      "Epoch 14/1000, Loss: 201.9399061203003\n",
      "Epoch 15/1000, Loss: 182.9896047860384\n",
      "Epoch 16/1000, Loss: 172.0141593515873\n",
      "Epoch 17/1000, Loss: 169.30880476534367\n",
      "Epoch 18/1000, Loss: 156.33979614824057\n",
      "Epoch 19/1000, Loss: 153.13747150450945\n",
      "Epoch 20/1000, Loss: 153.09689316153526\n",
      "Epoch 21/1000, Loss: 144.44090854376554\n",
      "Epoch 22/1000, Loss: 142.0796439871192\n",
      "Epoch 23/1000, Loss: 146.01142153143883\n",
      "Epoch 24/1000, Loss: 133.36737098544836\n",
      "Epoch 25/1000, Loss: 124.46246548742056\n",
      "Epoch 26/1000, Loss: 114.38500310480595\n",
      "Epoch 27/1000, Loss: 111.81061121821404\n",
      "Epoch 28/1000, Loss: 114.92178344726562\n",
      "Epoch 29/1000, Loss: 107.35776251554489\n",
      "Epoch 30/1000, Loss: 101.8377664834261\n",
      "Epoch 31/1000, Loss: 102.13250274956226\n",
      "Epoch 32/1000, Loss: 103.28179482370615\n",
      "Epoch 33/1000, Loss: 109.28278897702694\n",
      "Epoch 34/1000, Loss: 100.53423695266247\n",
      "Epoch 35/1000, Loss: 109.17622044682503\n",
      "Epoch 36/1000, Loss: 101.5771976262331\n",
      "Epoch 37/1000, Loss: 94.39007326215506\n",
      "Epoch 38/1000, Loss: 85.81650850176811\n",
      "Epoch 39/1000, Loss: 80.17301827669144\n",
      "Epoch 40/1000, Loss: 79.96115247905254\n",
      "Epoch 41/1000, Loss: 82.94071312993765\n",
      "Epoch 42/1000, Loss: 77.43529425561428\n",
      "Epoch 43/1000, Loss: 81.90458048507571\n",
      "Epoch 44/1000, Loss: 76.97896751761436\n",
      "Epoch 45/1000, Loss: 75.4415609613061\n",
      "Epoch 46/1000, Loss: 76.34740432724357\n",
      "Epoch 47/1000, Loss: 75.68794446066022\n",
      "Epoch 48/1000, Loss: 77.1002660356462\n",
      "Epoch 49/1000, Loss: 72.88190218806267\n",
      "Epoch 50/1000, Loss: 87.00267115235329\n",
      "Epoch 51/1000, Loss: 72.93307692930102\n",
      "Epoch 52/1000, Loss: 64.64793185144663\n",
      "Epoch 53/1000, Loss: 58.99922210350633\n",
      "Epoch 54/1000, Loss: 63.27713614329696\n",
      "Epoch 55/1000, Loss: 68.65943380072713\n",
      "Epoch 56/1000, Loss: 62.07721168547869\n",
      "Epoch 57/1000, Loss: 61.79177628830075\n",
      "Epoch 58/1000, Loss: 61.42707750573754\n",
      "Epoch 59/1000, Loss: 71.40844177827239\n",
      "Epoch 60/1000, Loss: 61.94638139382005\n",
      "Epoch 61/1000, Loss: 58.82545916363597\n",
      "Epoch 62/1000, Loss: 57.1782517619431\n",
      "Epoch 63/1000, Loss: 51.279441982507706\n",
      "Epoch 64/1000, Loss: 53.527800157666206\n",
      "Epoch 65/1000, Loss: 52.74836777150631\n",
      "Epoch 66/1000, Loss: 54.84596421942115\n",
      "Epoch 67/1000, Loss: 54.522287879139185\n",
      "Epoch 68/1000, Loss: 53.42222495004535\n",
      "Epoch 69/1000, Loss: 55.79672884568572\n",
      "Epoch 70/1000, Loss: 56.50558525323868\n",
      "Epoch 71/1000, Loss: 51.21476807817817\n",
      "Epoch 72/1000, Loss: 50.14790600538254\n",
      "Epoch 73/1000, Loss: 60.32375351153314\n",
      "Epoch 74/1000, Loss: 52.44881509244442\n",
      "Epoch 75/1000, Loss: 48.72807664051652\n",
      "Epoch 76/1000, Loss: 52.966139908879995\n",
      "Epoch 77/1000, Loss: 44.49269421771169\n",
      "Epoch 78/1000, Loss: 44.54292057082057\n",
      "Epoch 79/1000, Loss: 41.529021106660366\n",
      "Epoch 80/1000, Loss: 45.75723872333765\n",
      "Epoch 81/1000, Loss: 46.81652660667896\n",
      "Epoch 82/1000, Loss: 48.65072264149785\n",
      "Epoch 83/1000, Loss: 45.20862936973572\n",
      "Epoch 84/1000, Loss: 43.51077377796173\n",
      "Epoch 85/1000, Loss: 41.45057456381619\n",
      "Epoch 86/1000, Loss: 44.251445669680834\n",
      "Epoch 87/1000, Loss: 42.25408884137869\n",
      "Epoch 88/1000, Loss: 45.21645696461201\n",
      "Epoch 89/1000, Loss: 44.497137762606144\n",
      "Epoch 90/1000, Loss: 47.811785669997334\n",
      "Epoch 91/1000, Loss: 47.585232615470886\n",
      "Epoch 92/1000, Loss: 41.84580730646849\n",
      "Epoch 93/1000, Loss: 38.80873337760568\n",
      "Epoch 94/1000, Loss: 40.41204481199384\n",
      "Epoch 95/1000, Loss: 40.14116461202502\n",
      "Epoch 96/1000, Loss: 39.73487458191812\n",
      "Epoch 97/1000, Loss: 40.43867340683937\n",
      "Epoch 98/1000, Loss: 37.75293508544564\n",
      "Epoch 99/1000, Loss: 36.46059535816312\n",
      "Epoch 100/1000, Loss: 38.36464300751686\n",
      "Epoch 101/1000, Loss: 37.22714728862047\n",
      "Epoch 102/1000, Loss: 39.05780881084502\n",
      "Epoch 103/1000, Loss: 38.83552901446819\n",
      "Epoch 104/1000, Loss: 38.60471786186099\n",
      "Epoch 105/1000, Loss: 35.97335657291114\n",
      "Epoch 106/1000, Loss: 34.97576239518821\n",
      "Epoch 107/1000, Loss: 42.054554246366024\n",
      "Epoch 108/1000, Loss: 41.6623384449631\n",
      "Epoch 109/1000, Loss: 39.68302835896611\n",
      "Epoch 110/1000, Loss: 34.48932203091681\n",
      "Epoch 111/1000, Loss: 33.2306445222348\n",
      "Epoch 112/1000, Loss: 31.509112467989326\n",
      "Epoch 113/1000, Loss: 36.14502681046724\n",
      "Epoch 114/1000, Loss: 35.840957118198276\n",
      "Epoch 115/1000, Loss: 35.1765752825886\n",
      "Epoch 116/1000, Loss: 34.76567982509732\n",
      "Epoch 117/1000, Loss: 38.47209503687918\n",
      "Epoch 118/1000, Loss: 38.053368819877505\n",
      "Epoch 119/1000, Loss: 34.577449433505535\n",
      "Epoch 120/1000, Loss: 33.44541588053107\n",
      "Epoch 121/1000, Loss: 32.92449105344713\n",
      "Epoch 122/1000, Loss: 30.28226483054459\n",
      "Epoch 123/1000, Loss: 36.410896675661206\n",
      "Epoch 124/1000, Loss: 40.35917122848332\n",
      "Epoch 125/1000, Loss: 30.591295890510082\n",
      "Epoch 126/1000, Loss: 28.19272881373763\n",
      "Epoch 127/1000, Loss: 29.947990918532014\n",
      "Epoch 128/1000, Loss: 31.94517103396356\n",
      "Epoch 129/1000, Loss: 30.71587354876101\n",
      "Epoch 130/1000, Loss: 32.915180791169405\n",
      "Epoch 131/1000, Loss: 33.61463633924723\n",
      "Epoch 132/1000, Loss: 30.926336757838726\n",
      "Epoch 133/1000, Loss: 31.09678516536951\n",
      "Epoch 134/1000, Loss: 30.5891120005399\n",
      "Epoch 135/1000, Loss: 32.54505981877446\n",
      "Epoch 136/1000, Loss: 30.821362666785717\n",
      "Epoch 137/1000, Loss: 28.518349692225456\n",
      "Epoch 138/1000, Loss: 30.176128225401044\n",
      "Epoch 139/1000, Loss: 33.042774464935064\n",
      "Epoch 140/1000, Loss: 33.692418256774545\n",
      "Epoch 141/1000, Loss: 28.983011757954955\n",
      "Epoch 142/1000, Loss: 27.118525099009275\n",
      "Epoch 143/1000, Loss: 26.820571107789874\n",
      "Epoch 144/1000, Loss: 28.01444967649877\n",
      "Epoch 145/1000, Loss: 30.070745768025517\n",
      "Epoch 146/1000, Loss: 28.915638548322022\n",
      "Epoch 147/1000, Loss: 30.829682207666337\n",
      "Epoch 148/1000, Loss: 28.448664190247655\n",
      "Epoch 149/1000, Loss: 28.590575693175197\n",
      "Epoch 150/1000, Loss: 30.1330337356776\n",
      "Epoch 151/1000, Loss: 30.22667940892279\n",
      "Epoch 152/1000, Loss: 29.334903210401535\n",
      "Epoch 153/1000, Loss: 35.35583649761975\n",
      "Epoch 154/1000, Loss: 31.07170951552689\n",
      "Epoch 155/1000, Loss: 27.29019832983613\n",
      "Epoch 156/1000, Loss: 24.6856300951913\n",
      "Epoch 157/1000, Loss: 24.92093257419765\n",
      "Epoch 158/1000, Loss: 25.81072974577546\n",
      "Epoch 159/1000, Loss: 24.629720624536276\n",
      "Epoch 160/1000, Loss: 26.076642762869596\n",
      "Epoch 161/1000, Loss: 28.841022012755275\n",
      "Epoch 162/1000, Loss: 27.251963450573385\n",
      "Epoch 163/1000, Loss: 32.848718931898475\n",
      "Epoch 164/1000, Loss: 26.352439215406775\n",
      "Epoch 165/1000, Loss: 25.723304166458547\n",
      "Epoch 166/1000, Loss: 24.662250012159348\n",
      "Epoch 167/1000, Loss: 25.93607322126627\n",
      "Epoch 168/1000, Loss: 25.087312979623675\n",
      "Epoch 169/1000, Loss: 27.397194618359208\n",
      "Epoch 170/1000, Loss: 29.908847026526928\n",
      "Epoch 171/1000, Loss: 32.62157170660794\n",
      "Epoch 172/1000, Loss: 29.223442936316133\n",
      "Epoch 173/1000, Loss: 24.120901859365404\n",
      "Epoch 174/1000, Loss: 25.180799908936024\n",
      "Epoch 175/1000, Loss: 26.752474702894688\n",
      "Epoch 176/1000, Loss: 26.44606267195195\n",
      "Epoch 177/1000, Loss: 23.08241495396942\n",
      "Epoch 178/1000, Loss: 24.5839753607288\n",
      "Epoch 179/1000, Loss: 23.203080218285322\n",
      "Epoch 180/1000, Loss: 22.68372364155948\n",
      "Epoch 181/1000, Loss: 22.493225660175085\n",
      "Epoch 182/1000, Loss: 27.2821254581213\n",
      "Epoch 183/1000, Loss: 32.118836326524615\n",
      "Epoch 184/1000, Loss: 26.21699772402644\n",
      "Epoch 185/1000, Loss: 26.324626780115068\n",
      "Epoch 186/1000, Loss: 24.271038877777755\n",
      "Epoch 187/1000, Loss: 23.58729548472911\n",
      "Epoch 188/1000, Loss: 21.315091591328382\n",
      "Epoch 189/1000, Loss: 21.26817575097084\n",
      "Epoch 190/1000, Loss: 22.312403421849012\n",
      "Epoch 191/1000, Loss: 25.593045696616173\n",
      "Epoch 192/1000, Loss: 25.127005846239626\n",
      "Epoch 193/1000, Loss: 24.340316184796393\n",
      "Epoch 194/1000, Loss: 25.79185820557177\n",
      "Epoch 195/1000, Loss: 22.757116018794477\n",
      "Epoch 196/1000, Loss: 24.532655444927514\n",
      "Epoch 197/1000, Loss: 26.00707416702062\n",
      "Epoch 198/1000, Loss: 25.42614754009992\n",
      "Epoch 199/1000, Loss: 21.98206594027579\n",
      "Epoch 200/1000, Loss: 22.093143952079117\n",
      "Epoch 201/1000, Loss: 22.563227181322873\n",
      "Epoch 202/1000, Loss: 21.434279991313815\n",
      "Epoch 203/1000, Loss: 30.907607873901725\n",
      "Epoch 204/1000, Loss: 24.742428600788116\n",
      "Epoch 205/1000, Loss: 23.92988257110119\n",
      "Epoch 206/1000, Loss: 25.45874429680407\n",
      "Epoch 207/1000, Loss: 27.64648761227727\n",
      "Epoch 208/1000, Loss: 27.498419016599655\n",
      "Epoch 209/1000, Loss: 22.356796928681433\n",
      "Epoch 210/1000, Loss: 20.827480908483267\n",
      "Epoch 211/1000, Loss: 20.331703742966056\n",
      "Epoch 212/1000, Loss: 19.807650468312204\n",
      "Epoch 213/1000, Loss: 21.401169085875154\n",
      "Epoch 214/1000, Loss: 25.842123586684465\n",
      "Epoch 215/1000, Loss: 25.85995693039149\n",
      "Epoch 216/1000, Loss: 22.78739400021732\n",
      "Epoch 217/1000, Loss: 19.617876388132572\n",
      "Epoch 218/1000, Loss: 18.97351219225675\n",
      "Epoch 219/1000, Loss: 19.396502520889044\n",
      "Epoch 220/1000, Loss: 21.541596461087465\n",
      "Epoch 221/1000, Loss: 21.560824972577393\n",
      "Epoch 222/1000, Loss: 22.359210801310837\n",
      "Epoch 223/1000, Loss: 23.67509611789137\n",
      "Epoch 224/1000, Loss: 21.088184957392514\n",
      "Epoch 225/1000, Loss: 22.226012555882335\n",
      "Epoch 226/1000, Loss: 27.725875912234187\n",
      "Epoch 227/1000, Loss: 23.78178637754172\n",
      "Epoch 228/1000, Loss: 19.070399183779955\n",
      "Epoch 229/1000, Loss: 18.634607147425413\n",
      "Epoch 230/1000, Loss: 15.912321496754885\n",
      "Epoch 231/1000, Loss: 17.970230326987803\n",
      "Epoch 232/1000, Loss: 19.737740400247276\n",
      "Epoch 233/1000, Loss: 21.34905102662742\n",
      "Epoch 234/1000, Loss: 19.885017415508628\n",
      "Epoch 235/1000, Loss: 22.375684699974954\n",
      "Epoch 236/1000, Loss: 19.476731264963746\n",
      "Epoch 237/1000, Loss: 20.3348075626418\n",
      "Epoch 238/1000, Loss: 18.741955439560115\n",
      "Epoch 239/1000, Loss: 18.26587489526719\n",
      "Epoch 240/1000, Loss: 19.00104528479278\n",
      "Epoch 241/1000, Loss: 20.078350879251957\n",
      "Epoch 242/1000, Loss: 22.15682539343834\n",
      "Epoch 243/1000, Loss: 21.073848425410688\n",
      "Epoch 244/1000, Loss: 20.517254971899092\n",
      "Epoch 245/1000, Loss: 21.01321545150131\n",
      "Epoch 246/1000, Loss: 20.864554684609175\n",
      "Epoch 247/1000, Loss: 20.094573301263154\n",
      "Epoch 248/1000, Loss: 22.50080536492169\n",
      "Epoch 249/1000, Loss: 20.327306204475462\n",
      "Epoch 250/1000, Loss: 20.05136052519083\n",
      "Epoch 251/1000, Loss: 17.44675908330828\n",
      "Epoch 252/1000, Loss: 16.572849086020142\n",
      "Epoch 253/1000, Loss: 19.357280964963138\n",
      "Epoch 254/1000, Loss: 19.773204456083477\n",
      "Epoch 255/1000, Loss: 21.60836442746222\n",
      "Epoch 256/1000, Loss: 19.236438390333205\n",
      "Epoch 257/1000, Loss: 22.29393025301397\n",
      "Epoch 258/1000, Loss: 18.388235392980278\n",
      "Epoch 259/1000, Loss: 17.237105602398515\n",
      "Epoch 260/1000, Loss: 18.081416530534625\n",
      "Epoch 261/1000, Loss: 18.49439286161214\n",
      "Epoch 262/1000, Loss: 19.700979694724083\n",
      "Epoch 263/1000, Loss: 20.02217183355242\n",
      "Epoch 264/1000, Loss: 21.068085213191807\n",
      "Epoch 265/1000, Loss: 19.070537260733545\n",
      "Epoch 266/1000, Loss: 17.915764139033854\n",
      "Epoch 267/1000, Loss: 16.722810133360326\n",
      "Epoch 268/1000, Loss: 24.554571501910686\n",
      "Epoch 269/1000, Loss: 19.45075120218098\n",
      "Epoch 270/1000, Loss: 17.54745609499514\n",
      "Epoch 271/1000, Loss: 16.38612789195031\n",
      "Epoch 272/1000, Loss: 19.688827384263277\n",
      "Epoch 273/1000, Loss: 16.543428217060864\n",
      "Epoch 274/1000, Loss: 17.873142364434898\n",
      "Epoch 275/1000, Loss: 19.519459205679595\n",
      "Epoch 276/1000, Loss: 19.339621904306114\n",
      "Epoch 277/1000, Loss: 17.833928524516523\n",
      "Epoch 278/1000, Loss: 17.329620024189353\n",
      "Epoch 279/1000, Loss: 16.32488157134503\n",
      "Epoch 280/1000, Loss: 16.972669540904462\n",
      "Epoch 281/1000, Loss: 16.80696507357061\n",
      "Epoch 282/1000, Loss: 16.202804475091398\n",
      "Epoch 283/1000, Loss: 21.956957096233964\n",
      "Epoch 284/1000, Loss: 21.087490134872496\n",
      "Epoch 285/1000, Loss: 16.750753732398152\n",
      "Epoch 286/1000, Loss: 16.660210276022553\n",
      "Epoch 287/1000, Loss: 16.3734844410792\n",
      "Epoch 288/1000, Loss: 18.76494532544166\n",
      "Epoch 289/1000, Loss: 22.26674667932093\n",
      "Epoch 290/1000, Loss: 20.410761536099017\n",
      "Epoch 291/1000, Loss: 25.695856575854123\n",
      "Epoch 292/1000, Loss: 23.251263480633497\n",
      "Epoch 293/1000, Loss: 17.64740785676986\n",
      "Epoch 294/1000, Loss: 16.013009526766837\n",
      "Epoch 295/1000, Loss: 14.186908304225653\n",
      "Epoch 296/1000, Loss: 15.09319250099361\n",
      "Epoch 297/1000, Loss: 15.319483879487962\n",
      "Epoch 298/1000, Loss: 15.428327057510614\n",
      "Epoch 299/1000, Loss: 18.216563887894154\n",
      "Epoch 300/1000, Loss: 16.648312629200518\n",
      "Epoch 301/1000, Loss: 17.189512534067035\n",
      "Epoch 302/1000, Loss: 17.218622461892664\n",
      "Epoch 303/1000, Loss: 17.020604541525245\n",
      "Epoch 304/1000, Loss: 17.521386926993728\n",
      "Epoch 305/1000, Loss: 18.37175650894642\n",
      "Epoch 306/1000, Loss: 18.05724160373211\n",
      "Epoch 307/1000, Loss: 16.33932256512344\n",
      "Epoch 308/1000, Loss: 15.719956068787724\n",
      "Epoch 309/1000, Loss: 16.38877698685974\n",
      "Epoch 310/1000, Loss: 22.551015866920352\n",
      "Epoch 311/1000, Loss: 18.60052880179137\n",
      "Epoch 312/1000, Loss: 15.049410968087614\n",
      "Epoch 313/1000, Loss: 16.06824324838817\n",
      "Epoch 314/1000, Loss: 14.780213118065149\n",
      "Epoch 315/1000, Loss: 14.631225787568837\n",
      "Epoch 316/1000, Loss: 15.655262301675975\n",
      "Epoch 317/1000, Loss: 15.248174739535898\n",
      "Epoch 318/1000, Loss: 15.780019274912775\n",
      "Epoch 319/1000, Loss: 17.422679897397757\n",
      "Epoch 320/1000, Loss: 19.65090009663254\n",
      "Epoch 321/1000, Loss: 19.4519475242123\n",
      "Epoch 322/1000, Loss: 16.32138563040644\n",
      "Epoch 323/1000, Loss: 16.44041788810864\n",
      "Epoch 324/1000, Loss: 15.560954270418733\n",
      "Epoch 325/1000, Loss: 15.133224329911172\n",
      "Epoch 326/1000, Loss: 16.899416159838438\n",
      "Epoch 327/1000, Loss: 15.653744738548994\n",
      "Epoch 328/1000, Loss: 13.9519406799227\n",
      "Epoch 329/1000, Loss: 14.2203452847898\n",
      "Epoch 330/1000, Loss: 15.582156007178128\n",
      "Epoch 331/1000, Loss: 17.307565945200622\n",
      "Epoch 332/1000, Loss: 14.544434936251491\n",
      "Epoch 333/1000, Loss: 15.250964569859207\n",
      "Epoch 334/1000, Loss: 15.409350104164332\n",
      "Epoch 335/1000, Loss: 17.678328317590058\n",
      "Epoch 336/1000, Loss: 16.050507305189967\n",
      "Epoch 337/1000, Loss: 15.561164366547018\n",
      "Epoch 338/1000, Loss: 15.796690463088453\n",
      "Epoch 339/1000, Loss: 16.015846907161176\n",
      "Epoch 340/1000, Loss: 15.725271987728775\n",
      "Epoch 341/1000, Loss: 17.412580871023238\n",
      "Epoch 342/1000, Loss: 15.509927093051374\n",
      "Epoch 343/1000, Loss: 13.63005946110934\n",
      "Epoch 344/1000, Loss: 14.569792368914932\n",
      "Epoch 345/1000, Loss: 15.070774199441075\n",
      "Epoch 346/1000, Loss: 15.540940189734101\n",
      "Epoch 347/1000, Loss: 16.576763158664107\n",
      "Epoch 348/1000, Loss: 15.355584776960313\n",
      "Epoch 349/1000, Loss: 14.24836830701679\n",
      "Epoch 350/1000, Loss: 13.7588941892609\n",
      "Epoch 351/1000, Loss: 14.209822020493448\n",
      "Epoch 352/1000, Loss: 15.047560296021402\n",
      "Epoch 353/1000, Loss: 16.302571445703506\n",
      "Epoch 354/1000, Loss: 15.32377363462001\n",
      "Epoch 355/1000, Loss: 14.751765364781022\n",
      "Epoch 356/1000, Loss: 13.623671268112957\n",
      "Epoch 357/1000, Loss: 14.64087782241404\n",
      "Epoch 358/1000, Loss: 14.686253254301846\n",
      "Epoch 359/1000, Loss: 15.444959341548383\n",
      "Epoch 360/1000, Loss: 14.594919747207314\n",
      "Epoch 361/1000, Loss: 14.377770962193608\n",
      "Epoch 362/1000, Loss: 16.872087088413537\n",
      "Epoch 363/1000, Loss: 16.1815243139863\n",
      "Epoch 364/1000, Loss: 15.41292314324528\n",
      "Epoch 365/1000, Loss: 15.37968796864152\n",
      "Epoch 366/1000, Loss: 15.500874074175954\n",
      "Epoch 367/1000, Loss: 15.383823324926198\n",
      "Epoch 368/1000, Loss: 13.888375624082983\n",
      "Epoch 369/1000, Loss: 13.967740491498262\n",
      "Epoch 370/1000, Loss: 14.228929845616221\n",
      "Epoch 371/1000, Loss: 14.487076388206333\n",
      "Epoch 372/1000, Loss: 14.518899806775153\n",
      "Epoch 373/1000, Loss: 15.653067905455828\n",
      "Epoch 374/1000, Loss: 16.11603955551982\n",
      "Epoch 375/1000, Loss: 19.611233728006482\n",
      "Epoch 376/1000, Loss: 14.200760898180306\n",
      "Epoch 377/1000, Loss: 14.949666106607765\n",
      "Epoch 378/1000, Loss: 12.80298276990652\n",
      "Epoch 379/1000, Loss: 13.47351685911417\n",
      "Epoch 380/1000, Loss: 12.864099157974124\n",
      "Epoch 381/1000, Loss: 12.915939365047961\n",
      "Epoch 382/1000, Loss: 13.65468612499535\n",
      "Epoch 383/1000, Loss: 14.027304980903864\n",
      "Epoch 384/1000, Loss: 15.178528890945017\n",
      "Epoch 385/1000, Loss: 14.152373063378036\n",
      "Epoch 386/1000, Loss: 13.786738958675414\n",
      "Epoch 387/1000, Loss: 15.045235328376293\n",
      "Epoch 388/1000, Loss: 13.39763675397262\n",
      "Epoch 389/1000, Loss: 14.66767635056749\n",
      "Epoch 390/1000, Loss: 13.985816605389118\n",
      "Epoch 391/1000, Loss: 13.471429427154362\n",
      "Epoch 392/1000, Loss: 13.618077018298209\n",
      "Epoch 393/1000, Loss: 14.391667450778186\n",
      "Epoch 394/1000, Loss: 14.358444104902446\n",
      "Epoch 395/1000, Loss: 14.922099428717047\n",
      "Epoch 396/1000, Loss: 16.58677490334958\n",
      "Epoch 397/1000, Loss: 18.268751569092274\n",
      "Epoch 398/1000, Loss: 14.353414844721556\n",
      "Epoch 399/1000, Loss: 15.197524367831647\n",
      "Epoch 400/1000, Loss: 13.601968507748097\n",
      "Epoch 401/1000, Loss: 13.844561673235148\n",
      "Epoch 402/1000, Loss: 13.534464702475816\n",
      "Epoch 403/1000, Loss: 13.687332722358406\n",
      "Epoch 404/1000, Loss: 12.399192941375077\n",
      "Epoch 405/1000, Loss: 14.14325508987531\n",
      "Epoch 406/1000, Loss: 13.119826725684106\n",
      "Epoch 407/1000, Loss: 14.8886694079265\n",
      "Epoch 408/1000, Loss: 14.753629256039858\n",
      "Epoch 409/1000, Loss: 14.576941832900047\n",
      "Epoch 410/1000, Loss: 12.718414043541998\n",
      "Epoch 411/1000, Loss: 13.061435657553375\n",
      "Epoch 412/1000, Loss: 12.656596918590367\n",
      "Epoch 413/1000, Loss: 14.338197343517095\n",
      "Epoch 414/1000, Loss: 18.269636605866253\n",
      "Epoch 415/1000, Loss: 23.313012802042067\n",
      "Epoch 416/1000, Loss: 13.829602929297835\n",
      "Epoch 417/1000, Loss: 13.396236715372652\n",
      "Epoch 418/1000, Loss: 14.997783109545708\n",
      "Epoch 419/1000, Loss: 11.810185699723661\n",
      "Epoch 420/1000, Loss: 11.501850124448538\n",
      "Epoch 421/1000, Loss: 12.423674947116524\n",
      "Epoch 422/1000, Loss: 12.55073100188747\n",
      "Epoch 423/1000, Loss: 11.898477246053517\n",
      "Epoch 424/1000, Loss: 13.405765355098993\n",
      "Epoch 425/1000, Loss: 14.993841495364904\n",
      "Epoch 426/1000, Loss: 12.862161443103105\n",
      "Epoch 427/1000, Loss: 12.645733749959618\n",
      "Epoch 428/1000, Loss: 13.575951582286507\n",
      "Epoch 429/1000, Loss: 12.359372696839273\n",
      "Epoch 430/1000, Loss: 12.936425236053765\n",
      "Epoch 431/1000, Loss: 13.492383523844182\n",
      "Epoch 432/1000, Loss: 13.679176927078515\n",
      "Epoch 433/1000, Loss: 14.022919454146177\n",
      "Epoch 434/1000, Loss: 14.035173123702407\n",
      "Epoch 435/1000, Loss: 12.914264430757612\n",
      "Epoch 436/1000, Loss: 12.62300910660997\n",
      "Epoch 437/1000, Loss: 12.477345537859946\n",
      "Epoch 438/1000, Loss: 14.093058312311769\n",
      "Epoch 439/1000, Loss: 13.401378792710602\n",
      "Epoch 440/1000, Loss: 13.218088808935136\n",
      "Epoch 441/1000, Loss: 13.385615764185786\n",
      "Epoch 442/1000, Loss: 13.41567356698215\n",
      "Epoch 443/1000, Loss: 13.624728955794126\n",
      "Epoch 444/1000, Loss: 13.257840630598366\n",
      "Epoch 445/1000, Loss: 13.539710238575935\n",
      "Epoch 446/1000, Loss: 13.502312884200364\n",
      "Epoch 447/1000, Loss: 14.068844454362988\n",
      "Epoch 448/1000, Loss: 13.010770194698125\n",
      "Epoch 449/1000, Loss: 12.781183383893222\n",
      "Epoch 450/1000, Loss: 12.548821691889316\n",
      "Epoch 451/1000, Loss: 11.929931396152824\n",
      "Epoch 452/1000, Loss: 13.104232599958777\n",
      "Epoch 453/1000, Loss: 12.030202005524188\n",
      "Epoch 454/1000, Loss: 12.268947943113744\n",
      "Epoch 455/1000, Loss: 12.031809304375201\n",
      "Epoch 456/1000, Loss: 13.34828669950366\n",
      "Epoch 457/1000, Loss: 13.7637994219549\n",
      "Epoch 458/1000, Loss: 13.336889427155256\n",
      "Epoch 459/1000, Loss: 13.952347470447421\n",
      "Epoch 460/1000, Loss: 13.752485469449311\n",
      "Epoch 461/1000, Loss: 11.64877297822386\n",
      "Epoch 462/1000, Loss: 10.894291056320071\n",
      "Epoch 463/1000, Loss: 11.594676238950342\n",
      "Epoch 464/1000, Loss: 13.009568879846483\n",
      "Epoch 465/1000, Loss: 12.783145481254905\n",
      "Epoch 466/1000, Loss: 12.404429028742015\n",
      "Epoch 467/1000, Loss: 11.516150399111211\n",
      "Epoch 468/1000, Loss: 11.980213586706668\n",
      "Epoch 469/1000, Loss: 12.83677232870832\n",
      "Epoch 470/1000, Loss: 15.824401248246431\n",
      "Epoch 471/1000, Loss: 14.094732724595815\n",
      "Epoch 472/1000, Loss: 12.09449576260522\n",
      "Epoch 473/1000, Loss: 10.846885586623102\n",
      "Epoch 474/1000, Loss: 11.754574168939143\n",
      "Epoch 475/1000, Loss: 13.44422418717295\n",
      "Epoch 476/1000, Loss: 12.453017166815698\n",
      "Epoch 477/1000, Loss: 12.935773719567806\n",
      "Epoch 478/1000, Loss: 12.076387658715248\n",
      "Epoch 479/1000, Loss: 12.623949815984815\n",
      "Epoch 480/1000, Loss: 12.470674314070493\n",
      "Epoch 481/1000, Loss: 11.847901844419539\n",
      "Epoch 482/1000, Loss: 11.983242655638605\n",
      "Epoch 483/1000, Loss: 12.495858257170767\n",
      "Epoch 484/1000, Loss: 12.875161051284522\n",
      "Epoch 485/1000, Loss: 12.11928388569504\n",
      "Epoch 486/1000, Loss: 11.780133091378957\n",
      "Epoch 487/1000, Loss: 11.46105591347441\n",
      "Epoch 488/1000, Loss: 11.454889308661222\n",
      "Epoch 489/1000, Loss: 12.787733887322247\n",
      "Epoch 490/1000, Loss: 12.595562990754843\n",
      "Epoch 491/1000, Loss: 13.736800836864859\n",
      "Epoch 492/1000, Loss: 14.195901867002249\n",
      "Epoch 493/1000, Loss: 12.614810986444354\n",
      "Epoch 494/1000, Loss: 10.883240116294473\n",
      "Epoch 495/1000, Loss: 11.592996786814183\n",
      "Epoch 496/1000, Loss: 11.949263129848987\n",
      "Epoch 497/1000, Loss: 11.579615922179073\n",
      "Epoch 498/1000, Loss: 11.95508745033294\n",
      "Epoch 499/1000, Loss: 13.89131556916982\n",
      "Epoch 500/1000, Loss: 13.67525181081146\n",
      "Epoch 501/1000, Loss: 11.800022937823087\n",
      "Epoch 502/1000, Loss: 12.423360566142946\n",
      "Epoch 503/1000, Loss: 11.53628515265882\n",
      "Epoch 504/1000, Loss: 12.51137205120176\n",
      "Epoch 505/1000, Loss: 11.284607876092196\n",
      "Epoch 506/1000, Loss: 12.23060828819871\n",
      "Epoch 507/1000, Loss: 11.939100987277925\n",
      "Epoch 508/1000, Loss: 11.971114583779126\n",
      "Epoch 509/1000, Loss: 11.039832737762481\n",
      "Epoch 510/1000, Loss: 11.876540339086205\n",
      "Epoch 511/1000, Loss: 12.40656459517777\n",
      "Epoch 512/1000, Loss: 12.641994771547616\n",
      "Epoch 513/1000, Loss: 12.401831480208784\n",
      "Epoch 514/1000, Loss: 12.266142168547958\n",
      "Epoch 515/1000, Loss: 11.411980671342462\n",
      "Epoch 516/1000, Loss: 11.250703371595591\n",
      "Epoch 517/1000, Loss: 11.832277045352384\n",
      "Epoch 518/1000, Loss: 11.183845464140177\n",
      "Epoch 519/1000, Loss: 15.025648304261267\n",
      "Epoch 520/1000, Loss: 13.504089609719813\n",
      "Epoch 521/1000, Loss: 12.546097768004984\n",
      "Epoch 522/1000, Loss: 11.447986057493836\n",
      "Epoch 523/1000, Loss: 10.221283182501793\n",
      "Epoch 524/1000, Loss: 12.17622357653454\n",
      "Epoch 525/1000, Loss: 13.191657193005085\n",
      "Epoch 526/1000, Loss: 12.846632057335228\n",
      "Epoch 527/1000, Loss: 12.518161765532568\n",
      "Epoch 528/1000, Loss: 11.939403671305627\n",
      "Epoch 529/1000, Loss: 10.389588698279113\n",
      "Epoch 530/1000, Loss: 10.647848940920085\n",
      "Epoch 531/1000, Loss: 10.771957966731861\n",
      "Epoch 532/1000, Loss: 12.786247435491532\n",
      "Epoch 533/1000, Loss: 11.764350379351526\n",
      "Epoch 534/1000, Loss: 11.43738370994106\n",
      "Epoch 535/1000, Loss: 12.025106029119343\n",
      "Epoch 536/1000, Loss: 11.119615379255265\n",
      "Epoch 537/1000, Loss: 11.059415450319648\n",
      "Epoch 538/1000, Loss: 11.118203154299408\n",
      "Epoch 539/1000, Loss: 11.632481810171157\n",
      "Epoch 540/1000, Loss: 11.266727516427636\n",
      "Epoch 541/1000, Loss: 13.190845956094563\n",
      "Epoch 542/1000, Loss: 13.741829925682396\n",
      "Epoch 543/1000, Loss: 15.006076547317207\n",
      "Epoch 544/1000, Loss: 11.434431412257254\n",
      "Epoch 545/1000, Loss: 11.064337206073105\n",
      "Epoch 546/1000, Loss: 9.805692875292152\n",
      "Epoch 547/1000, Loss: 12.128729393705726\n",
      "Epoch 548/1000, Loss: 10.452206695452332\n",
      "Epoch 549/1000, Loss: 10.919975503347814\n",
      "Epoch 550/1000, Loss: 11.178392942529172\n",
      "Epoch 551/1000, Loss: 11.569461134262383\n",
      "Epoch 552/1000, Loss: 11.15854230709374\n",
      "Epoch 553/1000, Loss: 11.340050166007131\n",
      "Epoch 554/1000, Loss: 11.958759140688926\n",
      "Epoch 555/1000, Loss: 12.059230124112219\n",
      "Epoch 556/1000, Loss: 13.043456505052745\n",
      "Epoch 557/1000, Loss: 12.61928738327697\n",
      "Epoch 558/1000, Loss: 10.82215799484402\n",
      "Epoch 559/1000, Loss: 12.119921549223363\n",
      "Epoch 560/1000, Loss: 11.096111175604165\n",
      "Epoch 561/1000, Loss: 11.962537520099431\n",
      "Epoch 562/1000, Loss: 10.877536511979997\n",
      "Epoch 563/1000, Loss: 12.184659535996616\n",
      "Epoch 564/1000, Loss: 16.79492030479014\n",
      "Epoch 565/1000, Loss: 12.280524658970535\n",
      "Epoch 566/1000, Loss: 11.282611881848425\n",
      "Epoch 567/1000, Loss: 10.504223430994898\n",
      "Epoch 568/1000, Loss: 10.253179143182933\n",
      "Epoch 569/1000, Loss: 10.024366923840716\n",
      "Epoch 570/1000, Loss: 10.144089070614427\n",
      "Epoch 571/1000, Loss: 12.033103706780821\n",
      "Epoch 572/1000, Loss: 12.472138363402337\n",
      "Epoch 573/1000, Loss: 11.406711210496724\n",
      "Epoch 574/1000, Loss: 11.350858257617801\n",
      "Epoch 575/1000, Loss: 11.772222956176847\n",
      "Epoch 576/1000, Loss: 16.164664648473263\n",
      "Epoch 577/1000, Loss: 14.000546688213944\n",
      "Epoch 578/1000, Loss: 11.914743260946125\n",
      "Epoch 579/1000, Loss: 11.362553877756\n",
      "Epoch 580/1000, Loss: 10.259787579532713\n",
      "Epoch 581/1000, Loss: 9.460209775716066\n",
      "Epoch 582/1000, Loss: 9.547996515408158\n",
      "Epoch 583/1000, Loss: 11.336208341643214\n",
      "Epoch 584/1000, Loss: 12.477480402216315\n",
      "Epoch 585/1000, Loss: 100.3424340444617\n",
      "Epoch 586/1000, Loss: 27.497831946238875\n",
      "Epoch 587/1000, Loss: 42.49117836076766\n",
      "Epoch 588/1000, Loss: 13.342808002606034\n",
      "Epoch 589/1000, Loss: 10.928879449842498\n",
      "Epoch 590/1000, Loss: 12.57275324780494\n",
      "Epoch 591/1000, Loss: 9.709814566653222\n",
      "Epoch 592/1000, Loss: 10.178818631917238\n",
      "Epoch 593/1000, Loss: 9.196079297689721\n",
      "Epoch 594/1000, Loss: 9.799925769679248\n",
      "Epoch 595/1000, Loss: 9.72249455167912\n",
      "Epoch 596/1000, Loss: 9.751437964849174\n",
      "Epoch 597/1000, Loss: 9.51228910125792\n",
      "Epoch 598/1000, Loss: 9.996341285295784\n",
      "Epoch 599/1000, Loss: 10.316172174643725\n",
      "Epoch 600/1000, Loss: 10.536091950722039\n",
      "Epoch 601/1000, Loss: 11.845554538536817\n",
      "Epoch 602/1000, Loss: 12.491190093569458\n",
      "Epoch 603/1000, Loss: 11.211045713629574\n",
      "Epoch 604/1000, Loss: 10.987949108239263\n",
      "Epoch 605/1000, Loss: 11.1213144576177\n",
      "Epoch 606/1000, Loss: 11.646294910926372\n",
      "Epoch 607/1000, Loss: 11.018707997631282\n",
      "Epoch 608/1000, Loss: 11.622988122049719\n",
      "Epoch 609/1000, Loss: 9.84174524154514\n",
      "Epoch 610/1000, Loss: 10.319667724892497\n",
      "Epoch 611/1000, Loss: 11.009740938898176\n",
      "Epoch 612/1000, Loss: 11.365493946243078\n",
      "Epoch 613/1000, Loss: 12.721281697973609\n",
      "Epoch 614/1000, Loss: 11.03052433906123\n",
      "Epoch 615/1000, Loss: 10.863751525059342\n",
      "Epoch 616/1000, Loss: 13.42680737003684\n",
      "Epoch 617/1000, Loss: 10.965194201562554\n",
      "Epoch 618/1000, Loss: 11.061878181062639\n",
      "Epoch 619/1000, Loss: 11.868703790009022\n",
      "Epoch 620/1000, Loss: 11.929581760894507\n",
      "Epoch 621/1000, Loss: 11.641044279094785\n",
      "Epoch 622/1000, Loss: 10.229775252286345\n",
      "Epoch 623/1000, Loss: 11.600289102178067\n",
      "Epoch 624/1000, Loss: 10.91590880881995\n",
      "Epoch 625/1000, Loss: 10.87463859282434\n",
      "Epoch 626/1000, Loss: 10.276370513252914\n",
      "Epoch 627/1000, Loss: 10.594259249046445\n",
      "Epoch 628/1000, Loss: 11.881616055034101\n",
      "Epoch 629/1000, Loss: 11.835905855987221\n",
      "Epoch 630/1000, Loss: 11.248289180453867\n",
      "Epoch 631/1000, Loss: 11.34974685544148\n",
      "Epoch 632/1000, Loss: 10.748642485588789\n",
      "Epoch 633/1000, Loss: 9.852661338634789\n",
      "Epoch 634/1000, Loss: 10.668985001277179\n",
      "Epoch 635/1000, Loss: 12.361588873434812\n",
      "Epoch 636/1000, Loss: 15.676547654904425\n",
      "Epoch 637/1000, Loss: 12.66397716384381\n",
      "Epoch 638/1000, Loss: 13.027744673658162\n",
      "Epoch 639/1000, Loss: 10.459678096696734\n",
      "Epoch 640/1000, Loss: 9.868901853216812\n",
      "Epoch 641/1000, Loss: 9.79783835588023\n",
      "Epoch 642/1000, Loss: 9.66077209962532\n",
      "Epoch 643/1000, Loss: 12.833217583596706\n",
      "Epoch 644/1000, Loss: 11.580210448708385\n",
      "Epoch 645/1000, Loss: 10.973087179474533\n",
      "Epoch 646/1000, Loss: 10.362639444414526\n",
      "Epoch 647/1000, Loss: 9.526018945034593\n",
      "Epoch 648/1000, Loss: 9.777530838735402\n",
      "Epoch 649/1000, Loss: 9.932173099834472\n",
      "Epoch 650/1000, Loss: 10.328584273578599\n",
      "Epoch 651/1000, Loss: 11.023979902267456\n",
      "Epoch 652/1000, Loss: 10.173285145312548\n",
      "Epoch 653/1000, Loss: 10.45307352207601\n",
      "Epoch 654/1000, Loss: 11.162840902339667\n",
      "Epoch 655/1000, Loss: 10.959838002221659\n",
      "Epoch 656/1000, Loss: 11.349139458499849\n",
      "Epoch 657/1000, Loss: 9.76815617736429\n",
      "Epoch 658/1000, Loss: 11.202654657419771\n",
      "Epoch 659/1000, Loss: 10.66462918650359\n",
      "Epoch 660/1000, Loss: 10.72957715485245\n",
      "Epoch 661/1000, Loss: 9.94374637096189\n",
      "Epoch 662/1000, Loss: 10.633852388709784\n",
      "Epoch 663/1000, Loss: 12.341434281785041\n",
      "Epoch 664/1000, Loss: 11.242955202236772\n",
      "Epoch 665/1000, Loss: 10.251534300390631\n",
      "Epoch 666/1000, Loss: 9.600124824326485\n",
      "Epoch 667/1000, Loss: 10.92387370672077\n",
      "Epoch 668/1000, Loss: 10.44050486665219\n",
      "Epoch 669/1000, Loss: 11.328869974706322\n",
      "Epoch 670/1000, Loss: 10.33571253856644\n",
      "Epoch 671/1000, Loss: 9.074198759626597\n",
      "Epoch 672/1000, Loss: 9.810449441894889\n",
      "Epoch 673/1000, Loss: 9.74887253693305\n",
      "Epoch 674/1000, Loss: 10.132361584343016\n",
      "Epoch 675/1000, Loss: 11.072944914456457\n",
      "Epoch 676/1000, Loss: 10.832230614498258\n",
      "Epoch 677/1000, Loss: 11.118666059337556\n",
      "Epoch 678/1000, Loss: 11.678292011842132\n",
      "Epoch 679/1000, Loss: 10.766909375786781\n",
      "Epoch 680/1000, Loss: 10.819275355432183\n",
      "Epoch 681/1000, Loss: 10.006865916308016\n",
      "Epoch 682/1000, Loss: 9.328912451164797\n",
      "Epoch 683/1000, Loss: 10.158879227470607\n",
      "Epoch 684/1000, Loss: 10.994257497601211\n",
      "Epoch 685/1000, Loss: 11.097881389781833\n",
      "Epoch 686/1000, Loss: 9.672272175783291\n",
      "Epoch 687/1000, Loss: 8.933468007482588\n",
      "Epoch 688/1000, Loss: 10.007502060383558\n",
      "Epoch 689/1000, Loss: 11.920752371195704\n",
      "Epoch 690/1000, Loss: 12.827473035547882\n",
      "Epoch 691/1000, Loss: 11.116923158988357\n",
      "Epoch 692/1000, Loss: 10.736085492186248\n",
      "Epoch 693/1000, Loss: 10.901972788386047\n",
      "Epoch 694/1000, Loss: 9.79859797284007\n",
      "Epoch 695/1000, Loss: 10.448663067538291\n",
      "Epoch 696/1000, Loss: 9.45553984772414\n",
      "Epoch 697/1000, Loss: 9.422875764314085\n",
      "Epoch 698/1000, Loss: 10.026329764863476\n",
      "Epoch 699/1000, Loss: 9.758636123500764\n",
      "Epoch 700/1000, Loss: 10.019764550961554\n",
      "Epoch 701/1000, Loss: 9.799089027103037\n",
      "Epoch 702/1000, Loss: 9.630120467860252\n",
      "Epoch 703/1000, Loss: 10.648319536820054\n",
      "Epoch 704/1000, Loss: 9.592946956166998\n",
      "Epoch 705/1000, Loss: 10.456110567320138\n",
      "Epoch 706/1000, Loss: 11.16366061475128\n",
      "Epoch 707/1000, Loss: 15.005745836067945\n",
      "Epoch 708/1000, Loss: 11.396919250488281\n",
      "Epoch 709/1000, Loss: 9.493204147554934\n",
      "Epoch 710/1000, Loss: 8.772777614183724\n",
      "Epoch 711/1000, Loss: 9.795899477321655\n",
      "Epoch 712/1000, Loss: 9.779791603097692\n",
      "Epoch 713/1000, Loss: 9.667127016931772\n",
      "Epoch 714/1000, Loss: 9.833665670827031\n",
      "Epoch 715/1000, Loss: 11.660927451215684\n",
      "Epoch 716/1000, Loss: 9.730008488055319\n",
      "Epoch 717/1000, Loss: 9.715658912900835\n",
      "Epoch 718/1000, Loss: 10.451837217900902\n",
      "Epoch 719/1000, Loss: 10.30659183068201\n",
      "Epoch 720/1000, Loss: 13.62615092471242\n",
      "Epoch 721/1000, Loss: 10.10812668246217\n",
      "Epoch 722/1000, Loss: 9.266237745294347\n",
      "Epoch 723/1000, Loss: 9.638563720975071\n",
      "Epoch 724/1000, Loss: 9.29994824831374\n",
      "Epoch 725/1000, Loss: 11.061396820936352\n",
      "Epoch 726/1000, Loss: 11.124529170803726\n",
      "Epoch 727/1000, Loss: 10.036389623768628\n",
      "Epoch 728/1000, Loss: 10.810396243585274\n",
      "Epoch 729/1000, Loss: 9.357992622302845\n",
      "Epoch 730/1000, Loss: 10.50891683390364\n",
      "Epoch 731/1000, Loss: 9.638974950881675\n",
      "Epoch 732/1000, Loss: 9.376194380223751\n",
      "Epoch 733/1000, Loss: 9.832678344566375\n",
      "Epoch 734/1000, Loss: 9.187955400208011\n",
      "Epoch 735/1000, Loss: 10.081307851709425\n",
      "Epoch 736/1000, Loss: 11.285074533428997\n",
      "Epoch 737/1000, Loss: 9.719090384431183\n",
      "Epoch 738/1000, Loss: 9.733175903791562\n",
      "Epoch 739/1000, Loss: 10.72043483541347\n",
      "Epoch 740/1000, Loss: 11.391186594963074\n",
      "Epoch 741/1000, Loss: 9.65961573063396\n",
      "Epoch 742/1000, Loss: 9.461027525831014\n",
      "Epoch 743/1000, Loss: 10.011437448207289\n",
      "Epoch 744/1000, Loss: 10.182830629404634\n",
      "Epoch 745/1000, Loss: 90.37802042718977\n",
      "Epoch 746/1000, Loss: 18.73261077143252\n",
      "Epoch 747/1000, Loss: 13.783929858356714\n",
      "Epoch 748/1000, Loss: 11.853612925391644\n",
      "Epoch 749/1000, Loss: 9.144680638797581\n",
      "Epoch 750/1000, Loss: 9.167856695596129\n",
      "Epoch 751/1000, Loss: 9.072995459893718\n",
      "Epoch 752/1000, Loss: 8.998425537720323\n",
      "Epoch 753/1000, Loss: 9.985031622461975\n",
      "Epoch 754/1000, Loss: 9.01776127028279\n",
      "Epoch 755/1000, Loss: 9.935835258336738\n",
      "Epoch 756/1000, Loss: 9.593726251274347\n",
      "Epoch 757/1000, Loss: 9.13832637714222\n",
      "Epoch 758/1000, Loss: 9.748029523529112\n",
      "Epoch 759/1000, Loss: 10.824379770085216\n",
      "Epoch 760/1000, Loss: 10.590822017751634\n",
      "Epoch 761/1000, Loss: 11.947581198066473\n",
      "Epoch 762/1000, Loss: 15.169588084332645\n",
      "Epoch 763/1000, Loss: 9.613450154894963\n",
      "Epoch 764/1000, Loss: 8.69711259426549\n",
      "Epoch 765/1000, Loss: 8.351134990807623\n",
      "Epoch 766/1000, Loss: 8.43477402953431\n",
      "Epoch 767/1000, Loss: 9.019639471778646\n",
      "Epoch 768/1000, Loss: 8.985553106758744\n",
      "Epoch 769/1000, Loss: 9.08090998372063\n",
      "Epoch 770/1000, Loss: 9.699564669979736\n",
      "Epoch 771/1000, Loss: 10.226403575856239\n",
      "Epoch 772/1000, Loss: 10.197951721493155\n",
      "Epoch 773/1000, Loss: 9.892357848584652\n",
      "Epoch 774/1000, Loss: 11.554910216946155\n",
      "Epoch 775/1000, Loss: 11.337492544669658\n",
      "Epoch 776/1000, Loss: 9.80980839440599\n",
      "Epoch 777/1000, Loss: 10.033073284197599\n",
      "Epoch 778/1000, Loss: 10.732759739272296\n",
      "Epoch 779/1000, Loss: 10.816078648902476\n",
      "Epoch 780/1000, Loss: 10.752952976617962\n",
      "Epoch 781/1000, Loss: 10.080573262879625\n",
      "Epoch 782/1000, Loss: 9.743889402132481\n",
      "Epoch 783/1000, Loss: 8.84642465901561\n",
      "Epoch 784/1000, Loss: 8.850351623492315\n",
      "Epoch 785/1000, Loss: 9.968814380001277\n",
      "Epoch 786/1000, Loss: 9.794309589779004\n",
      "Epoch 787/1000, Loss: 9.655884956009686\n",
      "Epoch 788/1000, Loss: 11.787772914394736\n",
      "Epoch 789/1000, Loss: 10.193498905748129\n",
      "Epoch 790/1000, Loss: 11.559619093779474\n",
      "Epoch 791/1000, Loss: 11.811844694428146\n",
      "Epoch 792/1000, Loss: 9.915791208855808\n",
      "Epoch 793/1000, Loss: 9.607490366557613\n",
      "Epoch 794/1000, Loss: 9.601893283193931\n",
      "Epoch 795/1000, Loss: 9.139110778691247\n",
      "Epoch 796/1000, Loss: 15.326655741315335\n",
      "Epoch 797/1000, Loss: 10.758710633497685\n",
      "Epoch 798/1000, Loss: 9.275689104571939\n",
      "Epoch 799/1000, Loss: 9.573482569307089\n",
      "Epoch 800/1000, Loss: 8.93737966241315\n",
      "Epoch 801/1000, Loss: 8.780812031589448\n",
      "Epoch 802/1000, Loss: 8.80670243082568\n",
      "Epoch 803/1000, Loss: 9.024758960818872\n",
      "Epoch 804/1000, Loss: 9.505265844520181\n",
      "Epoch 805/1000, Loss: 9.691480290610343\n",
      "Epoch 806/1000, Loss: 10.24680473189801\n",
      "Epoch 807/1000, Loss: 11.418270656373352\n",
      "Epoch 808/1000, Loss: 12.70213700295426\n",
      "Epoch 809/1000, Loss: 10.45149840740487\n",
      "Epoch 810/1000, Loss: 8.6766261709854\n",
      "Epoch 811/1000, Loss: 9.474574971944094\n",
      "Epoch 812/1000, Loss: 11.869218597188592\n",
      "Epoch 813/1000, Loss: 8.614454258000478\n",
      "Epoch 814/1000, Loss: 8.394193269778043\n",
      "Epoch 815/1000, Loss: 9.65091174049303\n",
      "Epoch 816/1000, Loss: 8.609494473552331\n",
      "Epoch 817/1000, Loss: 9.107007668586448\n",
      "Epoch 818/1000, Loss: 10.504974304698408\n",
      "Epoch 819/1000, Loss: 9.974816455040127\n",
      "Epoch 820/1000, Loss: 9.16311061475426\n",
      "Epoch 821/1000, Loss: 9.740361202508211\n",
      "Epoch 822/1000, Loss: 9.980757446493953\n",
      "Epoch 823/1000, Loss: 10.077970401383936\n",
      "Epoch 824/1000, Loss: 9.269914556993172\n",
      "Epoch 825/1000, Loss: 9.547255704645067\n",
      "Epoch 826/1000, Loss: 9.581790806958452\n",
      "Epoch 827/1000, Loss: 10.134505741298199\n",
      "Epoch 828/1000, Loss: 10.769608417060226\n",
      "Epoch 829/1000, Loss: 10.086511579342186\n",
      "Epoch 830/1000, Loss: 12.481239723041654\n",
      "Epoch 831/1000, Loss: 9.896663170307875\n",
      "Epoch 832/1000, Loss: 9.305048830807209\n",
      "Epoch 833/1000, Loss: 9.081560556776822\n",
      "Epoch 834/1000, Loss: 8.568823934998363\n",
      "Epoch 835/1000, Loss: 8.827763926237822\n",
      "Epoch 836/1000, Loss: 9.184831675840542\n",
      "Epoch 837/1000, Loss: 9.775174506474286\n",
      "Epoch 838/1000, Loss: 11.299085090402514\n",
      "Epoch 839/1000, Loss: 10.994454384315759\n",
      "Epoch 840/1000, Loss: 9.84708599280566\n",
      "Epoch 841/1000, Loss: 8.686099604004994\n",
      "Epoch 842/1000, Loss: 8.915923801250756\n",
      "Epoch 843/1000, Loss: 9.371410509105772\n",
      "Epoch 844/1000, Loss: 10.151438129832968\n",
      "Epoch 845/1000, Loss: 9.485723637975752\n",
      "Epoch 846/1000, Loss: 10.586741620209068\n",
      "Epoch 847/1000, Loss: 9.507438571192324\n",
      "Epoch 848/1000, Loss: 10.098967428319156\n",
      "Epoch 849/1000, Loss: 9.587767677847296\n",
      "Epoch 850/1000, Loss: 10.103512634523213\n",
      "Epoch 851/1000, Loss: 9.11031572939828\n",
      "Epoch 852/1000, Loss: 9.482876447029412\n",
      "Epoch 853/1000, Loss: 9.268589776474983\n",
      "Epoch 854/1000, Loss: 9.19144649617374\n",
      "Epoch 855/1000, Loss: 9.780045753112063\n",
      "Epoch 856/1000, Loss: 9.241027883719653\n",
      "Epoch 857/1000, Loss: 8.905923263635486\n",
      "Epoch 858/1000, Loss: 8.520720987813547\n",
      "Epoch 859/1000, Loss: 9.268138020532206\n",
      "Epoch 860/1000, Loss: 8.740134837571532\n",
      "Epoch 861/1000, Loss: 9.672682277159765\n",
      "Epoch 862/1000, Loss: 9.927571373293176\n",
      "Epoch 863/1000, Loss: 9.691915401956066\n",
      "Epoch 864/1000, Loss: 10.856492170598358\n",
      "Epoch 865/1000, Loss: 9.962231780402362\n",
      "Epoch 866/1000, Loss: 9.335133631248027\n",
      "Epoch 867/1000, Loss: 9.044699289603159\n",
      "Epoch 868/1000, Loss: 9.527050913777202\n",
      "Epoch 869/1000, Loss: 9.226880931761116\n",
      "Epoch 870/1000, Loss: 9.463416930288076\n",
      "Epoch 871/1000, Loss: 10.170776541344821\n",
      "Epoch 872/1000, Loss: 9.389967101160437\n",
      "Epoch 873/1000, Loss: 9.830394624732435\n",
      "Epoch 874/1000, Loss: 10.481873850803822\n",
      "Epoch 875/1000, Loss: 9.511693388223648\n",
      "Epoch 876/1000, Loss: 9.01812838530168\n",
      "Epoch 877/1000, Loss: 8.543305239873007\n",
      "Epoch 878/1000, Loss: 10.301348254550248\n",
      "Epoch 879/1000, Loss: 9.833860756829381\n",
      "Epoch 880/1000, Loss: 9.515918338205665\n",
      "Epoch 881/1000, Loss: 9.858882143395022\n",
      "Epoch 882/1000, Loss: 9.386580935679376\n",
      "Epoch 883/1000, Loss: 9.75179173075594\n",
      "Epoch 884/1000, Loss: 9.538190072169527\n",
      "Epoch 885/1000, Loss: 9.829363546334207\n",
      "Epoch 886/1000, Loss: 10.221507444046438\n",
      "Epoch 887/1000, Loss: 8.87793047633022\n",
      "Epoch 888/1000, Loss: 10.967071549966931\n",
      "Epoch 889/1000, Loss: 10.925168734509498\n",
      "Epoch 890/1000, Loss: 9.255092228064314\n",
      "Epoch 891/1000, Loss: 9.230745400302112\n",
      "Epoch 892/1000, Loss: 8.688409857917577\n",
      "Epoch 893/1000, Loss: 9.267400272423401\n",
      "Epoch 894/1000, Loss: 10.947683868464082\n",
      "Epoch 895/1000, Loss: 9.911177170928568\n",
      "Epoch 896/1000, Loss: 9.61606743838638\n",
      "Epoch 897/1000, Loss: 9.311799784656614\n",
      "Epoch 898/1000, Loss: 8.317125636618584\n",
      "Epoch 899/1000, Loss: 9.82597662252374\n",
      "Epoch 900/1000, Loss: 9.734437106642872\n",
      "Epoch 901/1000, Loss: 8.726950029376894\n",
      "Epoch 902/1000, Loss: 8.753240269841626\n",
      "Epoch 903/1000, Loss: 9.27319647022523\n",
      "Epoch 904/1000, Loss: 9.896775944624096\n",
      "Epoch 905/1000, Loss: 10.293409760575742\n",
      "Epoch 906/1000, Loss: 9.320255417842418\n",
      "Epoch 907/1000, Loss: 9.147291248198599\n",
      "Epoch 908/1000, Loss: 9.212660625576973\n",
      "Epoch 909/1000, Loss: 9.89605415519327\n",
      "Epoch 910/1000, Loss: 11.654854170745239\n",
      "Epoch 911/1000, Loss: 11.521375187672675\n",
      "Epoch 912/1000, Loss: 12.594781608786434\n",
      "Epoch 913/1000, Loss: 9.722056184895337\n",
      "Epoch 914/1000, Loss: 9.073579367948696\n",
      "Epoch 915/1000, Loss: 7.882469249190763\n",
      "Epoch 916/1000, Loss: 8.776816190453246\n",
      "Epoch 917/1000, Loss: 9.255338829243556\n",
      "Epoch 918/1000, Loss: 8.632011437322944\n",
      "Epoch 919/1000, Loss: 8.402826835168526\n",
      "Epoch 920/1000, Loss: 9.15433650626801\n",
      "Epoch 921/1000, Loss: 9.856559654464945\n",
      "Epoch 922/1000, Loss: 8.859222885221243\n",
      "Epoch 923/1000, Loss: 8.917216064408422\n",
      "Epoch 924/1000, Loss: 9.927986323367804\n",
      "Epoch 925/1000, Loss: 9.216509831836447\n",
      "Epoch 926/1000, Loss: 9.181933613028377\n",
      "Epoch 927/1000, Loss: 9.409101037308574\n",
      "Epoch 928/1000, Loss: 8.42522421805188\n",
      "Epoch 929/1000, Loss: 8.750796826556325\n",
      "Epoch 930/1000, Loss: 9.273813880048692\n",
      "Epoch 931/1000, Loss: 9.838619065470994\n",
      "Epoch 932/1000, Loss: 9.327480626059696\n",
      "Epoch 933/1000, Loss: 9.94085452426225\n",
      "Epoch 934/1000, Loss: 10.3699802858755\n",
      "Epoch 935/1000, Loss: 9.635473549365997\n",
      "Epoch 936/1000, Loss: 8.995263324584812\n",
      "Epoch 937/1000, Loss: 10.636341344099492\n",
      "Epoch 938/1000, Loss: 9.667964073596522\n",
      "Epoch 939/1000, Loss: 9.50322061451152\n",
      "Epoch 940/1000, Loss: 9.547162525355816\n",
      "Epoch 941/1000, Loss: 10.266335353488103\n",
      "Epoch 942/1000, Loss: 10.203377143014222\n",
      "Epoch 943/1000, Loss: 9.663051649462432\n",
      "Epoch 944/1000, Loss: 8.773652227595448\n",
      "Epoch 945/1000, Loss: 8.558137186802924\n",
      "Epoch 946/1000, Loss: 9.349043789319694\n",
      "Epoch 947/1000, Loss: 9.020775517215952\n",
      "Epoch 948/1000, Loss: 9.929916426772252\n",
      "Epoch 949/1000, Loss: 12.053224639967084\n",
      "Epoch 950/1000, Loss: 12.33511618617922\n",
      "Epoch 951/1000, Loss: 10.53873237920925\n",
      "Epoch 952/1000, Loss: 8.561758901691064\n",
      "Epoch 953/1000, Loss: 8.623889363603666\n",
      "Epoch 954/1000, Loss: 8.158520779106766\n",
      "Epoch 955/1000, Loss: 7.756452600704506\n",
      "Epoch 956/1000, Loss: 7.913500540424138\n",
      "Epoch 957/1000, Loss: 8.20520015526563\n",
      "Epoch 958/1000, Loss: 9.266870869789273\n",
      "Epoch 959/1000, Loss: 9.308644989039749\n",
      "Epoch 960/1000, Loss: 8.406026020180434\n",
      "Epoch 961/1000, Loss: 9.658319553826004\n",
      "Epoch 962/1000, Loss: 9.6283093476668\n",
      "Epoch 963/1000, Loss: 10.134743202012032\n",
      "Epoch 964/1000, Loss: 9.329608702100813\n",
      "Epoch 965/1000, Loss: 8.516125396359712\n",
      "Epoch 966/1000, Loss: 8.64958316553384\n",
      "Epoch 967/1000, Loss: 10.42932386440225\n",
      "Epoch 968/1000, Loss: 10.304027116391808\n",
      "Epoch 969/1000, Loss: 11.90480148093775\n",
      "Epoch 970/1000, Loss: 10.442004155367613\n",
      "Epoch 971/1000, Loss: 8.758215747307986\n",
      "Epoch 972/1000, Loss: 8.437649000203237\n",
      "Epoch 973/1000, Loss: 10.006256561726332\n",
      "Epoch 974/1000, Loss: 8.631481787422672\n",
      "Epoch 975/1000, Loss: 9.30394053645432\n",
      "Epoch 976/1000, Loss: 10.535943577997386\n",
      "Epoch 977/1000, Loss: 9.216471374267712\n",
      "Epoch 978/1000, Loss: 8.423157583456486\n",
      "Epoch 979/1000, Loss: 8.274143162183464\n",
      "Epoch 980/1000, Loss: 8.680951406713575\n",
      "Epoch 981/1000, Loss: 8.496255768463016\n",
      "Epoch 982/1000, Loss: 8.948462783591822\n",
      "Epoch 983/1000, Loss: 10.881438955897465\n",
      "Epoch 984/1000, Loss: 9.69560436392203\n",
      "Epoch 985/1000, Loss: 8.816240287385881\n",
      "Epoch 986/1000, Loss: 8.414618538459763\n",
      "Epoch 987/1000, Loss: 8.207979116938077\n",
      "Epoch 988/1000, Loss: 8.439119641669095\n",
      "Epoch 989/1000, Loss: 9.255498286569491\n",
      "Epoch 990/1000, Loss: 8.95419705635868\n",
      "Epoch 991/1000, Loss: 9.69519459689036\n",
      "Epoch 992/1000, Loss: 9.649054477456957\n",
      "Epoch 993/1000, Loss: 9.064846870489419\n",
      "Epoch 994/1000, Loss: 12.839101777411997\n",
      "Epoch 995/1000, Loss: 10.583577231504023\n",
      "Epoch 996/1000, Loss: 9.911998859141022\n",
      "Epoch 997/1000, Loss: 8.234986984170973\n",
      "Epoch 998/1000, Loss: 8.579026326769963\n",
      "Epoch 999/1000, Loss: 8.216355495620519\n",
      "Epoch 1000/1000, Loss: 7.902829591650516\n",
      "Test Score (R^2): 0.7358247667031574\n"
     ]
    }
   ],
   "source": [
    "#### CREATING MODEL ####\n",
    "\n",
    "# Define the neural network architecture with multiple hidden layers\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        layers = []\n",
    "        previous_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(previous_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            previous_dim = hidden_dim\n",
    "        layers.append(nn.Linear(previous_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.activation = nn.ReLU() #best according to grid search\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_data.shape[1]  # Number of molecular descriptors\n",
    "hidden_dim = [512,512,512]  # (512,512,512) Best according to grid search\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function (nn.MSELoss) and optimizer\n",
    "model = SolubilityPredictor(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "test_score = r2_score(y_test, y_pred)\n",
    "print(\"Test Score (R^2):\", test_score)\n",
    "\n",
    "#maybe make plots of training and validation loss to see how it is learning\n",
    "#can help identify overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1.8490358591079712\n",
      "Test RMSE: 1.3597925901412964\n",
      "Test MAE: 0.7881057262420654\n",
      "Test R^2: 0.7358247667031574\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Perform the forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    test_predictions_tensor = model(X_test_tensor)\n",
    "\n",
    "# Convert predictions back to NumPy array\n",
    "test_predictions = test_predictions_tensor.numpy().flatten()\n",
    "\n",
    "# Calculate additional metrics\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With dropout, weight decay\n",
    "try dropout rates between 0.2 and 0.5. start with 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 681.7832328081131\n",
      "Epoch 101/1000, Loss: 111.76737007498741\n",
      "Epoch 201/1000, Loss: 74.74509523808956\n",
      "Epoch 301/1000, Loss: 64.90365134179592\n",
      "Epoch 401/1000, Loss: 56.54144813865423\n",
      "Epoch 501/1000, Loss: 55.897232234478\n",
      "Epoch 601/1000, Loss: 51.54749147221446\n",
      "Epoch 701/1000, Loss: 52.32152093201876\n",
      "Epoch 801/1000, Loss: 46.192381370812654\n",
      "Epoch 901/1000, Loss: 48.13287205994129\n",
      "Test MSE: 1.2193962335586548\n",
      "Test RMSE: 1.1042627096176147\n",
      "Test MAE: 0.695110559463501\n",
      "Test R^2: 0.7752118032844304\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network architecture with multiple hidden layers and dropout\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        layers = []\n",
    "        previous_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(previous_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))  # Adding dropout\n",
    "            previous_dim = hidden_dim\n",
    "        layers.append(nn.Linear(previous_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_data.shape[1]  # Number of molecular descriptors\n",
    "hidden_dims = [512, 512, 512]  # Best according to grid search\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function (nn.MSELoss) and optimizer with weight decay (L2 regularization)\n",
    "model = SolubilityPredictor(input_dim, hidden_dims, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy().flatten()\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'solubility_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to add extra layers and freeze others? or unfreeze some\n",
    "can also try to make it classification instead of regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = bio_avail_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### DATA PREPARATION ####\n",
    "# try with new data\n",
    "\n",
    "#remove nan values from data\n",
    "data = data.dropna(subset=['%F'])\n",
    "\n",
    "#get x and y data (x is the molecular descriptors, y is the solubility)\n",
    "y_data = data[\"%F\"]\n",
    "\n",
    "print(len(y_data))\n",
    "\n",
    "#generating features and making sure it's the same features as previous model\n",
    "smiles = data[\"Updated SMILES\"]\n",
    "featurizer = RDKitDescriptors()\n",
    "features = featurizer.featurize(smiles)\n",
    "X_data = features[:,used_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_data.shape[1])\n",
    "if True in np.isnan(X_data):\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.939842104248825"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#split data into training and validation using train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "# Convert y pandas Series to NumPy array\n",
    "y_train = y_train.values.astype(np.float32)\n",
    "y_test = y_test.values.astype(np.float32)\n",
    "\n",
    "#scale x values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#convert data to pytorch tensors (like numpy arrays but for pytorch)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Reshape the target tensor to match the shape of the output tensor\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "# Reshape the target tensor to match the shape of the output tensor\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define model again\n",
    "\n",
    "# Define the neural network architecture with multiple hidden layers\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        layers = []\n",
    "        previous_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(previous_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            previous_dim = hidden_dim\n",
    "        layers.append(nn.Linear(previous_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.activation = nn.ReLU() #best according to grid search\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_data.shape[1]  # Number of molecular descriptors\n",
    "hidden_dim = [512,512,512]  # (512,512,512) Best according to grid search\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "#loading saved model\n",
    "model = SolubilityPredictor(input_dim, hidden_dim, output_dim)\n",
    "model.load_state_dict(torch.load('solubility_model.pth'))\n",
    "\n",
    "# Optionally, modify the final layer if the output dimension is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BioavailabilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(BioavailabilityPredictor, self).__init__()\n",
    "        self.base_model = SolubilityPredictor(input_dim, hidden_dims, output_dim)\n",
    "        \n",
    "        # New layers for bioavailability prediction\n",
    "        self.new_layers = nn.Sequential(\n",
    "            nn.Linear(output_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.new_layers(x)\n",
    "        return x\n",
    "\n",
    "# Create the new model\n",
    "bioavailability_model = BioavailabilityPredictor(input_dim, hidden_dim, output_dim)\n",
    "bioavailability_model.base_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initially freeze the pre-trained layers\n",
    "for param in bioavailability_model.base_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 1196.350540280342\n",
      "Epoch 101/1000, Loss: 967.0957604050636\n",
      "Epoch 201/1000, Loss: 967.3271186351776\n",
      "Epoch 301/1000, Loss: 965.6701622009277\n",
      "Epoch 401/1000, Loss: 965.4490968585014\n",
      "Epoch 501/1000, Loss: 965.169162273407\n",
      "Epoch 601/1000, Loss: 965.6898025870323\n",
      "Epoch 701/1000, Loss: 966.0286057591438\n",
      "Epoch 801/1000, Loss: 965.6987157464027\n",
      "Epoch 901/1000, Loss: 964.937452673912\n"
     ]
    }
   ],
   "source": [
    "# train only the new layers added for the task:\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(bioavailability_model.new_layers.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set the model to training mode\n",
    "bioavailability_model.train()\n",
    "\n",
    "# Training loop for new layers\n",
    "for epoch in range(num_epochs):\n",
    "    bioavailability_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = bioavailability_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 3727.35498046875\n",
      "Test RMSE: 61.052066802978516\n",
      "Test MAE: 50.10044479370117\n",
      "Test R^2: -2.0400523087652602\n"
     ]
    }
   ],
   "source": [
    "bioavailability_model.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Perform the forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    test_predictions_tensor = bioavailability_model(X_test_tensor)\n",
    "\n",
    "# Convert predictions back to NumPy array\n",
    "test_predictions = test_predictions_tensor.numpy().flatten()\n",
    "\n",
    "# Calculate additional metrics\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 914.3759000301361\n",
      "Epoch 101/1000, Loss: 66.055250890553\n",
      "Epoch 201/1000, Loss: 20.716699776239693\n",
      "Epoch 301/1000, Loss: 12.415890370961279\n",
      "Epoch 401/1000, Loss: 9.232166432309896\n",
      "Epoch 501/1000, Loss: 7.7701833446044475\n",
      "Epoch 601/1000, Loss: 6.864622933673672\n",
      "Epoch 701/1000, Loss: 6.276480855362024\n",
      "Epoch 801/1000, Loss: 5.79789643723052\n",
      "Epoch 901/1000, Loss: 5.296067177318037\n"
     ]
    }
   ],
   "source": [
    "#fine-tune it by unfreezing some of the pre-trained layers\n",
    "\n",
    "# Unfreeze some of the pre-trained layers\n",
    "for param in bioavailability_model.base_model.network[-3].parameters():  # Unfreeze the second last layer as an example\n",
    "    param.requires_grad = True\n",
    "for param in bioavailability_model.base_model.network[-5].parameters():  # Unfreeze the third last layer (optional)\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Update the optimizer to include the parameters of the unfrozen layers\n",
    "optimizer = optim.Adam(bioavailability_model.parameters(), lr=learning_rate / 10)\n",
    "\n",
    "#maybe 1000 epochs are not needed\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(num_epochs):\n",
    "    bioavailability_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = bioavailability_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 3782.56396484375\n",
      "Test RMSE: 61.5025520324707\n",
      "Test MAE: 50.522132873535156\n",
      "Test R^2: -2.0850813379729947\n"
     ]
    }
   ],
   "source": [
    "bioavailability_model.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Perform the forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    test_predictions_tensor = bioavailability_model(X_test_tensor)\n",
    "\n",
    "# Convert predictions back to NumPy array\n",
    "test_predictions = test_predictions_tensor.numpy().flatten()\n",
    "\n",
    "# Calculate additional metrics\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
