{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article from Philippe:\n",
    "https://practicalcheminformatics.blogspot.com/2023/06/getting-real-with-molecular-property.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "c:\\Users\\nina\\anaconda3\\envs\\ai_chem\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (c:\\Users\\nina\\anaconda3\\envs\\ai_chem\\Lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from deepchem.feat import RDKitDescriptors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SMILES', 'Solubility_log(mol/L)', 'Solubility(mol/L)', 'MolW(Da)',\n",
      "       'NumHAcceptors', 'NumHDonors', 'LogP', 'Lipinski_rule'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#combined data\n",
    "#Merged_2 has curated, biogen and esol data. merged_solubility also has one other\n",
    "data = pd.read_csv(\"../Data/Merged_solubility.csv\")\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disable warnings from RDKit\n",
    "from rdkit import RDLogger \n",
    "RDLogger.DisableLog('rdApp.*') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize(Dataframe: pd.DataFrame):\n",
    "    \n",
    "    \"\"\"Canonicalizes the SMILES from Dataframe. A column called 'SMILES' is requiered\n",
    "\n",
    "    Args: Dataframe with 'SMILES' column contaning smiles. \n",
    "    \"\"\"\n",
    "    \n",
    "    Dataframe['SMILES'] = Dataframe['SMILES'].apply(lambda x: Chem.MolToSmiles(Chem.MolFromSmiles(x))) #canonicalize smiles from a Dataframe                                          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonicalize(data)\n",
    "data = data.drop_duplicates(subset=\"SMILES\", keep='first') #prioritizes curated, biogen, then esol and then pharmaceutical database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12309\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to generate features from SMILES strings using RDKit descriptors\n",
    "def generate_features(smiles_list):\n",
    "    featurizer = RDKitDescriptors()\n",
    "    features = featurizer.featurize(smiles_list)\n",
    "    # Drop features containing invalid values\n",
    "    used_features = ~np.isnan(features).any(axis=0)\n",
    "    features = features[:, ~np.isnan(features).any(axis=0)]\n",
    "    return features, used_features\n",
    "\n",
    "#remove nan values from data\n",
    "data = data.dropna(subset=['Solubility_log(mol/L)'])\n",
    "\n",
    "#get x and y data (x is the molecular descriptors, y is the solubility)\n",
    "y_data = data[\"Solubility_log(mol/L)\"]\n",
    "\n",
    "print(len(y_data))\n",
    "\n",
    "# Generate features from SMILES data (get smiles from df)\n",
    "smiles = data[\"SMILES\"]\n",
    "X_data, used_features = generate_features(smiles)\n",
    "\n",
    "#split data into training and validation using train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "# Convert y pandas Series to NumPy array\n",
    "y_train = y_train.values.astype(np.float32)\n",
    "y_test = y_test.values.astype(np.float32)\n",
    "\n",
    "#scale x values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#convert data to pytorch tensors (like numpy arrays but for pytorch)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Reshape the target tensor to match the shape of the output tensor\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "# Reshape the target tensor to match the shape of the output tensor\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.237340946572077"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation function:\n",
    "https://encord.com/blog/activation-functions-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 4.5min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 5.4min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 3.5min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 4.2min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 1.9min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.001; total time= 6.8min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.001; total time=10.4min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.001; total time= 7.5min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.001; total time=29.6min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.001; total time= 8.9min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.0001; total time=14.4min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.0001; total time=11.1min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.0001; total time= 7.8min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.0001; total time=16.4min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.0001; total time=12.6min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.01; total time=  32.3s\n",
      "[CV] END activation=relu, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.01; total time= 1.7min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.01; total time= 1.5min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.01; total time= 1.4min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.01; total time= 1.5min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.001; total time= 1.7min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.001; total time= 5.0min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.001; total time=13.8min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.001; total time= 1.3min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.001; total time= 1.4min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.0001; total time= 1.8min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.0001; total time= 2.0min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.0001; total time= 2.5min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.0001; total time= 2.5min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.0001; total time= 2.2min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.01; total time=19.1min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.01; total time= 2.1min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.01; total time= 5.0min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.01; total time= 4.2min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.01; total time= 8.3min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.001; total time=13.6min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.001; total time=10.9min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.001; total time=14.9min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.001; total time= 8.5min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.001; total time=57.5min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.0001; total time= 8.6min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.0001; total time=11.9min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.0001; total time=13.0min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.0001; total time=16.0min\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.0001; total time=15.7min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 1.0min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 1.4min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 1.1min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 1.3min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 1.0min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512), learning_rate_init=0.001; total time= 4.7min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512), learning_rate_init=0.001; total time= 5.4min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512), learning_rate_init=0.001; total time= 7.4min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512), learning_rate_init=0.001; total time= 5.8min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512), learning_rate_init=0.001; total time= 7.1min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512), learning_rate_init=0.0001; total time=16.1min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512), learning_rate_init=0.0001; total time=17.7min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512), learning_rate_init=0.0001; total time=19.1min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512), learning_rate_init=0.0001; total time=19.9min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512), learning_rate_init=0.0001; total time=21.7min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.01; total time=  25.5s\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.01; total time=  29.1s\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.01; total time=  22.3s\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.01; total time=  25.7s\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.01; total time=  29.0s\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.001; total time= 1.9min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.001; total time= 1.5min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.001; total time= 1.2min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.001; total time= 1.5min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.001; total time= 2.5min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.0001; total time= 5.4min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.0001; total time= 4.8min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.0001; total time= 5.6min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.0001; total time= 5.5min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.0001; total time= 5.1min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.01; total time= 3.0min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.01; total time= 3.5min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.01; total time= 4.1min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.01; total time= 3.6min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.01; total time= 3.0min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.001; total time=35.1min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.001; total time=10.1min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.001; total time= 7.1min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.001; total time=11.0min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.001; total time=10.1min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.0001; total time=32.1min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.0001; total time=36.3min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.0001; total time=33.1min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.0001; total time=25.2min\n",
      "[CV] END activation=tanh, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.0001; total time=28.8min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 3.0min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 3.9min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 3.6min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 2.9min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 2.8min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512), learning_rate_init=0.001; total time= 9.4min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512), learning_rate_init=0.001; total time=13.9min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512), learning_rate_init=0.001; total time= 9.6min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512), learning_rate_init=0.001; total time=11.6min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512), learning_rate_init=0.001; total time=11.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nina\\anaconda3\\envs\\ai_chem\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512), learning_rate_init=0.0001; total time=36.9min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512), learning_rate_init=0.0001; total time=29.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nina\\anaconda3\\envs\\ai_chem\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512), learning_rate_init=0.0001; total time=37.0min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512), learning_rate_init=0.0001; total time=35.1min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512), learning_rate_init=0.0001; total time=33.0min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.01; total time= 1.3min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.01; total time=  59.9s\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.01; total time= 1.1min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.01; total time= 1.2min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.01; total time=  53.4s\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.001; total time= 3.5min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.001; total time= 3.9min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.001; total time= 2.7min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.001; total time= 4.1min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.001; total time= 3.3min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.0001; total time= 9.1min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.0001; total time= 8.7min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.0001; total time=10.1min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.0001; total time=10.0min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(256, 256, 256), learning_rate_init=0.0001; total time= 8.2min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.01; total time= 6.8min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.01; total time= 5.4min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.01; total time= 5.0min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.01; total time= 5.9min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.01; total time= 5.3min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.001; total time=17.7min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.001; total time=20.3min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.001; total time=17.6min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.001; total time=22.2min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.001; total time=22.6min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.0001; total time=58.3min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.0001; total time=44.0min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.0001; total time=40.8min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.0001; total time=60.9min\n",
      "[CV] END activation=logistic, hidden_layer_sizes=(512, 512, 512), learning_rate_init=0.0001; total time=58.3min\n",
      "Best Hyperparameters: {'activation': 'tanh', 'hidden_layer_sizes': (512, 512, 512), 'learning_rate_init': 0.001}\n",
      "Test Score (R^2): 0.7920846939140014\n"
     ]
    }
   ],
   "source": [
    "#### CREATING MODEL ####\n",
    "\n",
    "# Define the neural network architecture\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_data.shape[1]  # Number of molecular descriptors\n",
    "hidden_dim = 256 #gets overruled by grid search\n",
    "output_dim = 1\n",
    "learning_rate = 0.001 #gets overruled by the grid search\n",
    "num_epochs = 600\n",
    "batch_size = 32\n",
    "\n",
    "#create dataloader for batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#initialize model, loss function (nn.MSELoss) and optimizer\n",
    "model = SolubilityPredictor(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "##### Grid-search for best hyperparameters #####\n",
    "# Define your hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(512,512),(256,256,256),(512,512,512)],  # Number of neurons in the hidden layer(s)\n",
    "    'activation': ['relu', 'tanh','logistic'],  # Activation function. logistic is same as sigmoid\n",
    "    #'solver': ['adam', 'sgd'],  # Optimization algorithm\n",
    "    #'learning_rate': ['constant', 'adaptive'],  # Learning rate schedule\n",
    "    'learning_rate_init':[0.01,0.001,0.0001]\n",
    "}\n",
    "\n",
    "# Create an MLPRegressor object\n",
    "mlp = MLPRegressor(max_iter=num_epochs, batch_size=batch_size)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_score = model.score(X_test, y_test)\n",
    "print(\"Test Score (R^2):\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1.0550654999961255\n",
      "Test RMSE: 1.0271638136130603\n",
      "Test MAE: 0.6723179912846099\n",
      "Test R^2: 0.7920846939140014\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate additional metrics\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_mlp_model.pkl']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "joblib.dump(best_model, 'best_mlp_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800, Loss: 737.8418161571026\n",
      "Epoch 2/800, Loss: 430.92745900154114\n",
      "Epoch 3/800, Loss: 383.20267313718796\n",
      "Epoch 4/800, Loss: 342.50970643758774\n",
      "Epoch 5/800, Loss: 339.3627875447273\n",
      "Epoch 6/800, Loss: 304.23266220092773\n",
      "Epoch 7/800, Loss: 281.53028905391693\n",
      "Epoch 8/800, Loss: 267.02549183368683\n",
      "Epoch 9/800, Loss: 241.57228228449821\n",
      "Epoch 10/800, Loss: 235.14435601234436\n",
      "Epoch 11/800, Loss: 222.03872872889042\n",
      "Epoch 12/800, Loss: 213.52282650768757\n",
      "Epoch 13/800, Loss: 198.0638274475932\n",
      "Epoch 14/800, Loss: 182.05337834358215\n",
      "Epoch 15/800, Loss: 179.3672907203436\n",
      "Epoch 16/800, Loss: 166.13004910945892\n",
      "Epoch 17/800, Loss: 162.54093024134636\n",
      "Epoch 18/800, Loss: 145.5851913690567\n",
      "Epoch 19/800, Loss: 140.91619138419628\n",
      "Epoch 20/800, Loss: 140.74349889159203\n",
      "Epoch 21/800, Loss: 126.72975526750088\n",
      "Epoch 22/800, Loss: 123.09767834842205\n",
      "Epoch 23/800, Loss: 118.020671620965\n",
      "Epoch 24/800, Loss: 110.8193983733654\n",
      "Epoch 25/800, Loss: 107.43464197963476\n",
      "Epoch 26/800, Loss: 101.87012980878353\n",
      "Epoch 27/800, Loss: 97.70407901704311\n",
      "Epoch 28/800, Loss: 95.00671505182981\n",
      "Epoch 29/800, Loss: 91.40022045373917\n",
      "Epoch 30/800, Loss: 83.29192307218909\n",
      "Epoch 31/800, Loss: 85.54477240890265\n",
      "Epoch 32/800, Loss: 82.6342989988625\n",
      "Epoch 33/800, Loss: 81.22181906923652\n",
      "Epoch 34/800, Loss: 85.17544435709715\n",
      "Epoch 35/800, Loss: 74.6519904434681\n",
      "Epoch 36/800, Loss: 80.2468811199069\n",
      "Epoch 37/800, Loss: 69.69480338320136\n",
      "Epoch 38/800, Loss: 72.9858746305108\n",
      "Epoch 39/800, Loss: 79.61856816336513\n",
      "Epoch 40/800, Loss: 67.63672002777457\n",
      "Epoch 41/800, Loss: 61.370383240282536\n",
      "Epoch 42/800, Loss: 61.03077905997634\n",
      "Epoch 43/800, Loss: 60.71952444687486\n",
      "Epoch 44/800, Loss: 59.426340363919735\n",
      "Epoch 45/800, Loss: 57.64399604126811\n",
      "Epoch 46/800, Loss: 57.033519614487886\n",
      "Epoch 47/800, Loss: 60.790000069886446\n",
      "Epoch 48/800, Loss: 55.810207329690456\n",
      "Epoch 49/800, Loss: 53.89028324559331\n",
      "Epoch 50/800, Loss: 54.70493905618787\n",
      "Epoch 51/800, Loss: 52.927600521594286\n",
      "Epoch 52/800, Loss: 57.710769813507795\n",
      "Epoch 53/800, Loss: 56.20780407637358\n",
      "Epoch 54/800, Loss: 51.49187404662371\n",
      "Epoch 55/800, Loss: 50.613636676222086\n",
      "Epoch 56/800, Loss: 57.425910698249936\n",
      "Epoch 57/800, Loss: 54.523797515779734\n",
      "Epoch 58/800, Loss: 49.523194029927254\n",
      "Epoch 59/800, Loss: 48.35116270557046\n",
      "Epoch 60/800, Loss: 46.594532538205385\n",
      "Epoch 61/800, Loss: 45.69755525700748\n",
      "Epoch 62/800, Loss: 38.42919784784317\n",
      "Epoch 63/800, Loss: 49.7634882889688\n",
      "Epoch 64/800, Loss: 47.10405774228275\n",
      "Epoch 65/800, Loss: 40.62811229005456\n",
      "Epoch 66/800, Loss: 40.154081923887134\n",
      "Epoch 67/800, Loss: 42.312849493697286\n",
      "Epoch 68/800, Loss: 43.027141470462084\n",
      "Epoch 69/800, Loss: 46.9380389880389\n",
      "Epoch 70/800, Loss: 44.223699152469635\n",
      "Epoch 71/800, Loss: 40.19678835570812\n",
      "Epoch 72/800, Loss: 37.08533808402717\n",
      "Epoch 73/800, Loss: 38.3334840182215\n",
      "Epoch 74/800, Loss: 40.226582396775484\n",
      "Epoch 75/800, Loss: 44.3472117241472\n",
      "Epoch 76/800, Loss: 40.68285878561437\n",
      "Epoch 77/800, Loss: 45.07438496500254\n",
      "Epoch 78/800, Loss: 41.336288910359144\n",
      "Epoch 79/800, Loss: 40.740236423909664\n",
      "Epoch 80/800, Loss: 35.36158642731607\n",
      "Epoch 81/800, Loss: 41.78676947765052\n",
      "Epoch 82/800, Loss: 36.934910733252764\n",
      "Epoch 83/800, Loss: 36.851012136787176\n",
      "Epoch 84/800, Loss: 34.28836070001125\n",
      "Epoch 85/800, Loss: 32.28643269650638\n",
      "Epoch 86/800, Loss: 32.52242012694478\n",
      "Epoch 87/800, Loss: 35.06473860144615\n",
      "Epoch 88/800, Loss: 35.4130113851279\n",
      "Epoch 89/800, Loss: 34.02530271559954\n",
      "Epoch 90/800, Loss: 41.46400870941579\n",
      "Epoch 91/800, Loss: 37.268385142087936\n",
      "Epoch 92/800, Loss: 34.765857910737395\n",
      "Epoch 93/800, Loss: 34.09099543467164\n",
      "Epoch 94/800, Loss: 32.99576055444777\n",
      "Epoch 95/800, Loss: 42.67398261278868\n",
      "Epoch 96/800, Loss: 40.90156535431743\n",
      "Epoch 97/800, Loss: 36.36029330827296\n",
      "Epoch 98/800, Loss: 35.0784801915288\n",
      "Epoch 99/800, Loss: 34.86397774703801\n",
      "Epoch 100/800, Loss: 31.444695947691798\n",
      "Epoch 101/800, Loss: 30.607792608439922\n",
      "Epoch 102/800, Loss: 31.619635216891766\n",
      "Epoch 103/800, Loss: 31.25290430150926\n",
      "Epoch 104/800, Loss: 30.73243815265596\n",
      "Epoch 105/800, Loss: 30.171114966273308\n",
      "Epoch 106/800, Loss: 34.1274830494076\n",
      "Epoch 107/800, Loss: 32.36069789528847\n",
      "Epoch 108/800, Loss: 32.21092186123133\n",
      "Epoch 109/800, Loss: 30.90181598160416\n",
      "Epoch 110/800, Loss: 36.36019619740546\n",
      "Epoch 111/800, Loss: 36.13196994177997\n",
      "Epoch 112/800, Loss: 33.4397749081254\n",
      "Epoch 113/800, Loss: 33.507070844992995\n",
      "Epoch 114/800, Loss: 28.77819361910224\n",
      "Epoch 115/800, Loss: 29.16022053360939\n",
      "Epoch 116/800, Loss: 30.79664439894259\n",
      "Epoch 117/800, Loss: 28.21049334667623\n",
      "Epoch 118/800, Loss: 29.917167616076767\n",
      "Epoch 119/800, Loss: 29.507556978613138\n",
      "Epoch 120/800, Loss: 29.81071162968874\n",
      "Epoch 121/800, Loss: 31.16024331189692\n",
      "Epoch 122/800, Loss: 29.693309295922518\n",
      "Epoch 123/800, Loss: 30.326658527366817\n",
      "Epoch 124/800, Loss: 29.48634291253984\n",
      "Epoch 125/800, Loss: 32.048910200595856\n",
      "Epoch 126/800, Loss: 32.28191786631942\n",
      "Epoch 127/800, Loss: 29.88782581873238\n",
      "Epoch 128/800, Loss: 29.797737458720803\n",
      "Epoch 129/800, Loss: 31.799586795270443\n",
      "Epoch 130/800, Loss: 30.692815750837326\n",
      "Epoch 131/800, Loss: 30.282641176134348\n",
      "Epoch 132/800, Loss: 28.911383939906955\n",
      "Epoch 133/800, Loss: 27.11486343294382\n",
      "Epoch 134/800, Loss: 30.54986355267465\n",
      "Epoch 135/800, Loss: 29.074773535132408\n",
      "Epoch 136/800, Loss: 26.63082909025252\n",
      "Epoch 137/800, Loss: 25.648186147212982\n",
      "Epoch 138/800, Loss: 29.28102663345635\n",
      "Epoch 139/800, Loss: 30.379336439073086\n",
      "Epoch 140/800, Loss: 26.928311908617616\n",
      "Epoch 141/800, Loss: 25.193363443017006\n",
      "Epoch 142/800, Loss: 25.362778226844966\n",
      "Epoch 143/800, Loss: 27.256806639954448\n",
      "Epoch 144/800, Loss: 28.888269900344312\n",
      "Epoch 145/800, Loss: 29.98107496649027\n",
      "Epoch 146/800, Loss: 32.82917669415474\n",
      "Epoch 147/800, Loss: 29.869144916534424\n",
      "Epoch 148/800, Loss: 26.936982486397028\n",
      "Epoch 149/800, Loss: 25.022283209487796\n",
      "Epoch 150/800, Loss: 25.868016615509987\n",
      "Epoch 151/800, Loss: 27.45912986434996\n",
      "Epoch 152/800, Loss: 27.772952368482947\n",
      "Epoch 153/800, Loss: 27.075035464018583\n",
      "Epoch 154/800, Loss: 25.63704231567681\n",
      "Epoch 155/800, Loss: 28.571406489238143\n",
      "Epoch 156/800, Loss: 25.054210832342505\n",
      "Epoch 157/800, Loss: 24.51138166151941\n",
      "Epoch 158/800, Loss: 25.607878281734884\n",
      "Epoch 159/800, Loss: 26.08993603475392\n",
      "Epoch 160/800, Loss: 28.395885914564133\n",
      "Epoch 161/800, Loss: 30.279051810503006\n",
      "Epoch 162/800, Loss: 27.611486975103617\n",
      "Epoch 163/800, Loss: 28.463660839945078\n",
      "Epoch 164/800, Loss: 25.13490134291351\n",
      "Epoch 165/800, Loss: 23.669444501399994\n",
      "Epoch 166/800, Loss: 23.7193338945508\n",
      "Epoch 167/800, Loss: 27.920614130795002\n",
      "Epoch 168/800, Loss: 24.637867491692305\n",
      "Epoch 169/800, Loss: 24.02445960044861\n",
      "Epoch 170/800, Loss: 25.358761182054877\n",
      "Epoch 171/800, Loss: 22.328019578009844\n",
      "Epoch 172/800, Loss: 23.377339808270335\n",
      "Epoch 173/800, Loss: 23.5664472412318\n",
      "Epoch 174/800, Loss: 23.084258808754385\n",
      "Epoch 175/800, Loss: 25.20552913285792\n",
      "Epoch 176/800, Loss: 34.189882162958384\n",
      "Epoch 177/800, Loss: 28.87901339493692\n",
      "Epoch 178/800, Loss: 24.676339890807867\n",
      "Epoch 179/800, Loss: 22.894451341591775\n",
      "Epoch 180/800, Loss: 23.213625635020435\n",
      "Epoch 181/800, Loss: 25.075684195384383\n",
      "Epoch 182/800, Loss: 23.784789035096765\n",
      "Epoch 183/800, Loss: 21.770225959829986\n",
      "Epoch 184/800, Loss: 23.990508815273643\n",
      "Epoch 185/800, Loss: 23.293835379183292\n",
      "Epoch 186/800, Loss: 24.00588639266789\n",
      "Epoch 187/800, Loss: 23.95646838005632\n",
      "Epoch 188/800, Loss: 24.296332001686096\n",
      "Epoch 189/800, Loss: 27.45721146836877\n",
      "Epoch 190/800, Loss: 26.588378390297294\n",
      "Epoch 191/800, Loss: 24.28948051854968\n",
      "Epoch 192/800, Loss: 22.199957869946957\n",
      "Epoch 193/800, Loss: 23.44839072227478\n",
      "Epoch 194/800, Loss: 22.275890786200762\n",
      "Epoch 195/800, Loss: 22.146137295290828\n",
      "Epoch 196/800, Loss: 23.465404859744012\n",
      "Epoch 197/800, Loss: 21.907557735219598\n",
      "Epoch 198/800, Loss: 21.88382187206298\n",
      "Epoch 199/800, Loss: 22.516019258648157\n",
      "Epoch 200/800, Loss: 23.335473304614425\n",
      "Epoch 201/800, Loss: 26.68569689616561\n",
      "Epoch 202/800, Loss: 29.433708103373647\n",
      "Epoch 203/800, Loss: 24.987480498850346\n",
      "Epoch 204/800, Loss: 21.783633740618825\n",
      "Epoch 205/800, Loss: 20.57767256256193\n",
      "Epoch 206/800, Loss: 21.623031605035067\n",
      "Epoch 207/800, Loss: 20.674510087817907\n",
      "Epoch 208/800, Loss: 22.755654145032167\n",
      "Epoch 209/800, Loss: 21.640499816276133\n",
      "Epoch 210/800, Loss: 23.27130143623799\n",
      "Epoch 211/800, Loss: 26.371170619502664\n",
      "Epoch 212/800, Loss: 24.707129037939012\n",
      "Epoch 213/800, Loss: 21.555633693933487\n",
      "Epoch 214/800, Loss: 19.928576149977744\n",
      "Epoch 215/800, Loss: 18.639249727129936\n",
      "Epoch 216/800, Loss: 18.560944344848394\n",
      "Epoch 217/800, Loss: 22.929356010630727\n",
      "Epoch 218/800, Loss: 24.907426370307803\n",
      "Epoch 219/800, Loss: 22.903740119189024\n",
      "Epoch 220/800, Loss: 23.6227214243263\n",
      "Epoch 221/800, Loss: 22.341746658086777\n",
      "Epoch 222/800, Loss: 22.746226819232106\n",
      "Epoch 223/800, Loss: 22.52509771566838\n",
      "Epoch 224/800, Loss: 19.870795339345932\n",
      "Epoch 225/800, Loss: 20.072418009862304\n",
      "Epoch 226/800, Loss: 19.7023322340101\n",
      "Epoch 227/800, Loss: 19.80819941032678\n",
      "Epoch 228/800, Loss: 20.928737992420793\n",
      "Epoch 229/800, Loss: 20.71544998884201\n",
      "Epoch 230/800, Loss: 18.325919449329376\n",
      "Epoch 231/800, Loss: 20.166611501015723\n",
      "Epoch 232/800, Loss: 19.648149019107223\n",
      "Epoch 233/800, Loss: 23.772165493108332\n",
      "Epoch 234/800, Loss: 20.49637155048549\n",
      "Epoch 235/800, Loss: 20.629576940089464\n",
      "Epoch 236/800, Loss: 19.226321189664304\n",
      "Epoch 237/800, Loss: 18.604042891412973\n",
      "Epoch 238/800, Loss: 18.921068382449448\n",
      "Epoch 239/800, Loss: 19.300065658055246\n",
      "Epoch 240/800, Loss: 21.7947008414194\n",
      "Epoch 241/800, Loss: 19.78855761885643\n",
      "Epoch 242/800, Loss: 18.79172396287322\n",
      "Epoch 243/800, Loss: 17.74977630097419\n",
      "Epoch 244/800, Loss: 19.535699993371964\n",
      "Epoch 245/800, Loss: 18.709183677099645\n",
      "Epoch 246/800, Loss: 20.914257037453353\n",
      "Epoch 247/800, Loss: 19.556395693682134\n",
      "Epoch 248/800, Loss: 19.53552203718573\n",
      "Epoch 249/800, Loss: 21.396887941285968\n",
      "Epoch 250/800, Loss: 20.375685101374984\n",
      "Epoch 251/800, Loss: 20.46114546060562\n",
      "Epoch 252/800, Loss: 21.353995306417346\n",
      "Epoch 253/800, Loss: 19.0559463435784\n",
      "Epoch 254/800, Loss: 18.475454517640173\n",
      "Epoch 255/800, Loss: 17.820032094605267\n",
      "Epoch 256/800, Loss: 18.89444089215249\n",
      "Epoch 257/800, Loss: 20.851499839685857\n",
      "Epoch 258/800, Loss: 20.84351073578\n",
      "Epoch 259/800, Loss: 19.74477551318705\n",
      "Epoch 260/800, Loss: 19.195603567175567\n",
      "Epoch 261/800, Loss: 19.84464583452791\n",
      "Epoch 262/800, Loss: 17.928043824620545\n",
      "Epoch 263/800, Loss: 19.633055040612817\n",
      "Epoch 264/800, Loss: 20.827631760388613\n",
      "Epoch 265/800, Loss: 18.854742946103215\n",
      "Epoch 266/800, Loss: 19.06359224114567\n",
      "Epoch 267/800, Loss: 18.300065615214407\n",
      "Epoch 268/800, Loss: 19.29435384273529\n",
      "Epoch 269/800, Loss: 19.046937464736402\n",
      "Epoch 270/800, Loss: 21.26474816352129\n",
      "Epoch 271/800, Loss: 20.893658699467778\n",
      "Epoch 272/800, Loss: 20.394206358119845\n",
      "Epoch 273/800, Loss: 18.561394235119224\n",
      "Epoch 274/800, Loss: 20.47982342261821\n",
      "Epoch 275/800, Loss: 19.530359177850187\n",
      "Epoch 276/800, Loss: 19.767397756688297\n",
      "Epoch 277/800, Loss: 17.895818282850087\n",
      "Epoch 278/800, Loss: 18.763131237588823\n",
      "Epoch 279/800, Loss: 19.85799859650433\n",
      "Epoch 280/800, Loss: 18.66274229809642\n",
      "Epoch 281/800, Loss: 17.74251423869282\n",
      "Epoch 282/800, Loss: 17.859920754097402\n",
      "Epoch 283/800, Loss: 17.913851842284203\n",
      "Epoch 284/800, Loss: 21.922375660389662\n",
      "Epoch 285/800, Loss: 21.23915616236627\n",
      "Epoch 286/800, Loss: 19.19531940575689\n",
      "Epoch 287/800, Loss: 18.15112217143178\n",
      "Epoch 288/800, Loss: 19.073482252657413\n",
      "Epoch 289/800, Loss: 25.021857840009034\n",
      "Epoch 290/800, Loss: 22.293310184031725\n",
      "Epoch 291/800, Loss: 17.85231109894812\n",
      "Epoch 292/800, Loss: 16.777076398022473\n",
      "Epoch 293/800, Loss: 17.516277096234262\n",
      "Epoch 294/800, Loss: 18.41857865359634\n",
      "Epoch 295/800, Loss: 17.14449799247086\n",
      "Epoch 296/800, Loss: 16.67258435022086\n",
      "Epoch 297/800, Loss: 15.91858631465584\n",
      "Epoch 298/800, Loss: 16.3890253957361\n",
      "Epoch 299/800, Loss: 18.32208163291216\n",
      "Epoch 300/800, Loss: 17.59663732443005\n",
      "Epoch 301/800, Loss: 19.843799963593483\n",
      "Epoch 302/800, Loss: 18.414858263917267\n",
      "Epoch 303/800, Loss: 17.5963993947953\n",
      "Epoch 304/800, Loss: 17.10322941467166\n",
      "Epoch 305/800, Loss: 16.811966434121132\n",
      "Epoch 306/800, Loss: 17.550156513229012\n",
      "Epoch 307/800, Loss: 19.562780269421637\n",
      "Epoch 308/800, Loss: 21.887624531053007\n",
      "Epoch 309/800, Loss: 19.757829612120986\n",
      "Epoch 310/800, Loss: 17.406072601675987\n",
      "Epoch 311/800, Loss: 16.994594879448414\n",
      "Epoch 312/800, Loss: 16.439781389199197\n",
      "Epoch 313/800, Loss: 17.54428133368492\n",
      "Epoch 314/800, Loss: 18.305267087183893\n",
      "Epoch 315/800, Loss: 18.510666761547327\n",
      "Epoch 316/800, Loss: 18.301752172410488\n",
      "Epoch 317/800, Loss: 17.41136158350855\n",
      "Epoch 318/800, Loss: 17.971120706759393\n",
      "Epoch 319/800, Loss: 18.992464459501207\n",
      "Epoch 320/800, Loss: 18.398896131664515\n",
      "Epoch 321/800, Loss: 20.701726832427084\n",
      "Epoch 322/800, Loss: 19.477939288131893\n",
      "Epoch 323/800, Loss: 22.03171419352293\n",
      "Epoch 324/800, Loss: 21.19371875282377\n",
      "Epoch 325/800, Loss: 17.581025608815253\n",
      "Epoch 326/800, Loss: 16.932803735136986\n",
      "Epoch 327/800, Loss: 16.3047406245023\n",
      "Epoch 328/800, Loss: 15.98720262106508\n",
      "Epoch 329/800, Loss: 16.553552761673927\n",
      "Epoch 330/800, Loss: 16.29331671819091\n",
      "Epoch 331/800, Loss: 15.65044611506164\n",
      "Epoch 332/800, Loss: 15.866102940868586\n",
      "Epoch 333/800, Loss: 16.777074180543423\n",
      "Epoch 334/800, Loss: 18.623737583868206\n",
      "Epoch 335/800, Loss: 19.613299529068172\n",
      "Epoch 336/800, Loss: 20.010755943134427\n",
      "Epoch 337/800, Loss: 20.720770854502916\n",
      "Epoch 338/800, Loss: 18.404241665266454\n",
      "Epoch 339/800, Loss: 17.635129631496966\n",
      "Epoch 340/800, Loss: 16.108321765437722\n",
      "Epoch 341/800, Loss: 16.956078140996397\n",
      "Epoch 342/800, Loss: 15.902314672246575\n",
      "Epoch 343/800, Loss: 16.633291406556964\n",
      "Epoch 344/800, Loss: 17.21762279700488\n",
      "Epoch 345/800, Loss: 17.489042541012168\n",
      "Epoch 346/800, Loss: 16.404940482228994\n",
      "Epoch 347/800, Loss: 17.140491189435124\n",
      "Epoch 348/800, Loss: 19.70562760811299\n",
      "Epoch 349/800, Loss: 18.56231462582946\n",
      "Epoch 350/800, Loss: 16.926765588112175\n",
      "Epoch 351/800, Loss: 17.833672213368118\n",
      "Epoch 352/800, Loss: 16.53697412274778\n",
      "Epoch 353/800, Loss: 16.23987414035946\n",
      "Epoch 354/800, Loss: 15.805451882071793\n",
      "Epoch 355/800, Loss: 18.262123866938055\n",
      "Epoch 356/800, Loss: 16.418572845868766\n",
      "Epoch 357/800, Loss: 16.759413450025022\n",
      "Epoch 358/800, Loss: 15.762372757308185\n",
      "Epoch 359/800, Loss: 15.844640591181815\n",
      "Epoch 360/800, Loss: 16.67437770590186\n",
      "Epoch 361/800, Loss: 15.943419598042965\n",
      "Epoch 362/800, Loss: 15.080622164532542\n",
      "Epoch 363/800, Loss: 16.496540292166173\n",
      "Epoch 364/800, Loss: 18.926881201565266\n",
      "Epoch 365/800, Loss: 16.69549913611263\n",
      "Epoch 366/800, Loss: 16.45182265434414\n",
      "Epoch 367/800, Loss: 15.985390365123749\n",
      "Epoch 368/800, Loss: 15.823436871170998\n",
      "Epoch 369/800, Loss: 15.196762502193451\n",
      "Epoch 370/800, Loss: 15.805970161221921\n",
      "Epoch 371/800, Loss: 15.246234900318086\n",
      "Epoch 372/800, Loss: 15.26337430998683\n",
      "Epoch 373/800, Loss: 17.334660140797496\n",
      "Epoch 374/800, Loss: 16.410331699997187\n",
      "Epoch 375/800, Loss: 16.20747292228043\n",
      "Epoch 376/800, Loss: 15.78921394329518\n",
      "Epoch 377/800, Loss: 16.77502123825252\n",
      "Epoch 378/800, Loss: 18.069433702155948\n",
      "Epoch 379/800, Loss: 17.920134325511754\n",
      "Epoch 380/800, Loss: 17.91968920081854\n",
      "Epoch 381/800, Loss: 15.574443912133574\n",
      "Epoch 382/800, Loss: 17.890709940344095\n",
      "Epoch 383/800, Loss: 15.804772976785898\n",
      "Epoch 384/800, Loss: 15.567785651423037\n",
      "Epoch 385/800, Loss: 15.589318997226655\n",
      "Epoch 386/800, Loss: 15.354618487879634\n",
      "Epoch 387/800, Loss: 15.101853851228952\n",
      "Epoch 388/800, Loss: 17.100134253501892\n",
      "Epoch 389/800, Loss: 14.996998003683984\n",
      "Epoch 390/800, Loss: 15.551021930761635\n",
      "Epoch 391/800, Loss: 16.39218118134886\n",
      "Epoch 392/800, Loss: 16.54153494350612\n",
      "Epoch 393/800, Loss: 15.364055807702243\n",
      "Epoch 394/800, Loss: 16.12099640071392\n",
      "Epoch 395/800, Loss: 18.310017066076398\n",
      "Epoch 396/800, Loss: 16.031670684926212\n",
      "Epoch 397/800, Loss: 16.421120181679726\n",
      "Epoch 398/800, Loss: 15.762949358671904\n",
      "Epoch 399/800, Loss: 17.505479915998876\n",
      "Epoch 400/800, Loss: 19.189536998048425\n",
      "Epoch 401/800, Loss: 18.134650354273617\n",
      "Epoch 402/800, Loss: 16.695729285478592\n",
      "Epoch 403/800, Loss: 13.964944467879832\n",
      "Epoch 404/800, Loss: 13.778380580246449\n",
      "Epoch 405/800, Loss: 16.090639181435108\n",
      "Epoch 406/800, Loss: 16.554131503216922\n",
      "Epoch 407/800, Loss: 15.778300632722676\n",
      "Epoch 408/800, Loss: 14.135300687514246\n",
      "Epoch 409/800, Loss: 14.243026569485664\n",
      "Epoch 410/800, Loss: 14.138502622023225\n",
      "Epoch 411/800, Loss: 15.00994984433055\n",
      "Epoch 412/800, Loss: 14.278263306245208\n",
      "Epoch 413/800, Loss: 16.2463670684956\n",
      "Epoch 414/800, Loss: 16.689163150265813\n",
      "Epoch 415/800, Loss: 16.069533571600914\n",
      "Epoch 416/800, Loss: 16.755628732964396\n",
      "Epoch 417/800, Loss: 17.392956448718905\n",
      "Epoch 418/800, Loss: 16.807157243601978\n",
      "Epoch 419/800, Loss: 15.212017821148038\n",
      "Epoch 420/800, Loss: 15.324973998591304\n",
      "Epoch 421/800, Loss: 14.653440047986805\n",
      "Epoch 422/800, Loss: 15.32382341939956\n",
      "Epoch 423/800, Loss: 16.90490534156561\n",
      "Epoch 424/800, Loss: 17.238387782126665\n",
      "Epoch 425/800, Loss: 17.374090171419084\n",
      "Epoch 426/800, Loss: 15.026864300481975\n",
      "Epoch 427/800, Loss: 14.183231526985765\n",
      "Epoch 428/800, Loss: 15.094609539490193\n",
      "Epoch 429/800, Loss: 16.377719092182815\n",
      "Epoch 430/800, Loss: 16.20369947142899\n",
      "Epoch 431/800, Loss: 14.937494441866875\n",
      "Epoch 432/800, Loss: 14.357152853161097\n",
      "Epoch 433/800, Loss: 13.44781340053305\n",
      "Epoch 434/800, Loss: 14.938427376560867\n",
      "Epoch 435/800, Loss: 15.335491666570306\n",
      "Epoch 436/800, Loss: 15.620396556332707\n",
      "Epoch 437/800, Loss: 15.662792719900608\n",
      "Epoch 438/800, Loss: 17.768022092990577\n",
      "Epoch 439/800, Loss: 16.720022930763662\n",
      "Epoch 440/800, Loss: 16.127635001204908\n",
      "Epoch 441/800, Loss: 16.263938065618277\n",
      "Epoch 442/800, Loss: 17.15594020485878\n",
      "Epoch 443/800, Loss: 16.25134813040495\n",
      "Epoch 444/800, Loss: 15.605939256027341\n",
      "Epoch 445/800, Loss: 15.872469479218125\n",
      "Epoch 446/800, Loss: 15.560073733329773\n",
      "Epoch 447/800, Loss: 15.92360070347786\n",
      "Epoch 448/800, Loss: 15.406592325307429\n",
      "Epoch 449/800, Loss: 15.07078958582133\n",
      "Epoch 450/800, Loss: 14.334613465704024\n",
      "Epoch 451/800, Loss: 14.816296990495175\n",
      "Epoch 452/800, Loss: 13.816262621432543\n",
      "Epoch 453/800, Loss: 13.624483164399862\n",
      "Epoch 454/800, Loss: 15.206791114527732\n",
      "Epoch 455/800, Loss: 16.776652086526155\n",
      "Epoch 456/800, Loss: 16.124866367317736\n",
      "Epoch 457/800, Loss: 16.16250065062195\n",
      "Epoch 458/800, Loss: 15.875906670466065\n",
      "Epoch 459/800, Loss: 14.781674896366894\n",
      "Epoch 460/800, Loss: 15.177996001206338\n",
      "Epoch 461/800, Loss: 14.331039849668741\n",
      "Epoch 462/800, Loss: 14.486206715926528\n",
      "Epoch 463/800, Loss: 12.640011018607765\n",
      "Epoch 464/800, Loss: 13.367579923942685\n",
      "Epoch 465/800, Loss: 16.5996794430539\n",
      "Epoch 466/800, Loss: 18.787006585858762\n",
      "Epoch 467/800, Loss: 17.939121523872018\n",
      "Epoch 468/800, Loss: 16.9109602086246\n",
      "Epoch 469/800, Loss: 15.37956829648465\n",
      "Epoch 470/800, Loss: 15.572313691489398\n",
      "Epoch 471/800, Loss: 15.346672776155174\n",
      "Epoch 472/800, Loss: 14.700973683502525\n",
      "Epoch 473/800, Loss: 15.278577288612723\n",
      "Epoch 474/800, Loss: 14.762694460339844\n",
      "Epoch 475/800, Loss: 13.789923997595906\n",
      "Epoch 476/800, Loss: 16.654935345053673\n",
      "Epoch 477/800, Loss: 17.039885222911835\n",
      "Epoch 478/800, Loss: 17.544949096627533\n",
      "Epoch 479/800, Loss: 17.092211929615587\n",
      "Epoch 480/800, Loss: 18.326115561649203\n",
      "Epoch 481/800, Loss: 16.905344191007316\n",
      "Epoch 482/800, Loss: 15.781980453059077\n",
      "Epoch 483/800, Loss: 14.113998179323971\n",
      "Epoch 484/800, Loss: 13.715023606084287\n",
      "Epoch 485/800, Loss: 14.141744351014495\n",
      "Epoch 486/800, Loss: 15.250519284978509\n",
      "Epoch 487/800, Loss: 14.643604433164\n",
      "Epoch 488/800, Loss: 14.878865908831358\n",
      "Epoch 489/800, Loss: 15.273783709853888\n",
      "Epoch 490/800, Loss: 14.775393969845027\n",
      "Epoch 491/800, Loss: 14.549363284371793\n",
      "Epoch 492/800, Loss: 14.698299840092659\n",
      "Epoch 493/800, Loss: 15.593894024845213\n",
      "Epoch 494/800, Loss: 15.166531424969435\n",
      "Epoch 495/800, Loss: 15.935032170265913\n",
      "Epoch 496/800, Loss: 14.134126822464168\n",
      "Epoch 497/800, Loss: 14.457531764172018\n",
      "Epoch 498/800, Loss: 14.66634519258514\n",
      "Epoch 499/800, Loss: 15.292681684717536\n",
      "Epoch 500/800, Loss: 17.428498803637922\n",
      "Epoch 501/800, Loss: 15.682060807943344\n",
      "Epoch 502/800, Loss: 15.226666618138552\n",
      "Epoch 503/800, Loss: 13.639890209771693\n",
      "Epoch 504/800, Loss: 13.686422471888363\n",
      "Epoch 505/800, Loss: 13.941868708934635\n",
      "Epoch 506/800, Loss: 13.529988731257617\n",
      "Epoch 507/800, Loss: 14.980109649710357\n",
      "Epoch 508/800, Loss: 14.246155879460275\n",
      "Epoch 509/800, Loss: 13.644351026974618\n",
      "Epoch 510/800, Loss: 14.271586200688034\n",
      "Epoch 511/800, Loss: 16.85619305446744\n",
      "Epoch 512/800, Loss: 15.459793566726148\n",
      "Epoch 513/800, Loss: 14.610093664377928\n",
      "Epoch 514/800, Loss: 14.06308018323034\n",
      "Epoch 515/800, Loss: 14.939966591075063\n",
      "Epoch 516/800, Loss: 14.575307914055884\n",
      "Epoch 517/800, Loss: 16.296122438274324\n",
      "Epoch 518/800, Loss: 15.148696495220065\n",
      "Epoch 519/800, Loss: 15.040795079432428\n",
      "Epoch 520/800, Loss: 13.737775033339858\n",
      "Epoch 521/800, Loss: 13.906830799300224\n",
      "Epoch 522/800, Loss: 14.95711109507829\n",
      "Epoch 523/800, Loss: 15.952851861715317\n",
      "Epoch 524/800, Loss: 14.995931890793145\n",
      "Epoch 525/800, Loss: 18.193016675300896\n",
      "Epoch 526/800, Loss: 14.528701382689178\n",
      "Epoch 527/800, Loss: 14.234395267441869\n",
      "Epoch 528/800, Loss: 13.877550560981035\n",
      "Epoch 529/800, Loss: 13.805939494632185\n",
      "Epoch 530/800, Loss: 13.435022113844752\n",
      "Epoch 531/800, Loss: 13.323695951141417\n",
      "Epoch 532/800, Loss: 13.196556712500751\n",
      "Epoch 533/800, Loss: 14.219103121198714\n",
      "Epoch 534/800, Loss: 15.043839959427714\n",
      "Epoch 535/800, Loss: 16.297866991721094\n",
      "Epoch 536/800, Loss: 15.385659417137504\n",
      "Epoch 537/800, Loss: 15.810110053047538\n",
      "Epoch 538/800, Loss: 15.277857950888574\n",
      "Epoch 539/800, Loss: 14.421351727098227\n",
      "Epoch 540/800, Loss: 14.340202109888196\n",
      "Epoch 541/800, Loss: 14.496588011272252\n",
      "Epoch 542/800, Loss: 14.635585187003016\n",
      "Epoch 543/800, Loss: 14.676217812113464\n",
      "Epoch 544/800, Loss: 13.131455612368882\n",
      "Epoch 545/800, Loss: 13.744709556922317\n",
      "Epoch 546/800, Loss: 13.044474458321929\n",
      "Epoch 547/800, Loss: 13.422617834992707\n",
      "Epoch 548/800, Loss: 14.601504739373922\n",
      "Epoch 549/800, Loss: 13.799857773818076\n",
      "Epoch 550/800, Loss: 13.827644972130656\n",
      "Epoch 551/800, Loss: 13.69575574528426\n",
      "Epoch 552/800, Loss: 14.615739474073052\n",
      "Epoch 553/800, Loss: 14.619324452243745\n",
      "Epoch 554/800, Loss: 14.41048401594162\n",
      "Epoch 555/800, Loss: 14.61101827584207\n",
      "Epoch 556/800, Loss: 14.545897289179265\n",
      "Epoch 557/800, Loss: 14.809105576016009\n",
      "Epoch 558/800, Loss: 14.42771623749286\n",
      "Epoch 559/800, Loss: 14.271722769364715\n",
      "Epoch 560/800, Loss: 13.857587691396475\n",
      "Epoch 561/800, Loss: 13.619433287531137\n",
      "Epoch 562/800, Loss: 14.252932003233582\n",
      "Epoch 563/800, Loss: 15.501462986692786\n",
      "Epoch 564/800, Loss: 15.860635583288968\n",
      "Epoch 565/800, Loss: 14.414335600100458\n",
      "Epoch 566/800, Loss: 15.231500789523125\n",
      "Epoch 567/800, Loss: 15.362651506438851\n",
      "Epoch 568/800, Loss: 15.200859774835408\n",
      "Epoch 569/800, Loss: 13.746006828732789\n",
      "Epoch 570/800, Loss: 13.666449576616287\n",
      "Epoch 571/800, Loss: 12.572405397891998\n",
      "Epoch 572/800, Loss: 12.36054983921349\n",
      "Epoch 573/800, Loss: 12.071469366084784\n",
      "Epoch 574/800, Loss: 12.42844417039305\n",
      "Epoch 575/800, Loss: 13.074747310951352\n",
      "Epoch 576/800, Loss: 12.881472670007497\n",
      "Epoch 577/800, Loss: 14.00540978461504\n",
      "Epoch 578/800, Loss: 14.67134502157569\n",
      "Epoch 579/800, Loss: 14.74117491254583\n",
      "Epoch 580/800, Loss: 14.413236275315285\n",
      "Epoch 581/800, Loss: 14.113358062691987\n",
      "Epoch 582/800, Loss: 14.162000766955316\n",
      "Epoch 583/800, Loss: 14.01133206859231\n",
      "Epoch 584/800, Loss: 13.378005343489349\n",
      "Epoch 585/800, Loss: 14.530775154940784\n",
      "Epoch 586/800, Loss: 14.83019819110632\n",
      "Epoch 587/800, Loss: 15.887785421684384\n",
      "Epoch 588/800, Loss: 15.891453393734992\n",
      "Epoch 589/800, Loss: 14.916076302528381\n",
      "Epoch 590/800, Loss: 13.34531037742272\n",
      "Epoch 591/800, Loss: 13.583894726354629\n",
      "Epoch 592/800, Loss: 13.442806954495609\n",
      "Epoch 593/800, Loss: 13.998595189768821\n",
      "Epoch 594/800, Loss: 14.059369685128331\n",
      "Epoch 595/800, Loss: 13.683008995838463\n",
      "Epoch 596/800, Loss: 13.111833157017827\n",
      "Epoch 597/800, Loss: 13.264373529702425\n",
      "Epoch 598/800, Loss: 13.56352311372757\n",
      "Epoch 599/800, Loss: 13.736173989251256\n",
      "Epoch 600/800, Loss: 13.111229435540736\n",
      "Epoch 601/800, Loss: 13.948055771179497\n",
      "Epoch 602/800, Loss: 12.944923338480294\n",
      "Epoch 603/800, Loss: 13.482483957894146\n",
      "Epoch 604/800, Loss: 16.565834011882544\n",
      "Epoch 605/800, Loss: 16.90074773877859\n",
      "Epoch 606/800, Loss: 15.907357353717089\n",
      "Epoch 607/800, Loss: 15.409802687354386\n",
      "Epoch 608/800, Loss: 13.421331613324583\n",
      "Epoch 609/800, Loss: 12.852396238129586\n",
      "Epoch 610/800, Loss: 15.064994963817298\n",
      "Epoch 611/800, Loss: 13.561790214851499\n",
      "Epoch 612/800, Loss: 13.391897046007216\n",
      "Epoch 613/800, Loss: 13.471958797425032\n",
      "Epoch 614/800, Loss: 12.550885565113276\n",
      "Epoch 615/800, Loss: 13.968171717599034\n",
      "Epoch 616/800, Loss: 13.538097163662314\n",
      "Epoch 617/800, Loss: 13.45149598410353\n",
      "Epoch 618/800, Loss: 13.337577803526074\n",
      "Epoch 619/800, Loss: 13.010311957448721\n",
      "Epoch 620/800, Loss: 13.293874341994524\n",
      "Epoch 621/800, Loss: 13.791125915944576\n",
      "Epoch 622/800, Loss: 13.864921057596803\n",
      "Epoch 623/800, Loss: 14.759310867637396\n",
      "Epoch 624/800, Loss: 14.395296943373978\n",
      "Epoch 625/800, Loss: 14.526982857845724\n",
      "Epoch 626/800, Loss: 13.285814828239381\n",
      "Epoch 627/800, Loss: 13.234676937572658\n",
      "Epoch 628/800, Loss: 12.86854162439704\n",
      "Epoch 629/800, Loss: 16.433615959249437\n",
      "Epoch 630/800, Loss: 13.995638482272625\n",
      "Epoch 631/800, Loss: 13.326471457257867\n",
      "Epoch 632/800, Loss: 12.89875403791666\n",
      "Epoch 633/800, Loss: 15.42581561487168\n",
      "Epoch 634/800, Loss: 15.51916379481554\n",
      "Epoch 635/800, Loss: 13.831091487780213\n",
      "Epoch 636/800, Loss: 16.4310721764341\n",
      "Epoch 637/800, Loss: 16.310799379833043\n",
      "Epoch 638/800, Loss: 13.499959891662002\n",
      "Epoch 639/800, Loss: 12.4735406646505\n",
      "Epoch 640/800, Loss: 12.647006417624652\n",
      "Epoch 641/800, Loss: 12.505616874434054\n",
      "Epoch 642/800, Loss: 12.943026775494218\n",
      "Epoch 643/800, Loss: 13.206919303163886\n",
      "Epoch 644/800, Loss: 12.579138118773699\n",
      "Epoch 645/800, Loss: 13.45757142547518\n",
      "Epoch 646/800, Loss: 13.283317606896162\n",
      "Epoch 647/800, Loss: 16.7340738279745\n",
      "Epoch 648/800, Loss: 16.47681702580303\n",
      "Epoch 649/800, Loss: 14.488655024208128\n",
      "Epoch 650/800, Loss: 14.554273769259453\n",
      "Epoch 651/800, Loss: 13.057866069488227\n",
      "Epoch 652/800, Loss: 11.802482631988823\n",
      "Epoch 653/800, Loss: 11.94968740735203\n",
      "Epoch 654/800, Loss: 11.63122136425227\n",
      "Epoch 655/800, Loss: 12.852732566185296\n",
      "Epoch 656/800, Loss: 15.144754220731556\n",
      "Epoch 657/800, Loss: 14.72868641698733\n",
      "Epoch 658/800, Loss: 14.126222194172442\n",
      "Epoch 659/800, Loss: 14.269202234223485\n",
      "Epoch 660/800, Loss: 14.498497074935585\n",
      "Epoch 661/800, Loss: 13.498223301023245\n",
      "Epoch 662/800, Loss: 14.773318253457546\n",
      "Epoch 663/800, Loss: 13.660898806527257\n",
      "Epoch 664/800, Loss: 13.49994042608887\n",
      "Epoch 665/800, Loss: 13.097143134102225\n",
      "Epoch 666/800, Loss: 12.756852744612843\n",
      "Epoch 667/800, Loss: 12.484347343444824\n",
      "Epoch 668/800, Loss: 14.395523024722934\n",
      "Epoch 669/800, Loss: 13.253596703521907\n",
      "Epoch 670/800, Loss: 12.994153308682144\n",
      "Epoch 671/800, Loss: 14.081086468882859\n",
      "Epoch 672/800, Loss: 14.558800854254514\n",
      "Epoch 673/800, Loss: 13.380331168882549\n",
      "Epoch 674/800, Loss: 12.72744769230485\n",
      "Epoch 675/800, Loss: 12.588535745628178\n",
      "Epoch 676/800, Loss: 12.278003953397274\n",
      "Epoch 677/800, Loss: 14.250231676734984\n",
      "Epoch 678/800, Loss: 15.995929611846805\n",
      "Epoch 679/800, Loss: 14.491929854266346\n",
      "Epoch 680/800, Loss: 14.745972158387303\n",
      "Epoch 681/800, Loss: 13.180145025253296\n",
      "Epoch 682/800, Loss: 13.489540376700461\n",
      "Epoch 683/800, Loss: 13.35488876979798\n",
      "Epoch 684/800, Loss: 12.610896133817732\n",
      "Epoch 685/800, Loss: 12.829653618857265\n",
      "Epoch 686/800, Loss: 13.689164243638515\n",
      "Epoch 687/800, Loss: 13.983783283270895\n",
      "Epoch 688/800, Loss: 15.298202118836343\n",
      "Epoch 689/800, Loss: 14.741498908028007\n",
      "Epoch 690/800, Loss: 14.781224554404616\n",
      "Epoch 691/800, Loss: 14.945624141022563\n",
      "Epoch 692/800, Loss: 14.52770171314478\n",
      "Epoch 693/800, Loss: 13.55256524682045\n",
      "Epoch 694/800, Loss: 13.784098505042493\n",
      "Epoch 695/800, Loss: 12.721387922763824\n",
      "Epoch 696/800, Loss: 12.292822430841625\n",
      "Epoch 697/800, Loss: 12.301257381215692\n",
      "Epoch 698/800, Loss: 11.246805753093213\n",
      "Epoch 699/800, Loss: 12.13106951629743\n",
      "Epoch 700/800, Loss: 13.179949921555817\n",
      "Epoch 701/800, Loss: 14.167834307067096\n",
      "Epoch 702/800, Loss: 13.617968843784183\n",
      "Epoch 703/800, Loss: 13.437370265834033\n",
      "Epoch 704/800, Loss: 15.034772278741002\n",
      "Epoch 705/800, Loss: 15.129306448623538\n",
      "Epoch 706/800, Loss: 13.64944621361792\n",
      "Epoch 707/800, Loss: 12.924421575386077\n",
      "Epoch 708/800, Loss: 13.248288308270276\n",
      "Epoch 709/800, Loss: 11.962385586462915\n",
      "Epoch 710/800, Loss: 12.812159693799913\n",
      "Epoch 711/800, Loss: 14.804766621440649\n",
      "Epoch 712/800, Loss: 13.554791121277958\n",
      "Epoch 713/800, Loss: 13.26277611637488\n",
      "Epoch 714/800, Loss: 12.827365635894239\n",
      "Epoch 715/800, Loss: 12.05574938422069\n",
      "Epoch 716/800, Loss: 13.157992626540363\n",
      "Epoch 717/800, Loss: 12.947100090794265\n",
      "Epoch 718/800, Loss: 13.411968144588172\n",
      "Epoch 719/800, Loss: 12.318389908410609\n",
      "Epoch 720/800, Loss: 12.800440804101527\n",
      "Epoch 721/800, Loss: 12.379698107019067\n",
      "Epoch 722/800, Loss: 12.447521679569036\n",
      "Epoch 723/800, Loss: 12.21062662312761\n",
      "Epoch 724/800, Loss: 12.031192335300148\n",
      "Epoch 725/800, Loss: 11.661239941604435\n",
      "Epoch 726/800, Loss: 11.939688419923186\n",
      "Epoch 727/800, Loss: 13.02256735228002\n",
      "Epoch 728/800, Loss: 12.879360294435173\n",
      "Epoch 729/800, Loss: 14.232823110185564\n",
      "Epoch 730/800, Loss: 15.432792128063738\n",
      "Epoch 731/800, Loss: 14.8652438595891\n",
      "Epoch 732/800, Loss: 14.09735951013863\n",
      "Epoch 733/800, Loss: 12.2143713561818\n",
      "Epoch 734/800, Loss: 11.497116264887154\n",
      "Epoch 735/800, Loss: 11.986717322375625\n",
      "Epoch 736/800, Loss: 12.837496143300086\n",
      "Epoch 737/800, Loss: 12.731605197302997\n",
      "Epoch 738/800, Loss: 14.200536410324275\n",
      "Epoch 739/800, Loss: 15.973059705458581\n",
      "Epoch 740/800, Loss: 13.79989720787853\n",
      "Epoch 741/800, Loss: 12.812677909620106\n",
      "Epoch 742/800, Loss: 12.216218519955873\n",
      "Epoch 743/800, Loss: 12.591958343982697\n",
      "Epoch 744/800, Loss: 11.846369486302137\n",
      "Epoch 745/800, Loss: 12.966507596895099\n",
      "Epoch 746/800, Loss: 13.640222989022732\n",
      "Epoch 747/800, Loss: 13.308062200434506\n",
      "Epoch 748/800, Loss: 13.353446553461254\n",
      "Epoch 749/800, Loss: 12.182891379576176\n",
      "Epoch 750/800, Loss: 14.904632524121553\n",
      "Epoch 751/800, Loss: 15.295526063069701\n",
      "Epoch 752/800, Loss: 14.172900069970638\n",
      "Epoch 753/800, Loss: 13.47281951084733\n",
      "Epoch 754/800, Loss: 11.861210006289184\n",
      "Epoch 755/800, Loss: 11.439257135614753\n",
      "Epoch 756/800, Loss: 11.603805374354124\n",
      "Epoch 757/800, Loss: 13.608704697340727\n",
      "Epoch 758/800, Loss: 13.95429431181401\n",
      "Epoch 759/800, Loss: 14.22012747451663\n",
      "Epoch 760/800, Loss: 13.324437991715968\n",
      "Epoch 761/800, Loss: 13.73495671711862\n",
      "Epoch 762/800, Loss: 12.517467319499701\n",
      "Epoch 763/800, Loss: 12.998523907735944\n",
      "Epoch 764/800, Loss: 12.93791957711801\n",
      "Epoch 765/800, Loss: 12.514823872596025\n",
      "Epoch 766/800, Loss: 13.317234398797154\n",
      "Epoch 767/800, Loss: 12.400339004583657\n",
      "Epoch 768/800, Loss: 13.633539378643036\n",
      "Epoch 769/800, Loss: 14.83623859565705\n",
      "Epoch 770/800, Loss: 13.589224397670478\n",
      "Epoch 771/800, Loss: 14.407953991554677\n",
      "Epoch 772/800, Loss: 14.51132476143539\n",
      "Epoch 773/800, Loss: 14.216842455789447\n",
      "Epoch 774/800, Loss: 12.519170295447111\n",
      "Epoch 775/800, Loss: 12.96680304594338\n",
      "Epoch 776/800, Loss: 12.026398116722703\n",
      "Epoch 777/800, Loss: 11.952589282765985\n",
      "Epoch 778/800, Loss: 11.847694601863623\n",
      "Epoch 779/800, Loss: 11.527354718651623\n",
      "Epoch 780/800, Loss: 12.994092692621052\n",
      "Epoch 781/800, Loss: 12.152011313475668\n",
      "Epoch 782/800, Loss: 12.489263269118965\n",
      "Epoch 783/800, Loss: 13.974961419589818\n",
      "Epoch 784/800, Loss: 13.183582037687302\n",
      "Epoch 785/800, Loss: 13.085441689938307\n",
      "Epoch 786/800, Loss: 13.272905538789928\n",
      "Epoch 787/800, Loss: 13.294170091394335\n",
      "Epoch 788/800, Loss: 12.61100819800049\n",
      "Epoch 789/800, Loss: 12.185638103168458\n",
      "Epoch 790/800, Loss: 13.221540266647935\n",
      "Epoch 791/800, Loss: 13.551348927430809\n",
      "Epoch 792/800, Loss: 14.030080911703408\n",
      "Epoch 793/800, Loss: 15.072537714615464\n",
      "Epoch 794/800, Loss: 17.823348510079086\n",
      "Epoch 795/800, Loss: 14.485726043581963\n",
      "Epoch 796/800, Loss: 12.819293461740017\n",
      "Epoch 797/800, Loss: 11.567816223949194\n",
      "Epoch 798/800, Loss: 11.32525698421523\n",
      "Epoch 799/800, Loss: 11.43061943165958\n",
      "Epoch 800/800, Loss: 11.441358708776534\n",
      "Test Score (R^2): 0.8053126728041079\n"
     ]
    }
   ],
   "source": [
    "#### CREATING MODEL ####\n",
    "\n",
    "# Define the neural network architecture with multiple hidden layers\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        layers = []\n",
    "        previous_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(previous_dim, hidden_dim))\n",
    "            layers.append(nn.Tanh())\n",
    "            previous_dim = hidden_dim\n",
    "        layers.append(nn.Linear(previous_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers) \n",
    "        self.activation = nn.Tanh() #best according to grid search\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_data.shape[1]  # Number of molecular descriptors\n",
    "hidden_dim = [512,512,512]  # (512,512,512) Best according to grid search\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 800\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function (nn.MSELoss) and optimizer\n",
    "model = SolubilityPredictor(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "test_score = r2_score(y_test, y_pred)\n",
    "print(\"Test Score (R^2):\", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.9972437620162964\n",
      "Test RMSE: 0.9986209273338318\n",
      "Test MAE: 0.659709632396698\n",
      "Test R^2: 0.8053126728041079\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Perform the forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    test_predictions_tensor = model(X_test_tensor)\n",
    "\n",
    "# Convert predictions back to NumPy array\n",
    "test_predictions = test_predictions_tensor.numpy().flatten()\n",
    "\n",
    "# Calculate additional metrics\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With dropout, weight decay\n",
    "try dropout rates between 0.2 and 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 681.7832328081131\n",
      "Epoch 101/1000, Loss: 111.76737007498741\n",
      "Epoch 201/1000, Loss: 74.74509523808956\n",
      "Epoch 301/1000, Loss: 64.90365134179592\n",
      "Epoch 401/1000, Loss: 56.54144813865423\n",
      "Epoch 501/1000, Loss: 55.897232234478\n",
      "Epoch 601/1000, Loss: 51.54749147221446\n",
      "Epoch 701/1000, Loss: 52.32152093201876\n",
      "Epoch 801/1000, Loss: 46.192381370812654\n",
      "Epoch 901/1000, Loss: 48.13287205994129\n",
      "Test MSE: 1.2193962335586548\n",
      "Test RMSE: 1.1042627096176147\n",
      "Test MAE: 0.695110559463501\n",
      "Test R^2: 0.7752118032844304\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network architecture with multiple hidden layers and dropout\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        layers = []\n",
    "        previous_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(previous_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))  # Adding dropout\n",
    "            previous_dim = hidden_dim\n",
    "        layers.append(nn.Linear(previous_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_data.shape[1]  # Number of molecular descriptors\n",
    "hidden_dims = [512, 512, 512]  # Best according to grid search\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function (nn.MSELoss) and optimizer with weight decay (L2 regularization)\n",
    "model = SolubilityPredictor(input_dim, hidden_dims, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy().flatten()\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'solubility_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to make it classification instead of regression? No, just calculate how correct it is with a cut-off of 0.5 after predictions. \n",
    "can choose to say the values between 0.4 and 0.6 are inconclusive\n",
    "\n",
    "Also, use logK(%F) as it has a more balanced distribution\n",
    "\n",
    "See \"Critical Evaluation of Human Oral Bioavailability for Pharmaceutical Drugs by Using Various Cheminformatics Approaches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/Merged_Bioavailibility.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1587\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### DATA PREPARATION ####\n",
    "# try with new data\n",
    "\n",
    "#remove nan values from data\n",
    "data = data.dropna(subset=['logK(%F)'])\n",
    "\n",
    "#get x and y data (x is the molecular descriptors, y is the solubility)\n",
    "y_data = data[\"logK(%F)\"]\n",
    "\n",
    "print(len(y_data))\n",
    "\n",
    "#generating features and making sure it's the same features as previous model\n",
    "smiles = data[\"SMILES\"]\n",
    "featurizer = RDKitDescriptors()\n",
    "features = featurizer.featurize(smiles)\n",
    "X_data = features[:,used_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_data.shape[1])\n",
    "if True in np.isnan(X_data):\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2739898996323888"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation using train_test_split while retaining indices\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X_data, y_data, data.index, test_size=0.2, random_state=42)\n",
    "# Convert y pandas Series to NumPy array\n",
    "y_train = y_train.values.astype(np.float32)\n",
    "y_test = y_test.values.astype(np.float32)\n",
    "\n",
    "#scale x values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#convert data to pytorch tensors (like numpy arrays but for pytorch)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Reshape the target tensor to match the shape of the output tensor\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "# Reshape the target tensor to match the shape of the output tensor\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define model again\n",
    "\n",
    "# Define the neural network architecture with multiple hidden layers\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        layers = []\n",
    "        previous_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(previous_dim, hidden_dim))\n",
    "            layers.append(nn.Tanh())\n",
    "            previous_dim = hidden_dim\n",
    "        layers.append(nn.Linear(previous_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.activation = nn.Tanh() #best according to grid search\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_data.shape[1]  # Number of molecular descriptors\n",
    "hidden_dim = [512,512,512]  # (512,512,512) Best according to grid search\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "#loading saved model\n",
    "model = SolubilityPredictor(input_dim, hidden_dim, output_dim)\n",
    "model.load_state_dict(torch.load('solubility_model.pth'))\n",
    "\n",
    "# Optionally, modify the final layer if the output dimension is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BioavailabilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(BioavailabilityPredictor, self).__init__()\n",
    "        self.base_model = SolubilityPredictor(input_dim, hidden_dims, output_dim)\n",
    "        \n",
    "        # New layers for bioavailability prediction\n",
    "        self.new_layers = nn.Sequential(\n",
    "            nn.Linear(output_dim, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.new_layers(x)\n",
    "        return x\n",
    "\n",
    "# Create the new model\n",
    "bioavailability_model = BioavailabilityPredictor(input_dim, hidden_dim, output_dim)\n",
    "bioavailability_model.base_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initially freeze the pre-trained layers\n",
    "for param in bioavailability_model.base_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 139.46553444862366\n",
      "Epoch 101/500, Loss: 135.1947499215603\n",
      "Epoch 201/500, Loss: 135.11916008591652\n",
      "Epoch 301/500, Loss: 132.65568074584007\n",
      "Epoch 401/500, Loss: 132.32993498444557\n"
     ]
    }
   ],
   "source": [
    "# train only the new layers added for the task:\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(bioavailability_model.new_layers.parameters(), lr=learning_rate)\n",
    "\n",
    "#setting a smaller batch size for fine-tuning:\n",
    "batch_size = 16\n",
    "\n",
    "#create dataloader for batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set the model to training mode\n",
    "bioavailability_model.train()\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "# Training loop for new layers\n",
    "for epoch in range(num_epochs):\n",
    "    bioavailability_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = bioavailability_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000, Loss: 132.27835100889206\n",
      "Epoch 101/4000, Loss: 96.57175113260746\n",
      "Epoch 201/4000, Loss: 80.25051034241915\n",
      "Epoch 301/4000, Loss: 66.69406558573246\n",
      "Epoch 401/4000, Loss: 59.995260149240494\n",
      "Epoch 501/4000, Loss: 49.507852237671614\n",
      "Epoch 601/4000, Loss: 42.34172458574176\n",
      "Epoch 701/4000, Loss: 34.174675799906254\n",
      "Epoch 801/4000, Loss: 29.44404430873692\n",
      "Epoch 901/4000, Loss: 25.142369478708133\n",
      "Epoch 1001/4000, Loss: 21.815903378650546\n",
      "Epoch 1101/4000, Loss: 19.06011823983863\n",
      "Epoch 1201/4000, Loss: 16.48091491078958\n",
      "Epoch 1301/4000, Loss: 14.515323223546147\n",
      "Epoch 1401/4000, Loss: 13.718736708629876\n",
      "Epoch 1501/4000, Loss: 10.543535957112908\n",
      "Epoch 1601/4000, Loss: 8.771212974213995\n",
      "Epoch 1701/4000, Loss: 7.895337809924968\n",
      "Epoch 1801/4000, Loss: 7.160496835713275\n",
      "Epoch 1901/4000, Loss: 6.5896025251131505\n",
      "Epoch 2001/4000, Loss: 5.727444467833266\n",
      "Epoch 2101/4000, Loss: 6.027607025345787\n",
      "Epoch 2201/4000, Loss: 5.021162707125768\n",
      "Epoch 2301/4000, Loss: 4.628999706124887\n",
      "Epoch 2401/4000, Loss: 4.369435470784083\n",
      "Epoch 2501/4000, Loss: 3.866989053087309\n",
      "Epoch 2601/4000, Loss: 3.5437301305355504\n",
      "Epoch 2701/4000, Loss: 3.0986634532455355\n",
      "Epoch 2801/4000, Loss: 2.9351635592756793\n",
      "Epoch 2901/4000, Loss: 2.7228655220242217\n",
      "Epoch 3001/4000, Loss: 2.2291215664008632\n",
      "Epoch 3101/4000, Loss: 1.8462673455942422\n",
      "Epoch 3201/4000, Loss: 1.658312208368443\n",
      "Epoch 3301/4000, Loss: 1.5878552503418177\n",
      "Epoch 3401/4000, Loss: 1.2938749835593626\n",
      "Epoch 3501/4000, Loss: 1.337558648956474\n",
      "Epoch 3601/4000, Loss: 1.2894409066357184\n",
      "Epoch 3701/4000, Loss: 1.2067975196114276\n",
      "Epoch 3801/4000, Loss: 1.1701668058522046\n",
      "Epoch 3901/4000, Loss: 1.257735883409623\n"
     ]
    }
   ],
   "source": [
    "#fine-tune it by unfreezing some of the pre-trained layers\n",
    "\n",
    "# Unfreeze some of the pre-trained layers\n",
    "for param in bioavailability_model.base_model.network[-4].parameters():  # Unfreeze the second last layer\n",
    "    param.requires_grad = True\n",
    "for param in bioavailability_model.base_model.network[-7].parameters():  # Unfreeze the third last layer (optional)\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Update the optimizer to include the parameters of the unfrozen layers\n",
    "optimizer = optim.Adam(bioavailability_model.parameters(), lr=learning_rate / 10)\n",
    "\n",
    "#maybe more epochs?\n",
    "num_epochs = 4000\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(num_epochs):\n",
    "    bioavailability_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = bioavailability_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 3.240316867828369\n",
      "Test RMSE: 1.8000880479812622\n",
      "Test MAE: 1.3673293590545654\n",
      "Test R^2: -1.1959190894102032\n"
     ]
    }
   ],
   "source": [
    "bioavailability_model.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Perform the forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    test_predictions_tensor = bioavailability_model(X_test_tensor)\n",
    "\n",
    "# Convert predictions back to NumPy array\n",
    "test_predictions = test_predictions_tensor.numpy().flatten()\n",
    "\n",
    "# Calculate additional metrics\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame to store the results\n",
    "results_df = data.loc[test_indices, [\"logK(%F)\"]].copy()\n",
    "\n",
    "# Add the test predictions to the DataFrame\n",
    "results_df[\"Test Prediction\"] = test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6037735849056604\n"
     ]
    }
   ],
   "source": [
    "results_df = results_df.reset_index(drop=True)\n",
    "\n",
    "# count correct predictions\n",
    "count_total = len(results_df[\"logK(%F)\"])\n",
    "\n",
    "count_correct = 0\n",
    "for i in range(count_total):\n",
    "    if results_df[\"Test Prediction\"][i] > 0.5 and results_df[\"logK(%F)\"][i] > 0.5:\n",
    "        count_correct += 1\n",
    "    elif results_df[\"Test Prediction\"][i] < 0.5 and results_df[\"logK(%F)\"][i] < 0.5:\n",
    "        count_correct += 1\n",
    "\n",
    "print(count_correct/count_total)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) using sensitivity (percentage of high oral bioavailable drugs predict-\n",
    "ed correctly), specificity (percentage of low oral bioavailable\n",
    "drugs predicted correctly), and CCR (correct classification\n",
    "rate or balanced accuracy) for CTG models; and \n",
    "2) Pearson’s multiple linear correlation coefficient (R2) and mean absolute\n",
    "error (MAE) for CNT models\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.5192307692307693\n",
      "Specificity: 0.6448598130841121\n",
      "Correct classfication rate: 58.20452911574407\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics\n",
    "TP = ((results_df[\"Test Prediction\"] > 0.5) & (results_df[\"logK(%F)\"] > 0.5)).sum()\n",
    "FN = ((results_df[\"Test Prediction\"] < 0.5) & (results_df[\"logK(%F)\"] > 0.5)).sum()\n",
    "TN = ((results_df[\"Test Prediction\"] < 0.5) & (results_df[\"logK(%F)\"] < 0.5)).sum()\n",
    "FP = ((results_df[\"Test Prediction\"] > 0.5) & (results_df[\"logK(%F)\"] < 0.5)).sum()\n",
    "\n",
    "sensitivity = TP/(TP+FN)\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "\n",
    "specificity = TN/(TN+FP)\n",
    "print(f\"Specificity: {specificity}\")\n",
    "\n",
    "CCR = ((sensitivity+specificity)/2)*100\n",
    "print(f\"Correct classfication rate: {CCR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bioavailability_model.state_dict(), 'bioavailability_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
