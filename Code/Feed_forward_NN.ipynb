{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article from Philippe:\n",
    "https://practicalcheminformatics.blogspot.com/2023/06/getting-real-with-molecular-property.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for running in colab:\n",
    "\n",
    "# !pip install rdkit\n",
    "# !pip install deepchem\n",
    "\n",
    "\n",
    "# !wget https://raw.githubusercontent.com/NinaB99/AI-for-Chemistry/main/Data/ADME_public_set_3521.csv\n",
    "# !wget https://raw.githubusercontent.com/NinaB99/AI-for-Chemistry/main/Data/11095_2013_1222_MOESM2_ESM.csv\n",
    "\n",
    "# load biogen data\n",
    "# import pandas as pd\n",
    "\n",
    "# biogen_data=pd.read_csv(\"ADME_public_set_3521.csv\")\n",
    "# print(biogen_data.columns)\n",
    "\n",
    "# #load bioavailabity data\n",
    "# bio_avail_data = pd.read_csv(\"11095_2013_1222_MOESM2_ESM.csv\",sep=\";\")\n",
    "# print(bio_avail_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from deepchem.feat import RDKitDescriptors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biogen_data=pd.read_csv(\"../Data/Biogen.csv\")\n",
    "# print(biogen_data.columns)\n",
    "\n",
    "# #new data\n",
    "# data = pd.read_csv(\"../Data/CuratedSol.csv\")\n",
    "# print(data.columns)\n",
    "\n",
    "# #load bioavailabity data\n",
    "# bio_avail_data = pd.read_csv(\"../Data/Bioavailibility.csv\")\n",
    "# #print(bio_avail_data.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Solubility_log(mol/L)', 'SMILES', 'MolW(Da)', 'NumHAcceptors',\n",
      "       'NumHDonors', 'LogP', 'Lipinski_rule'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#combined data\n",
    "#Merged_2 has curated, biogen and esol data. merged_solubility also has one other\n",
    "data = pd.read_csv(\"../Data/Merged_solubility.csv\")\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disable warnings from RDKit\n",
    "from rdkit import RDLogger \n",
    "RDLogger.DisableLog('rdApp.*') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize(Dataframe: pd.DataFrame):\n",
    "    \n",
    "    \"\"\"Canonicalizes the SMILES from Dataframe. A column called 'SMILES' is requiered\n",
    "\n",
    "    Args: Dataframe with 'SMILES' column contaning smiles. \n",
    "    \"\"\"\n",
    "    \n",
    "    Dataframe['SMILES'] = Dataframe['SMILES'].apply(lambda x: Chem.MolToSmiles(Chem.MolFromSmiles(x))) #canonicalize smiles from a Dataframe                                          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonicalize(data)\n",
    "data = data.drop_duplicates(subset=\"SMILES\", keep='first') #prioritizes curated, biogen, then esol and then pharmaceutical database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12309\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to generate features from SMILES strings using RDKit descriptors\n",
    "def generate_features(smiles_list):\n",
    "    featurizer = RDKitDescriptors()\n",
    "    features = featurizer.featurize(smiles_list)\n",
    "    # Drop features containing invalid values\n",
    "    used_features = ~np.isnan(features).any(axis=0)\n",
    "    features = features[:, ~np.isnan(features).any(axis=0)]\n",
    "    return features, used_features\n",
    "\n",
    "#remove nan values from data\n",
    "data = data.dropna(subset=['Solubility_log(mol/L)'])\n",
    "\n",
    "#get x and y data (x is the molecular descriptors, y is the solubility)\n",
    "y_data = data[\"Solubility_log(mol/L)\"]\n",
    "\n",
    "print(len(y_data))\n",
    "\n",
    "# Generate features from SMILES data (get smiles from df)\n",
    "smiles = data[\"SMILES\"]\n",
    "X_data, used_features = generate_features(smiles)\n",
    "\n",
    "#split data into training and validation using train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "# Convert y pandas Series to NumPy array\n",
    "y_train = y_train.values.astype(np.float32)\n",
    "y_test = y_test.values.astype(np.float32)\n",
    "\n",
    "#scale x values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#convert data to pytorch tensors (like numpy arrays but for pytorch)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Reshape the target tensor to match the shape of the output tensor\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "# Reshape the target tensor to match the shape of the output tensor\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.237340946572074"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation function:\n",
    "https://encord.com/blog/activation-functions-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[CV] END activation=relu, hidden_layer_sizes=(512, 512), learning_rate_init=0.01; total time= 4.0min\n"
     ]
    }
   ],
   "source": [
    "#### CREATING MODEL ####\n",
    "\n",
    "# Define the neural network architecture\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_data.shape[1]  # Number of molecular descriptors\n",
    "hidden_dim = 256 #gets overruled by grid search\n",
    "output_dim = 1\n",
    "learning_rate = 0.001 #gets overruled by the grid search\n",
    "num_epochs = 600\n",
    "batch_size = 32\n",
    "\n",
    "#create dataloader for batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#initialize model, loss function (nn.MSELoss) and optimizer\n",
    "model = SolubilityPredictor(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "##### Grid-search for best hyperparameters #####\n",
    "# Define your hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(512,512),(256,256,256),(512,512,512)],  # Number of neurons in the hidden layer(s)\n",
    "    'activation': ['relu', 'tanh','sigmoid'],  # Activation function\n",
    "    #'solver': ['adam', 'sgd'],  # Optimization algorithm\n",
    "    #'learning_rate': ['constant', 'adaptive'],  # Learning rate schedule\n",
    "    'learning_rate_init':[0.01,0.001,0.0001]\n",
    "}\n",
    "\n",
    "# Create an MLPRegressor object\n",
    "mlp = MLPRegressor(max_iter=num_epochs, batch_size=batch_size)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_score = model.score(X_test, y_test)\n",
    "print(\"Test Score (R^2):\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.3812762971349799\n",
      "Test RMSE: 0.6174757461916864\n",
      "Test MAE: 0.4129440122100839\n",
      "Test R^2: 0.1923752586047588\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate additional metrics\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800, Loss: 674.1078715324402\n",
      "Epoch 2/800, Loss: 418.01693844795227\n",
      "Epoch 3/800, Loss: 359.9462621808052\n",
      "Epoch 4/800, Loss: 316.6794626712799\n",
      "Epoch 5/800, Loss: 293.5718384087086\n",
      "Epoch 6/800, Loss: 269.54273080825806\n",
      "Epoch 7/800, Loss: 261.95671385526657\n",
      "Epoch 8/800, Loss: 252.46561321616173\n",
      "Epoch 9/800, Loss: 230.47504185140133\n",
      "Epoch 10/800, Loss: 223.06986847519875\n",
      "Epoch 11/800, Loss: 216.3349032998085\n",
      "Epoch 12/800, Loss: 198.01436106860638\n",
      "Epoch 13/800, Loss: 193.48965829610825\n",
      "Epoch 14/800, Loss: 178.9999898970127\n",
      "Epoch 15/800, Loss: 174.1583729982376\n",
      "Epoch 16/800, Loss: 172.91613468527794\n",
      "Epoch 17/800, Loss: 166.9479850679636\n",
      "Epoch 18/800, Loss: 163.5591824427247\n",
      "Epoch 19/800, Loss: 155.17488899827003\n",
      "Epoch 20/800, Loss: 146.42333422601223\n",
      "Epoch 21/800, Loss: 140.38317926973104\n",
      "Epoch 22/800, Loss: 135.60137084126472\n",
      "Epoch 23/800, Loss: 131.39678034186363\n",
      "Epoch 24/800, Loss: 130.4039497077465\n",
      "Epoch 25/800, Loss: 124.5153890401125\n",
      "Epoch 26/800, Loss: 122.97401008754969\n",
      "Epoch 27/800, Loss: 117.53103009611368\n",
      "Epoch 28/800, Loss: 119.66892769932747\n",
      "Epoch 29/800, Loss: 110.97249632328749\n",
      "Epoch 30/800, Loss: 109.2535440325737\n",
      "Epoch 31/800, Loss: 101.59093255549669\n",
      "Epoch 32/800, Loss: 101.62780858203769\n",
      "Epoch 33/800, Loss: 103.8067141994834\n",
      "Epoch 34/800, Loss: 99.35086154937744\n",
      "Epoch 35/800, Loss: 99.7599283605814\n",
      "Epoch 36/800, Loss: 91.6475505977869\n",
      "Epoch 37/800, Loss: 91.83428214490414\n",
      "Epoch 38/800, Loss: 92.00708314031363\n",
      "Epoch 39/800, Loss: 89.09873834252357\n",
      "Epoch 40/800, Loss: 87.20413013547659\n",
      "Epoch 41/800, Loss: 91.66721947491169\n",
      "Epoch 42/800, Loss: 84.23743482679129\n",
      "Epoch 43/800, Loss: 81.90544587001204\n",
      "Epoch 44/800, Loss: 80.35560523718596\n",
      "Epoch 45/800, Loss: 75.68119418621063\n",
      "Epoch 46/800, Loss: 77.84961472451687\n",
      "Epoch 47/800, Loss: 74.22904362529516\n",
      "Epoch 48/800, Loss: 78.86914648860693\n",
      "Epoch 49/800, Loss: 75.09840553626418\n",
      "Epoch 50/800, Loss: 73.77857327833772\n",
      "Epoch 51/800, Loss: 70.00709591805935\n",
      "Epoch 52/800, Loss: 67.31083607673645\n",
      "Epoch 53/800, Loss: 74.7760176435113\n",
      "Epoch 54/800, Loss: 68.47552504017949\n",
      "Epoch 55/800, Loss: 68.28947395831347\n",
      "Epoch 56/800, Loss: 64.27103465422988\n",
      "Epoch 57/800, Loss: 66.4831517636776\n",
      "Epoch 58/800, Loss: 65.09461940824986\n",
      "Epoch 59/800, Loss: 66.5011345334351\n",
      "Epoch 60/800, Loss: 58.769939582794905\n",
      "Epoch 61/800, Loss: 58.39994964748621\n",
      "Epoch 62/800, Loss: 58.38477383926511\n",
      "Epoch 63/800, Loss: 58.304381642490625\n",
      "Epoch 64/800, Loss: 60.21233608573675\n",
      "Epoch 65/800, Loss: 56.716945827007294\n",
      "Epoch 66/800, Loss: 54.232441330328584\n",
      "Epoch 67/800, Loss: 57.681596994400024\n",
      "Epoch 68/800, Loss: 56.83752387389541\n",
      "Epoch 69/800, Loss: 53.626541528850794\n",
      "Epoch 70/800, Loss: 53.79815232753754\n",
      "Epoch 71/800, Loss: 52.01166427321732\n",
      "Epoch 72/800, Loss: 51.59170509129763\n",
      "Epoch 73/800, Loss: 52.79653322324157\n",
      "Epoch 74/800, Loss: 50.592700622975826\n",
      "Epoch 75/800, Loss: 48.21184456348419\n",
      "Epoch 76/800, Loss: 50.781568966805935\n",
      "Epoch 77/800, Loss: 53.752461053431034\n",
      "Epoch 78/800, Loss: 47.89822069182992\n",
      "Epoch 79/800, Loss: 47.99003843963146\n",
      "Epoch 80/800, Loss: 48.34880203381181\n",
      "Epoch 81/800, Loss: 48.26005802676082\n",
      "Epoch 82/800, Loss: 44.766456263139844\n",
      "Epoch 83/800, Loss: 50.844796005636454\n",
      "Epoch 84/800, Loss: 46.169764798134565\n",
      "Epoch 85/800, Loss: 47.487435430288315\n",
      "Epoch 86/800, Loss: 47.09302728250623\n",
      "Epoch 87/800, Loss: 47.2875079549849\n",
      "Epoch 88/800, Loss: 41.62055914849043\n",
      "Epoch 89/800, Loss: 44.49833135120571\n",
      "Epoch 90/800, Loss: 43.84067891724408\n",
      "Epoch 91/800, Loss: 41.71082712709904\n",
      "Epoch 92/800, Loss: 41.103978492319584\n",
      "Epoch 93/800, Loss: 38.64281848445535\n",
      "Epoch 94/800, Loss: 43.272664902731776\n",
      "Epoch 95/800, Loss: 41.86560401506722\n",
      "Epoch 96/800, Loss: 43.487695118412375\n",
      "Epoch 97/800, Loss: 41.63134776800871\n",
      "Epoch 98/800, Loss: 39.07023997697979\n",
      "Epoch 99/800, Loss: 38.114327508956194\n",
      "Epoch 100/800, Loss: 36.919351855292916\n",
      "Epoch 101/800, Loss: 41.84741199016571\n",
      "Epoch 102/800, Loss: 45.09124714508653\n",
      "Epoch 103/800, Loss: 40.41076138056815\n",
      "Epoch 104/800, Loss: 38.95571899227798\n",
      "Epoch 105/800, Loss: 36.895436910912395\n",
      "Epoch 106/800, Loss: 35.97216014191508\n",
      "Epoch 107/800, Loss: 32.89273457787931\n",
      "Epoch 108/800, Loss: 37.269821763038635\n",
      "Epoch 109/800, Loss: 39.25222105719149\n",
      "Epoch 110/800, Loss: 41.465616051107645\n",
      "Epoch 111/800, Loss: 36.02415221184492\n",
      "Epoch 112/800, Loss: 35.24013996310532\n",
      "Epoch 113/800, Loss: 35.36891773343086\n",
      "Epoch 114/800, Loss: 34.738425428047776\n",
      "Epoch 115/800, Loss: 36.96182222664356\n",
      "Epoch 116/800, Loss: 32.44148188456893\n",
      "Epoch 117/800, Loss: 34.41629401780665\n",
      "Epoch 118/800, Loss: 34.69885155837983\n",
      "Epoch 119/800, Loss: 34.81499992124736\n",
      "Epoch 120/800, Loss: 34.31400088220835\n",
      "Epoch 121/800, Loss: 33.196909725666046\n",
      "Epoch 122/800, Loss: 34.5509023591876\n",
      "Epoch 123/800, Loss: 33.327412663027644\n",
      "Epoch 124/800, Loss: 33.484316524118185\n",
      "Epoch 125/800, Loss: 32.792431727051735\n",
      "Epoch 126/800, Loss: 33.47846275381744\n",
      "Epoch 127/800, Loss: 32.33105567470193\n",
      "Epoch 128/800, Loss: 34.36828189715743\n",
      "Epoch 129/800, Loss: 33.36786790750921\n",
      "Epoch 130/800, Loss: 29.898736582137644\n",
      "Epoch 131/800, Loss: 30.009010499343276\n",
      "Epoch 132/800, Loss: 32.324601670727134\n",
      "Epoch 133/800, Loss: 31.590006874874234\n",
      "Epoch 134/800, Loss: 31.42400384694338\n",
      "Epoch 135/800, Loss: 30.763303110376\n",
      "Epoch 136/800, Loss: 30.030069132335484\n",
      "Epoch 137/800, Loss: 33.74885933659971\n",
      "Epoch 138/800, Loss: 33.514310323633254\n",
      "Epoch 139/800, Loss: 35.73102844692767\n",
      "Epoch 140/800, Loss: 30.90660868026316\n",
      "Epoch 141/800, Loss: 31.62491009198129\n",
      "Epoch 142/800, Loss: 29.801331151276827\n",
      "Epoch 143/800, Loss: 27.840221762657166\n",
      "Epoch 144/800, Loss: 28.299091923981905\n",
      "Epoch 145/800, Loss: 28.827082955278456\n",
      "Epoch 146/800, Loss: 32.58216437511146\n",
      "Epoch 147/800, Loss: 29.686743097379804\n",
      "Epoch 148/800, Loss: 27.45689177699387\n",
      "Epoch 149/800, Loss: 30.065764678642154\n",
      "Epoch 150/800, Loss: 33.61280430667102\n",
      "Epoch 151/800, Loss: 27.72076466679573\n",
      "Epoch 152/800, Loss: 30.489853266626596\n",
      "Epoch 153/800, Loss: 27.63458062708378\n",
      "Epoch 154/800, Loss: 24.19283869676292\n",
      "Epoch 155/800, Loss: 27.482895091176033\n",
      "Epoch 156/800, Loss: 26.67188704200089\n",
      "Epoch 157/800, Loss: 28.302666450850666\n",
      "Epoch 158/800, Loss: 30.314204456284642\n",
      "Epoch 159/800, Loss: 30.080909804441035\n",
      "Epoch 160/800, Loss: 28.469741807319224\n",
      "Epoch 161/800, Loss: 26.441408522427082\n",
      "Epoch 162/800, Loss: 24.390292825177312\n",
      "Epoch 163/800, Loss: 24.300503293052316\n",
      "Epoch 164/800, Loss: 26.85112751275301\n",
      "Epoch 165/800, Loss: 28.874834230169654\n",
      "Epoch 166/800, Loss: 26.43684801645577\n",
      "Epoch 167/800, Loss: 27.223926704376936\n",
      "Epoch 168/800, Loss: 27.481913927942514\n",
      "Epoch 169/800, Loss: 27.424059410579503\n",
      "Epoch 170/800, Loss: 24.7395263556391\n",
      "Epoch 171/800, Loss: 23.852758590131998\n",
      "Epoch 172/800, Loss: 24.315910706296563\n",
      "Epoch 173/800, Loss: 28.215076837688684\n",
      "Epoch 174/800, Loss: 31.698325904086232\n",
      "Epoch 175/800, Loss: 28.7739333845675\n",
      "Epoch 176/800, Loss: 22.51710541266948\n",
      "Epoch 177/800, Loss: 26.074263529852033\n",
      "Epoch 178/800, Loss: 24.574218451045454\n",
      "Epoch 179/800, Loss: 24.419927764683962\n",
      "Epoch 180/800, Loss: 23.544328335672617\n",
      "Epoch 181/800, Loss: 27.514083340764046\n",
      "Epoch 182/800, Loss: 26.62180124502629\n",
      "Epoch 183/800, Loss: 25.034823140129447\n",
      "Epoch 184/800, Loss: 26.916205743327737\n",
      "Epoch 185/800, Loss: 23.448672112077475\n",
      "Epoch 186/800, Loss: 21.953459619544446\n",
      "Epoch 187/800, Loss: 23.172171941958368\n",
      "Epoch 188/800, Loss: 24.18525518104434\n",
      "Epoch 189/800, Loss: 22.857352734543383\n",
      "Epoch 190/800, Loss: 23.071222238242626\n",
      "Epoch 191/800, Loss: 21.504159251227975\n",
      "Epoch 192/800, Loss: 22.45458369795233\n",
      "Epoch 193/800, Loss: 25.41165786422789\n",
      "Epoch 194/800, Loss: 24.187371019273996\n",
      "Epoch 195/800, Loss: 23.74121981486678\n",
      "Epoch 196/800, Loss: 23.276449042372406\n",
      "Epoch 197/800, Loss: 22.825411271303892\n",
      "Epoch 198/800, Loss: 25.989279782399535\n",
      "Epoch 199/800, Loss: 22.337568542920053\n",
      "Epoch 200/800, Loss: 25.923419058322906\n",
      "Epoch 201/800, Loss: 22.08515604585409\n",
      "Epoch 202/800, Loss: 24.026467715390027\n",
      "Epoch 203/800, Loss: 20.92959591280669\n",
      "Epoch 204/800, Loss: 21.325199908576906\n",
      "Epoch 205/800, Loss: 23.809789450839162\n",
      "Epoch 206/800, Loss: 28.10578512493521\n",
      "Epoch 207/800, Loss: 25.305335563607514\n",
      "Epoch 208/800, Loss: 19.2353635719046\n",
      "Epoch 209/800, Loss: 25.79530420061201\n",
      "Epoch 210/800, Loss: 24.43638119008392\n",
      "Epoch 211/800, Loss: 22.875678937882185\n",
      "Epoch 212/800, Loss: 21.43607884272933\n",
      "Epoch 213/800, Loss: 22.64787867665291\n",
      "Epoch 214/800, Loss: 22.07784591242671\n",
      "Epoch 215/800, Loss: 20.003907695412636\n",
      "Epoch 216/800, Loss: 21.483279887586832\n",
      "Epoch 217/800, Loss: 20.885776486247778\n",
      "Epoch 218/800, Loss: 20.77788612898439\n",
      "Epoch 219/800, Loss: 21.850574252195656\n",
      "Epoch 220/800, Loss: 21.753774498589337\n",
      "Epoch 221/800, Loss: 22.34641665685922\n",
      "Epoch 222/800, Loss: 21.72566144168377\n",
      "Epoch 223/800, Loss: 22.553376112133265\n",
      "Epoch 224/800, Loss: 22.480646681971848\n",
      "Epoch 225/800, Loss: 21.88673598971218\n",
      "Epoch 226/800, Loss: 20.843994825147092\n",
      "Epoch 227/800, Loss: 19.683905202895403\n",
      "Epoch 228/800, Loss: 19.3134227367118\n",
      "Epoch 229/800, Loss: 21.690598574467003\n",
      "Epoch 230/800, Loss: 23.816309618763626\n",
      "Epoch 231/800, Loss: 20.74106786865741\n",
      "Epoch 232/800, Loss: 21.426589590497315\n",
      "Epoch 233/800, Loss: 19.26327623706311\n",
      "Epoch 234/800, Loss: 20.166643114760518\n",
      "Epoch 235/800, Loss: 20.953245277516544\n",
      "Epoch 236/800, Loss: 20.084662945009768\n",
      "Epoch 237/800, Loss: 22.981610171496868\n",
      "Epoch 238/800, Loss: 19.747606847435236\n",
      "Epoch 239/800, Loss: 19.43241781555116\n",
      "Epoch 240/800, Loss: 18.101070820353925\n",
      "Epoch 241/800, Loss: 19.190462772734463\n",
      "Epoch 242/800, Loss: 19.601700043305755\n",
      "Epoch 243/800, Loss: 22.22602562326938\n",
      "Epoch 244/800, Loss: 21.091407888568938\n",
      "Epoch 245/800, Loss: 20.94333270471543\n",
      "Epoch 246/800, Loss: 21.640766587108374\n",
      "Epoch 247/800, Loss: 19.755929813720286\n",
      "Epoch 248/800, Loss: 19.087086364626884\n",
      "Epoch 249/800, Loss: 22.020094114355743\n",
      "Epoch 250/800, Loss: 20.947502406314015\n",
      "Epoch 251/800, Loss: 20.706886852160096\n",
      "Epoch 252/800, Loss: 21.04358682502061\n",
      "Epoch 253/800, Loss: 19.937329541891813\n",
      "Epoch 254/800, Loss: 19.317694602534175\n",
      "Epoch 255/800, Loss: 19.299114026129246\n",
      "Epoch 256/800, Loss: 19.59708624985069\n",
      "Epoch 257/800, Loss: 21.068957736715674\n",
      "Epoch 258/800, Loss: 19.76093232538551\n",
      "Epoch 259/800, Loss: 18.468018606305122\n",
      "Epoch 260/800, Loss: 18.884617015719414\n",
      "Epoch 261/800, Loss: 20.101106563583016\n",
      "Epoch 262/800, Loss: 18.674945572391152\n",
      "Epoch 263/800, Loss: 19.787871080450714\n",
      "Epoch 264/800, Loss: 18.068882345221937\n",
      "Epoch 265/800, Loss: 18.230351694393903\n",
      "Epoch 266/800, Loss: 16.103402799926698\n",
      "Epoch 267/800, Loss: 18.64488671068102\n",
      "Epoch 268/800, Loss: 19.255787354893982\n",
      "Epoch 269/800, Loss: 20.20114585198462\n",
      "Epoch 270/800, Loss: 26.022261932492256\n",
      "Epoch 271/800, Loss: 19.456135291606188\n",
      "Epoch 272/800, Loss: 18.860224274918437\n",
      "Epoch 273/800, Loss: 20.31931418646127\n",
      "Epoch 274/800, Loss: 16.490329836960882\n",
      "Epoch 275/800, Loss: 16.6837786603719\n",
      "Epoch 276/800, Loss: 16.548680872190744\n",
      "Epoch 277/800, Loss: 16.517641986720264\n",
      "Epoch 278/800, Loss: 16.75172006385401\n",
      "Epoch 279/800, Loss: 19.258267749100924\n",
      "Epoch 280/800, Loss: 18.38526999205351\n",
      "Epoch 281/800, Loss: 20.078048216179013\n",
      "Epoch 282/800, Loss: 16.197336570359766\n",
      "Epoch 283/800, Loss: 16.221094192005694\n",
      "Epoch 284/800, Loss: 17.746931664645672\n",
      "Epoch 285/800, Loss: 19.439914191141725\n",
      "Epoch 286/800, Loss: 17.650554427411407\n",
      "Epoch 287/800, Loss: 19.720617844723165\n",
      "Epoch 288/800, Loss: 18.511236121878028\n",
      "Epoch 289/800, Loss: 16.29426501505077\n",
      "Epoch 290/800, Loss: 16.772027054801583\n",
      "Epoch 291/800, Loss: 16.39285167120397\n",
      "Epoch 292/800, Loss: 16.37506799120456\n",
      "Epoch 293/800, Loss: 16.39877337217331\n",
      "Epoch 294/800, Loss: 17.4915780890733\n",
      "Epoch 295/800, Loss: 17.352506695315242\n",
      "Epoch 296/800, Loss: 16.975765878800303\n",
      "Epoch 297/800, Loss: 21.15669001173228\n",
      "Epoch 298/800, Loss: 21.211159623228014\n",
      "Epoch 299/800, Loss: 18.18246010504663\n",
      "Epoch 300/800, Loss: 17.912829414010048\n",
      "Epoch 301/800, Loss: 16.49389162985608\n",
      "Epoch 302/800, Loss: 16.65260689565912\n",
      "Epoch 303/800, Loss: 16.511204591952264\n",
      "Epoch 304/800, Loss: 16.417072162032127\n",
      "Epoch 305/800, Loss: 16.191247649490833\n",
      "Epoch 306/800, Loss: 17.570119532756507\n",
      "Epoch 307/800, Loss: 16.726974293589592\n",
      "Epoch 308/800, Loss: 17.972106150351465\n",
      "Epoch 309/800, Loss: 17.052890785969794\n",
      "Epoch 310/800, Loss: 16.457172469235957\n",
      "Epoch 311/800, Loss: 15.543025293387473\n",
      "Epoch 312/800, Loss: 16.086942518129945\n",
      "Epoch 313/800, Loss: 14.98080878984183\n",
      "Epoch 314/800, Loss: 15.75182399712503\n",
      "Epoch 315/800, Loss: 16.834858112968504\n",
      "Epoch 316/800, Loss: 16.567605289630592\n",
      "Epoch 317/800, Loss: 19.232507310807705\n",
      "Epoch 318/800, Loss: 20.969907022081316\n",
      "Epoch 319/800, Loss: 15.578337059821934\n",
      "Epoch 320/800, Loss: 15.222991228103638\n",
      "Epoch 321/800, Loss: 14.280904390849173\n",
      "Epoch 322/800, Loss: 15.216642443090677\n",
      "Epoch 323/800, Loss: 16.402717699762434\n",
      "Epoch 324/800, Loss: 14.71871054917574\n",
      "Epoch 325/800, Loss: 15.220323852263391\n",
      "Epoch 326/800, Loss: 15.376473364420235\n",
      "Epoch 327/800, Loss: 16.38900610152632\n",
      "Epoch 328/800, Loss: 15.752416568808258\n",
      "Epoch 329/800, Loss: 15.995044485665858\n",
      "Epoch 330/800, Loss: 16.78514523198828\n",
      "Epoch 331/800, Loss: 19.1005814736709\n",
      "Epoch 332/800, Loss: 17.596245002001524\n",
      "Epoch 333/800, Loss: 19.61219971626997\n",
      "Epoch 334/800, Loss: 16.084887964185327\n",
      "Epoch 335/800, Loss: 16.41789875458926\n",
      "Epoch 336/800, Loss: 15.209019992500544\n",
      "Epoch 337/800, Loss: 15.811819503083825\n",
      "Epoch 338/800, Loss: 15.383382606785744\n",
      "Epoch 339/800, Loss: 15.533932133577764\n",
      "Epoch 340/800, Loss: 16.807805099524558\n",
      "Epoch 341/800, Loss: 18.548695746809244\n",
      "Epoch 342/800, Loss: 17.076566302217543\n",
      "Epoch 343/800, Loss: 13.651966102886945\n",
      "Epoch 344/800, Loss: 13.821784347761422\n",
      "Epoch 345/800, Loss: 13.264612120576203\n",
      "Epoch 346/800, Loss: 14.905216889921576\n",
      "Epoch 347/800, Loss: 14.230977295897901\n",
      "Epoch 348/800, Loss: 15.977498325519264\n",
      "Epoch 349/800, Loss: 15.587619817815721\n",
      "Epoch 350/800, Loss: 14.233076125383377\n",
      "Epoch 351/800, Loss: 14.782354294322431\n",
      "Epoch 352/800, Loss: 14.637972072698176\n",
      "Epoch 353/800, Loss: 15.653107800520957\n",
      "Epoch 354/800, Loss: 15.915815909858793\n",
      "Epoch 355/800, Loss: 19.70205542538315\n",
      "Epoch 356/800, Loss: 16.06523159239441\n",
      "Epoch 357/800, Loss: 140.62503554672003\n",
      "Epoch 358/800, Loss: 17.10114784259349\n",
      "Epoch 359/800, Loss: 21.27441410999745\n",
      "Epoch 360/800, Loss: 15.128786356188357\n",
      "Epoch 361/800, Loss: 13.417328593321145\n",
      "Epoch 362/800, Loss: 12.268227687571198\n",
      "Epoch 363/800, Loss: 11.7272212728858\n",
      "Epoch 364/800, Loss: 13.433856943622231\n",
      "Epoch 365/800, Loss: 16.793777340557426\n",
      "Epoch 366/800, Loss: 13.924214166589081\n",
      "Epoch 367/800, Loss: 13.373773005791008\n",
      "Epoch 368/800, Loss: 13.519488048739731\n",
      "Epoch 369/800, Loss: 14.225117503665388\n",
      "Epoch 370/800, Loss: 12.47647077171132\n",
      "Epoch 371/800, Loss: 13.600880960933864\n",
      "Epoch 372/800, Loss: 14.875354590825737\n",
      "Epoch 373/800, Loss: 16.132394779007882\n",
      "Epoch 374/800, Loss: 15.507846139837056\n",
      "Epoch 375/800, Loss: 14.882235033903271\n",
      "Epoch 376/800, Loss: 13.971867719199508\n",
      "Epoch 377/800, Loss: 13.152527295053005\n",
      "Epoch 378/800, Loss: 13.238389781210572\n",
      "Epoch 379/800, Loss: 14.884377114009112\n",
      "Epoch 380/800, Loss: 15.312504542991519\n",
      "Epoch 381/800, Loss: 15.192251690663397\n",
      "Epoch 382/800, Loss: 14.727259472943842\n",
      "Epoch 383/800, Loss: 14.139367184601724\n",
      "Epoch 384/800, Loss: 13.834445416461676\n",
      "Epoch 385/800, Loss: 13.599575167987496\n",
      "Epoch 386/800, Loss: 14.111965340562165\n",
      "Epoch 387/800, Loss: 17.198056836612523\n",
      "Epoch 388/800, Loss: 14.669530706480145\n",
      "Epoch 389/800, Loss: 13.419022489804775\n",
      "Epoch 390/800, Loss: 13.508934800047427\n",
      "Epoch 391/800, Loss: 14.031851842068136\n",
      "Epoch 392/800, Loss: 16.967952278908342\n",
      "Epoch 393/800, Loss: 13.722877306863666\n",
      "Epoch 394/800, Loss: 12.873212238773704\n",
      "Epoch 395/800, Loss: 12.446607588790357\n",
      "Epoch 396/800, Loss: 13.471177679486573\n",
      "Epoch 397/800, Loss: 13.487603802699596\n",
      "Epoch 398/800, Loss: 13.994248025584966\n",
      "Epoch 399/800, Loss: 14.005866927094758\n",
      "Epoch 400/800, Loss: 14.541574665345252\n",
      "Epoch 401/800, Loss: 13.42371417582035\n",
      "Epoch 402/800, Loss: 14.96998815331608\n",
      "Epoch 403/800, Loss: 15.024429029319435\n",
      "Epoch 404/800, Loss: 17.74726422596723\n",
      "Epoch 405/800, Loss: 14.240510233677924\n",
      "Epoch 406/800, Loss: 14.206629186403006\n",
      "Epoch 407/800, Loss: 15.880141601432115\n",
      "Epoch 408/800, Loss: 14.14386141858995\n",
      "Epoch 409/800, Loss: 12.583085310645401\n",
      "Epoch 410/800, Loss: 12.57667851401493\n",
      "Epoch 411/800, Loss: 15.713781398255378\n",
      "Epoch 412/800, Loss: 16.631312192883343\n",
      "Epoch 413/800, Loss: 12.870896913576871\n",
      "Epoch 414/800, Loss: 13.920519245788455\n",
      "Epoch 415/800, Loss: 13.749731419607997\n",
      "Epoch 416/800, Loss: 12.274470248259604\n",
      "Epoch 417/800, Loss: 13.068435930646956\n",
      "Epoch 418/800, Loss: 12.936467790510505\n",
      "Epoch 419/800, Loss: 13.122395342681557\n",
      "Epoch 420/800, Loss: 14.381395742762834\n",
      "Epoch 421/800, Loss: 13.592645128257573\n",
      "Epoch 422/800, Loss: 16.694508250802755\n",
      "Epoch 423/800, Loss: 16.504855109378695\n",
      "Epoch 424/800, Loss: 12.925180399091914\n",
      "Epoch 425/800, Loss: 12.850931648164988\n",
      "Epoch 426/800, Loss: 11.523007734678686\n",
      "Epoch 427/800, Loss: 12.039195447228849\n",
      "Epoch 428/800, Loss: 11.880271730013192\n",
      "Epoch 429/800, Loss: 12.411618066951632\n",
      "Epoch 430/800, Loss: 12.728280512616038\n",
      "Epoch 431/800, Loss: 13.105180526617914\n",
      "Epoch 432/800, Loss: 13.88542141020298\n",
      "Epoch 433/800, Loss: 13.834557054098696\n",
      "Epoch 434/800, Loss: 14.548181506805122\n",
      "Epoch 435/800, Loss: 16.039337764959782\n",
      "Epoch 436/800, Loss: 15.651446190662682\n",
      "Epoch 437/800, Loss: 13.47435564827174\n",
      "Epoch 438/800, Loss: 12.288814584724605\n",
      "Epoch 439/800, Loss: 11.973081227391958\n",
      "Epoch 440/800, Loss: 12.605229957960546\n",
      "Epoch 441/800, Loss: 12.70294827548787\n",
      "Epoch 442/800, Loss: 12.834286584053189\n",
      "Epoch 443/800, Loss: 13.249820286408067\n",
      "Epoch 444/800, Loss: 13.979857622180134\n",
      "Epoch 445/800, Loss: 13.079892372712493\n",
      "Epoch 446/800, Loss: 16.805300147738308\n",
      "Epoch 447/800, Loss: 13.749107287731022\n",
      "Epoch 448/800, Loss: 12.937033611349761\n",
      "Epoch 449/800, Loss: 12.533180363010615\n",
      "Epoch 450/800, Loss: 12.90660529024899\n",
      "Epoch 451/800, Loss: 21.330715185496956\n",
      "Epoch 452/800, Loss: 14.357316556852311\n",
      "Epoch 453/800, Loss: 12.914578293450177\n",
      "Epoch 454/800, Loss: 12.875178955029696\n",
      "Epoch 455/800, Loss: 10.486256453441456\n",
      "Epoch 456/800, Loss: 11.13231773674488\n",
      "Epoch 457/800, Loss: 11.626366997603327\n",
      "Epoch 458/800, Loss: 11.874535771552473\n",
      "Epoch 459/800, Loss: 12.547270848415792\n",
      "Epoch 460/800, Loss: 11.702072396874428\n",
      "Epoch 461/800, Loss: 13.471974549815059\n",
      "Epoch 462/800, Loss: 15.680477235466242\n",
      "Epoch 463/800, Loss: 12.23792392294854\n",
      "Epoch 464/800, Loss: 12.044062913861126\n",
      "Epoch 465/800, Loss: 12.848574825096875\n",
      "Epoch 466/800, Loss: 11.680051126517355\n",
      "Epoch 467/800, Loss: 13.032851732801646\n",
      "Epoch 468/800, Loss: 12.848691157530993\n",
      "Epoch 469/800, Loss: 11.71982545638457\n",
      "Epoch 470/800, Loss: 12.479806396178901\n",
      "Epoch 471/800, Loss: 13.155577483586967\n",
      "Epoch 472/800, Loss: 14.015942073892802\n",
      "Epoch 473/800, Loss: 12.267348217312247\n",
      "Epoch 474/800, Loss: 11.67483931267634\n",
      "Epoch 475/800, Loss: 12.435187600553036\n",
      "Epoch 476/800, Loss: 15.571471929550171\n",
      "Epoch 477/800, Loss: 12.73105396144092\n",
      "Epoch 478/800, Loss: 13.117172398604453\n",
      "Epoch 479/800, Loss: 13.440316230989993\n",
      "Epoch 480/800, Loss: 13.180507312528789\n",
      "Epoch 481/800, Loss: 12.589527942240238\n",
      "Epoch 482/800, Loss: 11.488322542980313\n",
      "Epoch 483/800, Loss: 11.43435178231448\n",
      "Epoch 484/800, Loss: 12.055938048055395\n",
      "Epoch 485/800, Loss: 13.967829733155668\n",
      "Epoch 486/800, Loss: 12.76881449483335\n",
      "Epoch 487/800, Loss: 13.80394326057285\n",
      "Epoch 488/800, Loss: 12.080320748966187\n",
      "Epoch 489/800, Loss: 11.114869708195329\n",
      "Epoch 490/800, Loss: 12.375960134435445\n",
      "Epoch 491/800, Loss: 12.73806781694293\n",
      "Epoch 492/800, Loss: 13.464626993983984\n",
      "Epoch 493/800, Loss: 12.475317153614014\n",
      "Epoch 494/800, Loss: 11.919833672232926\n",
      "Epoch 495/800, Loss: 12.25720890564844\n",
      "Epoch 496/800, Loss: 12.438378260936588\n",
      "Epoch 497/800, Loss: 11.704482210800052\n",
      "Epoch 498/800, Loss: 11.109089485369623\n",
      "Epoch 499/800, Loss: 10.759991398779675\n",
      "Epoch 500/800, Loss: 11.256575545761734\n",
      "Epoch 501/800, Loss: 15.80552403209731\n",
      "Epoch 502/800, Loss: 18.384985965676606\n",
      "Epoch 503/800, Loss: 13.855802262667567\n",
      "Epoch 504/800, Loss: 12.370824757497758\n",
      "Epoch 505/800, Loss: 11.292576644569635\n",
      "Epoch 506/800, Loss: 11.47338839666918\n",
      "Epoch 507/800, Loss: 11.13713699998334\n",
      "Epoch 508/800, Loss: 11.07787559274584\n",
      "Epoch 509/800, Loss: 11.363151693716645\n",
      "Epoch 510/800, Loss: 13.690222209319472\n",
      "Epoch 511/800, Loss: 13.205653807148337\n",
      "Epoch 512/800, Loss: 12.335067486856133\n",
      "Epoch 513/800, Loss: 12.09632367407903\n",
      "Epoch 514/800, Loss: 11.97836741246283\n",
      "Epoch 515/800, Loss: 10.558154753409326\n",
      "Epoch 516/800, Loss: 11.17501970846206\n",
      "Epoch 517/800, Loss: 11.691749417223036\n",
      "Epoch 518/800, Loss: 10.785343075171113\n",
      "Epoch 519/800, Loss: 12.950025521684438\n",
      "Epoch 520/800, Loss: 12.99876980530098\n",
      "Epoch 521/800, Loss: 13.616745883133262\n",
      "Epoch 522/800, Loss: 14.200509392190725\n",
      "Epoch 523/800, Loss: 12.191858999896795\n",
      "Epoch 524/800, Loss: 11.027035063598305\n",
      "Epoch 525/800, Loss: 12.206453432328999\n",
      "Epoch 526/800, Loss: 12.978871581610292\n",
      "Epoch 527/800, Loss: 11.177508656866848\n",
      "Epoch 528/800, Loss: 11.689241589047015\n",
      "Epoch 529/800, Loss: 10.824255265295506\n",
      "Epoch 530/800, Loss: 10.31921045621857\n",
      "Epoch 531/800, Loss: 11.803392846137285\n",
      "Epoch 532/800, Loss: 11.530483500100672\n",
      "Epoch 533/800, Loss: 12.459874217864126\n",
      "Epoch 534/800, Loss: 11.726046105381101\n",
      "Epoch 535/800, Loss: 12.380598716903478\n",
      "Epoch 536/800, Loss: 11.80816325219348\n",
      "Epoch 537/800, Loss: 11.674030366819352\n",
      "Epoch 538/800, Loss: 10.895164066459984\n",
      "Epoch 539/800, Loss: 11.912015405949205\n",
      "Epoch 540/800, Loss: 11.88243364309892\n",
      "Epoch 541/800, Loss: 11.981382800731808\n",
      "Epoch 542/800, Loss: 11.92610557610169\n",
      "Epoch 543/800, Loss: 13.247805331368\n",
      "Epoch 544/800, Loss: 12.445120609365404\n",
      "Epoch 545/800, Loss: 14.457752182846889\n",
      "Epoch 546/800, Loss: 12.148703274317086\n",
      "Epoch 547/800, Loss: 12.675414237193763\n",
      "Epoch 548/800, Loss: 12.39118222356774\n",
      "Epoch 549/800, Loss: 12.42685362091288\n",
      "Epoch 550/800, Loss: 26.045939399395138\n",
      "Epoch 551/800, Loss: 14.68999994918704\n",
      "Epoch 552/800, Loss: 11.921950378920883\n",
      "Epoch 553/800, Loss: 10.431508484762162\n",
      "Epoch 554/800, Loss: 10.286884061060846\n",
      "Epoch 555/800, Loss: 13.113341484917328\n",
      "Epoch 556/800, Loss: 12.107042673043907\n",
      "Epoch 557/800, Loss: 13.617082354612648\n",
      "Epoch 558/800, Loss: 11.652972634416074\n",
      "Epoch 559/800, Loss: 11.249536590650678\n",
      "Epoch 560/800, Loss: 10.576318896375597\n",
      "Epoch 561/800, Loss: 10.682596219237894\n",
      "Epoch 562/800, Loss: 11.375753925181925\n",
      "Epoch 563/800, Loss: 10.95065589202568\n",
      "Epoch 564/800, Loss: 10.384878985583782\n",
      "Epoch 565/800, Loss: 11.459335245657712\n",
      "Epoch 566/800, Loss: 11.657394868321717\n",
      "Epoch 567/800, Loss: 10.835065428167582\n",
      "Epoch 568/800, Loss: 13.144030065275729\n",
      "Epoch 569/800, Loss: 12.977838437538594\n",
      "Epoch 570/800, Loss: 11.803752373438329\n",
      "Epoch 571/800, Loss: 12.952641473151743\n",
      "Epoch 572/800, Loss: 11.860643832944334\n",
      "Epoch 573/800, Loss: 11.426757698878646\n",
      "Epoch 574/800, Loss: 11.669143539387733\n",
      "Epoch 575/800, Loss: 10.323805565014482\n",
      "Epoch 576/800, Loss: 11.289274039445445\n",
      "Epoch 577/800, Loss: 11.761569562833756\n",
      "Epoch 578/800, Loss: 10.822943847160786\n",
      "Epoch 579/800, Loss: 11.961802672594786\n",
      "Epoch 580/800, Loss: 10.675031203310937\n",
      "Epoch 581/800, Loss: 10.691261888248846\n",
      "Epoch 582/800, Loss: 11.704186568036675\n",
      "Epoch 583/800, Loss: 11.449370612856\n",
      "Epoch 584/800, Loss: 11.822032620199025\n",
      "Epoch 585/800, Loss: 11.381853988626972\n",
      "Epoch 586/800, Loss: 15.321661827154458\n",
      "Epoch 587/800, Loss: 13.018824409227818\n",
      "Epoch 588/800, Loss: 13.646149314008653\n",
      "Epoch 589/800, Loss: 12.53158735530451\n",
      "Epoch 590/800, Loss: 11.262659892905504\n",
      "Epoch 591/800, Loss: 13.339466976933181\n",
      "Epoch 592/800, Loss: 11.281527730170637\n",
      "Epoch 593/800, Loss: 10.07664273492992\n",
      "Epoch 594/800, Loss: 10.78340160381049\n",
      "Epoch 595/800, Loss: 10.372062022797763\n",
      "Epoch 596/800, Loss: 12.066348375286907\n",
      "Epoch 597/800, Loss: 11.2567381542176\n",
      "Epoch 598/800, Loss: 11.641689023002982\n",
      "Epoch 599/800, Loss: 10.699348964262754\n",
      "Epoch 600/800, Loss: 11.504020132822916\n",
      "Epoch 601/800, Loss: 10.727849838789552\n",
      "Epoch 602/800, Loss: 11.230338319204748\n",
      "Epoch 603/800, Loss: 11.408081308240071\n",
      "Epoch 604/800, Loss: 11.135050061857328\n",
      "Epoch 605/800, Loss: 11.485789339756593\n",
      "Epoch 606/800, Loss: 10.567600149195641\n",
      "Epoch 607/800, Loss: 12.12105046166107\n",
      "Epoch 608/800, Loss: 11.77179068303667\n",
      "Epoch 609/800, Loss: 11.328119972255081\n",
      "Epoch 610/800, Loss: 12.44812347041443\n",
      "Epoch 611/800, Loss: 14.112809806596488\n",
      "Epoch 612/800, Loss: 11.842360347975045\n",
      "Epoch 613/800, Loss: 12.537025117781013\n",
      "Epoch 614/800, Loss: 11.811414984054863\n",
      "Epoch 615/800, Loss: 13.251270142849535\n",
      "Epoch 616/800, Loss: 12.564490273129195\n",
      "Epoch 617/800, Loss: 10.415510695427656\n",
      "Epoch 618/800, Loss: 11.571533237583935\n",
      "Epoch 619/800, Loss: 10.846597992349416\n",
      "Epoch 620/800, Loss: 10.784720293711871\n",
      "Epoch 621/800, Loss: 11.700612225569785\n",
      "Epoch 622/800, Loss: 10.826107890810817\n",
      "Epoch 623/800, Loss: 11.01786279072985\n",
      "Epoch 624/800, Loss: 10.54661470465362\n",
      "Epoch 625/800, Loss: 10.812727853190154\n",
      "Epoch 626/800, Loss: 9.924899504287168\n",
      "Epoch 627/800, Loss: 10.79179467028007\n",
      "Epoch 628/800, Loss: 13.046109045855701\n",
      "Epoch 629/800, Loss: 16.595699015073478\n",
      "Epoch 630/800, Loss: 13.899439803790301\n",
      "Epoch 631/800, Loss: 11.634917200077325\n",
      "Epoch 632/800, Loss: 10.922063235193491\n",
      "Epoch 633/800, Loss: 10.356310588307679\n",
      "Epoch 634/800, Loss: 10.776185401715338\n",
      "Epoch 635/800, Loss: 11.850187933538109\n",
      "Epoch 636/800, Loss: 12.157167083118111\n",
      "Epoch 637/800, Loss: 12.246125580277294\n",
      "Epoch 638/800, Loss: 12.833147570025176\n",
      "Epoch 639/800, Loss: 11.153513698838651\n",
      "Epoch 640/800, Loss: 9.853759580524638\n",
      "Epoch 641/800, Loss: 11.722655379446223\n",
      "Epoch 642/800, Loss: 11.477225416805595\n",
      "Epoch 643/800, Loss: 11.836635733488947\n",
      "Epoch 644/800, Loss: 9.786603526910767\n",
      "Epoch 645/800, Loss: 10.069438034202904\n",
      "Epoch 646/800, Loss: 10.863191286101937\n",
      "Epoch 647/800, Loss: 10.414898739662021\n",
      "Epoch 648/800, Loss: 10.699731237487867\n",
      "Epoch 649/800, Loss: 11.873267257586122\n",
      "Epoch 650/800, Loss: 10.939793881960213\n",
      "Epoch 651/800, Loss: 10.836960268672556\n",
      "Epoch 652/800, Loss: 11.006724467035383\n",
      "Epoch 653/800, Loss: 11.97809524461627\n",
      "Epoch 654/800, Loss: 13.180431814398617\n",
      "Epoch 655/800, Loss: 12.656307670287788\n",
      "Epoch 656/800, Loss: 9.495818774215877\n",
      "Epoch 657/800, Loss: 9.784144101198763\n",
      "Epoch 658/800, Loss: 11.159121058415622\n",
      "Epoch 659/800, Loss: 11.724044948350638\n",
      "Epoch 660/800, Loss: 10.873624599073082\n",
      "Epoch 661/800, Loss: 9.817226637154818\n",
      "Epoch 662/800, Loss: 12.892972907982767\n",
      "Epoch 663/800, Loss: 12.404164670500904\n",
      "Epoch 664/800, Loss: 10.885871003614739\n",
      "Epoch 665/800, Loss: 11.125443934462965\n",
      "Epoch 666/800, Loss: 10.017505401745439\n",
      "Epoch 667/800, Loss: 10.804276942741126\n",
      "Epoch 668/800, Loss: 12.511416159104556\n",
      "Epoch 669/800, Loss: 10.712058293167502\n",
      "Epoch 670/800, Loss: 11.547528428025544\n",
      "Epoch 671/800, Loss: 11.285954233258963\n",
      "Epoch 672/800, Loss: 10.068282225402072\n",
      "Epoch 673/800, Loss: 9.477332378039137\n",
      "Epoch 674/800, Loss: 10.088717793580145\n",
      "Epoch 675/800, Loss: 11.282756122294813\n",
      "Epoch 676/800, Loss: 10.787592193577439\n",
      "Epoch 677/800, Loss: 10.038123682374135\n",
      "Epoch 678/800, Loss: 10.038515955209732\n",
      "Epoch 679/800, Loss: 11.623675657436252\n",
      "Epoch 680/800, Loss: 10.416317323688418\n",
      "Epoch 681/800, Loss: 10.597862401977181\n",
      "Epoch 682/800, Loss: 11.818268965929747\n",
      "Epoch 683/800, Loss: 11.361979376059026\n",
      "Epoch 684/800, Loss: 10.729975104797632\n",
      "Epoch 685/800, Loss: 10.06437274068594\n",
      "Epoch 686/800, Loss: 13.000390490051359\n",
      "Epoch 687/800, Loss: 10.460363765945658\n",
      "Epoch 688/800, Loss: 11.574287522118539\n",
      "Epoch 689/800, Loss: 11.798991662915796\n",
      "Epoch 690/800, Loss: 27.985750089865178\n",
      "Epoch 691/800, Loss: 35.681435086764395\n",
      "Epoch 692/800, Loss: 13.604049507528543\n",
      "Epoch 693/800, Loss: 11.414315580390394\n",
      "Epoch 694/800, Loss: 9.593899382743984\n",
      "Epoch 695/800, Loss: 9.050129478098825\n",
      "Epoch 696/800, Loss: 9.353433526586741\n",
      "Epoch 697/800, Loss: 10.997513486305252\n",
      "Epoch 698/800, Loss: 9.780735263833776\n",
      "Epoch 699/800, Loss: 10.376508934888989\n",
      "Epoch 700/800, Loss: 9.899178513791412\n",
      "Epoch 701/800, Loss: 10.291534879244864\n",
      "Epoch 702/800, Loss: 10.49049493856728\n",
      "Epoch 703/800, Loss: 10.234017992625013\n",
      "Epoch 704/800, Loss: 10.537487810477614\n",
      "Epoch 705/800, Loss: 10.708263311069459\n",
      "Epoch 706/800, Loss: 10.873280956875533\n",
      "Epoch 707/800, Loss: 11.38671620702371\n",
      "Epoch 708/800, Loss: 11.115197418257594\n",
      "Epoch 709/800, Loss: 9.644665740663186\n",
      "Epoch 710/800, Loss: 9.2668435072992\n",
      "Epoch 711/800, Loss: 9.996008141897619\n",
      "Epoch 712/800, Loss: 10.263724061194807\n",
      "Epoch 713/800, Loss: 10.322658982593566\n",
      "Epoch 714/800, Loss: 9.905880509177223\n",
      "Epoch 715/800, Loss: 10.185262971092016\n",
      "Epoch 716/800, Loss: 10.4756824770011\n",
      "Epoch 717/800, Loss: 10.973939226008952\n",
      "Epoch 718/800, Loss: 9.605950776021928\n",
      "Epoch 719/800, Loss: 9.856234899023548\n",
      "Epoch 720/800, Loss: 10.236235217191279\n",
      "Epoch 721/800, Loss: 10.320493279956281\n",
      "Epoch 722/800, Loss: 9.745182684622705\n",
      "Epoch 723/800, Loss: 10.73642577463761\n",
      "Epoch 724/800, Loss: 9.961471796734259\n",
      "Epoch 725/800, Loss: 10.250104959588498\n",
      "Epoch 726/800, Loss: 10.193548573181033\n",
      "Epoch 727/800, Loss: 10.177163515705615\n",
      "Epoch 728/800, Loss: 10.535587756894529\n",
      "Epoch 729/800, Loss: 14.923760009929538\n",
      "Epoch 730/800, Loss: 12.560660724528134\n",
      "Epoch 731/800, Loss: 10.273993695620447\n",
      "Epoch 732/800, Loss: 9.42957665771246\n",
      "Epoch 733/800, Loss: 10.094213395612314\n",
      "Epoch 734/800, Loss: 9.40851478278637\n",
      "Epoch 735/800, Loss: 9.410106745781377\n",
      "Epoch 736/800, Loss: 9.369067401159555\n",
      "Epoch 737/800, Loss: 9.420388938859105\n",
      "Epoch 738/800, Loss: 10.869603240862489\n",
      "Epoch 739/800, Loss: 10.566033548675478\n",
      "Epoch 740/800, Loss: 10.187075891532004\n",
      "Epoch 741/800, Loss: 10.734933557920158\n",
      "Epoch 742/800, Loss: 9.934885243419558\n",
      "Epoch 743/800, Loss: 10.936647482216358\n",
      "Epoch 744/800, Loss: 10.165040694642812\n",
      "Epoch 745/800, Loss: 12.867668413091451\n",
      "Epoch 746/800, Loss: 12.091262910980731\n",
      "Epoch 747/800, Loss: 14.778737477492541\n",
      "Epoch 748/800, Loss: 13.24535603239201\n",
      "Epoch 749/800, Loss: 10.565415023826063\n",
      "Epoch 750/800, Loss: 9.155712255043909\n",
      "Epoch 751/800, Loss: 9.475121571915224\n",
      "Epoch 752/800, Loss: 9.775429260684177\n",
      "Epoch 753/800, Loss: 10.118992921430618\n",
      "Epoch 754/800, Loss: 11.34948505088687\n",
      "Epoch 755/800, Loss: 10.751460739411414\n",
      "Epoch 756/800, Loss: 9.998806859599426\n",
      "Epoch 757/800, Loss: 9.6193340504542\n",
      "Epoch 758/800, Loss: 9.471236216370016\n",
      "Epoch 759/800, Loss: 9.808571566827595\n",
      "Epoch 760/800, Loss: 10.836074534803629\n",
      "Epoch 761/800, Loss: 9.693181610666215\n",
      "Epoch 762/800, Loss: 10.298307999037206\n",
      "Epoch 763/800, Loss: 9.577935778070241\n",
      "Epoch 764/800, Loss: 10.855405745096505\n",
      "Epoch 765/800, Loss: 11.7535141967237\n",
      "Epoch 766/800, Loss: 10.385623794980347\n",
      "Epoch 767/800, Loss: 9.386677336180583\n",
      "Epoch 768/800, Loss: 9.421844403492287\n",
      "Epoch 769/800, Loss: 9.481288766022772\n",
      "Epoch 770/800, Loss: 11.289785758592188\n",
      "Epoch 771/800, Loss: 11.454803078901023\n",
      "Epoch 772/800, Loss: 10.21256157476455\n",
      "Epoch 773/800, Loss: 10.167431817390025\n",
      "Epoch 774/800, Loss: 10.598558564903215\n",
      "Epoch 775/800, Loss: 9.200279620010406\n",
      "Epoch 776/800, Loss: 10.189431108534336\n",
      "Epoch 777/800, Loss: 10.686137894401327\n",
      "Epoch 778/800, Loss: 10.704197123181075\n",
      "Epoch 779/800, Loss: 9.462730518076569\n",
      "Epoch 780/800, Loss: 10.637944885063916\n",
      "Epoch 781/800, Loss: 10.658967402298003\n",
      "Epoch 782/800, Loss: 10.967417992185801\n",
      "Epoch 783/800, Loss: 9.471393966116011\n",
      "Epoch 784/800, Loss: 10.93605111236684\n",
      "Epoch 785/800, Loss: 9.99353516777046\n",
      "Epoch 786/800, Loss: 9.42914702463895\n",
      "Epoch 787/800, Loss: 10.01884016674012\n",
      "Epoch 788/800, Loss: 9.872981662396342\n",
      "Epoch 789/800, Loss: 9.7154535879381\n",
      "Epoch 790/800, Loss: 9.665217350237072\n",
      "Epoch 791/800, Loss: 9.15350453183055\n",
      "Epoch 792/800, Loss: 9.515462558483705\n",
      "Epoch 793/800, Loss: 9.827856672462076\n",
      "Epoch 794/800, Loss: 13.336614599917084\n",
      "Epoch 795/800, Loss: 11.738964852876961\n",
      "Epoch 796/800, Loss: 10.877923847408965\n",
      "Epoch 797/800, Loss: 10.216155840549618\n",
      "Epoch 798/800, Loss: 9.741518629249185\n",
      "Epoch 799/800, Loss: 9.396960098063573\n",
      "Epoch 800/800, Loss: 9.214079433353618\n",
      "Test Score (R^2): 0.740723871405454\n"
     ]
    }
   ],
   "source": [
    "#### CREATING MODEL ####\n",
    "\n",
    "# Define the neural network architecture with multiple hidden layers\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        layers = []\n",
    "        previous_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(previous_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            previous_dim = hidden_dim\n",
    "        layers.append(nn.Linear(previous_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.activation = nn.ReLU() #best according to grid search\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_data.shape[1]  # Number of molecular descriptors\n",
    "hidden_dim = [512,512,512]  # (512,512,512) Best according to grid search\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 800\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function (nn.MSELoss) and optimizer\n",
    "model = SolubilityPredictor(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "test_score = r2_score(y_test, y_pred)\n",
    "print(\"Test Score (R^2):\", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1.3280858993530273\n",
      "Test RMSE: 1.1524261236190796\n",
      "Test MAE: 0.6691029071807861\n",
      "Test R^2: 0.740723871405454\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Perform the forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    test_predictions_tensor = model(X_test_tensor)\n",
    "\n",
    "# Convert predictions back to NumPy array\n",
    "test_predictions = test_predictions_tensor.numpy().flatten()\n",
    "\n",
    "# Calculate additional metrics\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With dropout, weight decay\n",
    "try dropout rates between 0.2 and 0.5. start with 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 681.7832328081131\n",
      "Epoch 101/1000, Loss: 111.76737007498741\n",
      "Epoch 201/1000, Loss: 74.74509523808956\n",
      "Epoch 301/1000, Loss: 64.90365134179592\n",
      "Epoch 401/1000, Loss: 56.54144813865423\n",
      "Epoch 501/1000, Loss: 55.897232234478\n",
      "Epoch 601/1000, Loss: 51.54749147221446\n",
      "Epoch 701/1000, Loss: 52.32152093201876\n",
      "Epoch 801/1000, Loss: 46.192381370812654\n",
      "Epoch 901/1000, Loss: 48.13287205994129\n",
      "Test MSE: 1.2193962335586548\n",
      "Test RMSE: 1.1042627096176147\n",
      "Test MAE: 0.695110559463501\n",
      "Test R^2: 0.7752118032844304\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network architecture with multiple hidden layers and dropout\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        layers = []\n",
    "        previous_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(previous_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))  # Adding dropout\n",
    "            previous_dim = hidden_dim\n",
    "        layers.append(nn.Linear(previous_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_data.shape[1]  # Number of molecular descriptors\n",
    "hidden_dims = [512, 512, 512]  # Best according to grid search\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function (nn.MSELoss) and optimizer with weight decay (L2 regularization)\n",
    "model = SolubilityPredictor(input_dim, hidden_dims, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy().flatten()\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'solubility_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to make it classification instead of regression? No, just calculate how correct it is with a cut-off of 0.5 after predictions. \n",
    "can choose to say the values between 0.4 and 0.6 are inconclusive\n",
    "\n",
    "Also, use logK(%F) as it has a more balanced distribution\n",
    "\n",
    "See \"Critical Evaluation of Human Oral Bioavailability for Pharmaceutical Drugs by Using Various Cheminformatics Approaches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../Data/Bioavailibility.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### DATA PREPARATION ####\n",
    "# try with new data\n",
    "\n",
    "#remove nan values from data\n",
    "data = data.dropna(subset=['logK(%F)'])\n",
    "\n",
    "#get x and y data (x is the molecular descriptors, y is the solubility)\n",
    "y_data = data[\"logK(%F)\"]\n",
    "\n",
    "print(len(y_data))\n",
    "\n",
    "#generating features and making sure it's the same features as previous model\n",
    "smiles = data[\"Updated SMILES\"]\n",
    "featurizer = RDKitDescriptors()\n",
    "features = featurizer.featurize(smiles)\n",
    "X_data = features[:,used_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_data.shape[1])\n",
    "if True in np.isnan(X_data):\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0604356961979122"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation using train_test_split while retaining indices\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X_data, y_data, data.index, test_size=0.2, random_state=42)\n",
    "# Convert y pandas Series to NumPy array\n",
    "y_train = y_train.values.astype(np.float32)\n",
    "y_test = y_test.values.astype(np.float32)\n",
    "\n",
    "#scale x values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#convert data to pytorch tensors (like numpy arrays but for pytorch)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Reshape the target tensor to match the shape of the output tensor\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "# Reshape the target tensor to match the shape of the output tensor\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define model again\n",
    "\n",
    "# Define the neural network architecture with multiple hidden layers\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        layers = []\n",
    "        previous_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(previous_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            previous_dim = hidden_dim\n",
    "        layers.append(nn.Linear(previous_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.activation = nn.ReLU() #best according to grid search\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_data.shape[1]  # Number of molecular descriptors\n",
    "hidden_dim = [512,512,512]  # (512,512,512) Best according to grid search\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "#loading saved model\n",
    "model = SolubilityPredictor(input_dim, hidden_dim, output_dim)\n",
    "model.load_state_dict(torch.load('solubility_model.pth'))\n",
    "\n",
    "# Optionally, modify the final layer if the output dimension is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BioavailabilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(BioavailabilityPredictor, self).__init__()\n",
    "        self.base_model = SolubilityPredictor(input_dim, hidden_dims, output_dim)\n",
    "        \n",
    "        # New layers for bioavailability prediction\n",
    "        self.new_layers = nn.Sequential(\n",
    "            nn.Linear(output_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.new_layers(x)\n",
    "        return x\n",
    "\n",
    "# Create the new model\n",
    "bioavailability_model = BioavailabilityPredictor(input_dim, hidden_dim, output_dim)\n",
    "bioavailability_model.base_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initially freeze the pre-trained layers\n",
    "for param in bioavailability_model.base_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 65.78714364767075\n",
      "Epoch 101/1000, Loss: 56.19918841123581\n",
      "Epoch 201/1000, Loss: 55.13509580492973\n",
      "Epoch 301/1000, Loss: 54.43697330355644\n",
      "Epoch 401/1000, Loss: 54.36219623684883\n",
      "Epoch 501/1000, Loss: 54.61751738190651\n",
      "Epoch 601/1000, Loss: 54.72561317682266\n",
      "Epoch 701/1000, Loss: 54.25527021288872\n",
      "Epoch 801/1000, Loss: 54.60600858926773\n",
      "Epoch 901/1000, Loss: 54.59326508641243\n"
     ]
    }
   ],
   "source": [
    "# train only the new layers added for the task:\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(bioavailability_model.new_layers.parameters(), lr=learning_rate)\n",
    "\n",
    "#setting a smaller batch size for fine-tuning:\n",
    "batch_size = 16\n",
    "\n",
    "#create dataloader for batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set the model to training mode\n",
    "bioavailability_model.train()\n",
    "\n",
    "# Training loop for new layers\n",
    "for epoch in range(num_epochs):\n",
    "    bioavailability_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = bioavailability_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1.176822543144226\n",
      "Test RMSE: 1.0848145484924316\n",
      "Test MAE: 0.8767499327659607\n",
      "Test R^2: 0.010767138650479402\n"
     ]
    }
   ],
   "source": [
    "bioavailability_model.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Perform the forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    test_predictions_tensor = bioavailability_model(X_test_tensor)\n",
    "\n",
    "# Convert predictions back to NumPy array\n",
    "test_predictions = test_predictions_tensor.numpy().flatten()\n",
    "\n",
    "# Calculate additional metrics\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000, Loss: 54.297961831092834\n",
      "Epoch 101/3000, Loss: 38.65286995470524\n",
      "Epoch 201/3000, Loss: 22.938462123274803\n",
      "Epoch 301/3000, Loss: 15.01365940272808\n",
      "Epoch 401/3000, Loss: 11.255611576139927\n",
      "Epoch 501/3000, Loss: 9.493599314242601\n",
      "Epoch 601/3000, Loss: 8.770880287513137\n",
      "Epoch 701/3000, Loss: 7.459051143378019\n",
      "Epoch 801/3000, Loss: 6.656189974397421\n",
      "Epoch 901/3000, Loss: 6.291818659752607\n",
      "Epoch 1001/3000, Loss: 6.091873999685049\n",
      "Epoch 1101/3000, Loss: 7.025822479277849\n",
      "Epoch 1201/3000, Loss: 7.1511173117905855\n",
      "Epoch 1301/3000, Loss: 5.270693223923445\n",
      "Epoch 1401/3000, Loss: 6.506126198917627\n",
      "Epoch 1501/3000, Loss: 6.489510610699654\n",
      "Epoch 1601/3000, Loss: 5.9529196713119745\n",
      "Epoch 1701/3000, Loss: 5.554582539014518\n",
      "Epoch 1801/3000, Loss: 5.462247736752033\n",
      "Epoch 1901/3000, Loss: 5.538098099641502\n",
      "Epoch 2001/3000, Loss: 5.102368185296655\n",
      "Epoch 2101/3000, Loss: 4.900372176431119\n",
      "Epoch 2201/3000, Loss: 5.881166679784656\n",
      "Epoch 2301/3000, Loss: 6.557747421786189\n",
      "Epoch 2401/3000, Loss: 5.708027580752969\n",
      "Epoch 2501/3000, Loss: 6.514948351308703\n",
      "Epoch 2601/3000, Loss: 5.498656248673797\n",
      "Epoch 2701/3000, Loss: 5.085499651730061\n",
      "Epoch 2801/3000, Loss: 5.30486211925745\n",
      "Epoch 2901/3000, Loss: 5.965517524629831\n"
     ]
    }
   ],
   "source": [
    "#fine-tune it by unfreezing some of the pre-trained layers\n",
    "\n",
    "# Unfreeze some of the pre-trained layers\n",
    "for param in bioavailability_model.base_model.network[-4].parameters():  # Unfreeze the second last layer\n",
    "    param.requires_grad = True\n",
    "for param in bioavailability_model.base_model.network[-7].parameters():  # Unfreeze the third last layer (optional)\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Update the optimizer to include the parameters of the unfrozen layers\n",
    "optimizer = optim.Adam(bioavailability_model.parameters(), lr=learning_rate / 10)\n",
    "\n",
    "#maybe more epochs?\n",
    "num_epochs = 1000\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(num_epochs):\n",
    "    bioavailability_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = bioavailability_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1.622246503829956\n",
      "Test RMSE: 1.2736743688583374\n",
      "Test MAE: 1.0059106349945068\n",
      "Test R^2: -0.363654738829426\n"
     ]
    }
   ],
   "source": [
    "bioavailability_model.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Perform the forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    test_predictions_tensor = bioavailability_model(X_test_tensor)\n",
    "\n",
    "# Convert predictions back to NumPy array\n",
    "test_predictions = test_predictions_tensor.numpy().flatten()\n",
    "\n",
    "# Calculate additional metrics\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      No           Name    %F  logK(%F)  Category  Test Prediction\n",
      "920  921    Tolmesoxide  85.0  0.753328         1        -1.285903\n",
      "525  526    Mebendazole  22.0 -0.549672         0        -1.817041\n",
      "567  568   Metopimazine  20.0 -0.602060         0         1.234128\n",
      "657  658     Olanzapine  60.0  0.176091         1        -0.746248\n",
      "633  634    Nilvadipine  14.0 -0.788370         0        -1.338885\n",
      "..   ...            ...   ...       ...       ...              ...\n",
      "486  487  Lercanidipine  10.0 -0.954243         0        -0.444153\n",
      "451  452     Indapamide  90.0  0.954243         1         0.702827\n",
      "65    66    Anastrozole  80.0  0.602060         1        -0.054903\n",
      "141  142    Carbimazole   0.0 -2.000000         0        -1.071693\n",
      "685  686    Pancuronium   0.0 -2.000000         0        -1.720845\n",
      "\n",
      "[199 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame to store the results\n",
    "results_df = data.loc[test_indices, [\"No\", \"Name\", \"%F\", \"logK(%F)\", \"Category\"]].copy()\n",
    "\n",
    "# Add the test predictions to the DataFrame\n",
    "results_df[\"Test Prediction\"] = test_predictions\n",
    "\n",
    "# Print or save the results DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5678391959798995\n"
     ]
    }
   ],
   "source": [
    "results_df = results_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# count correct predictions\n",
    "count_total = len(results_df[\"No\"])\n",
    "\n",
    "count_correct = 0\n",
    "for i in range(count_total):\n",
    "    if results_df[\"Test Prediction\"][i] > 0.5 and results_df[\"Category\"][i] == 1:\n",
    "        count_correct += 1\n",
    "    elif results_df[\"Test Prediction\"][i] < 0.5 and results_df[\"Category\"][i] == 0:\n",
    "        count_correct += 1\n",
    "\n",
    "print(count_correct/count_total)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) using sensitivity (percentage of high oral bioavailable drugs predict-\n",
    "ed correctly), specificity (percentage of low oral bioavailable\n",
    "drugs predicted correctly), and CCR (correct classification\n",
    "rate or balanced accuracy) for CTG models; and \n",
    "2) Pearson’s multiple linear correlation coefficient (R2) and mean absolute\n",
    "error (MAE) for CNT models\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "58\n",
      "80\n",
      "28\n",
      "0.3626373626373626\n",
      "0.7407407407407407\n",
      "55.16890516890517\n"
     ]
    }
   ],
   "source": [
    "# calculate metrics\n",
    "TP = ((results_df[\"Test Prediction\"] > 0.5) & (results_df[\"Category\"] == 1)).sum()\n",
    "FN = ((results_df[\"Test Prediction\"] < 0.5) & (results_df[\"Category\"] == 1)).sum()\n",
    "TN = ((results_df[\"Test Prediction\"] < 0.5) & (results_df[\"Category\"] == 0)).sum()\n",
    "FP = ((results_df[\"Test Prediction\"] > 0.5) & (results_df[\"Category\"] == 0)).sum()\n",
    "\n",
    "print(TP)\n",
    "print(FN)\n",
    "print(TN)\n",
    "print(FP)\n",
    "sensitivity = TP/(TP+FN)\n",
    "print(sensitivity)\n",
    "\n",
    "specificity = TN/(TN+FP)\n",
    "print(specificity)\n",
    "\n",
    "CCR = ((sensitivity+specificity)/2)*100\n",
    "print(CCR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
