! pip install chemprop
import random
import numpy as np
import pandas as pd
! pip install pytorch-lightning wandb rdkit ogb deepchem
import torch
VERSION = torch.__version__
! pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-{VERSION}.html
! pip install torch-geometric
! mkdir data/
from rdkit.Chem import MolFromSmiles
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Draw
IPythonConsole.ipython_useSVG = True  # < use SVGs instead of PNGs
IPythonConsole.drawOptions.addAtomIndices = True  # adding indices for atoms
IPythonConsole.drawOptions.addBondIndices = False  # not adding indices for bonds
IPythonConsole.molSize = 200, 200
# thinking about importing data
#! wget https://raw.githubusercontent.com/NinaB99/AI-for-Chemistry/main/Data/CuratedSol.csv
# Random Seeds and Reproducibility
torch.manual_seed(0)
torch.cuda.manual_seed(0)
np.random.seed(0)
random.seed(0)
! pip install deepchem.data
import torch.nn.functional as F
from torch.nn import GRU
import pytorch_lightning as pl
from pytorch_lightning.loggers import WandbLogger
from torch_geometric.loader import DataLoader
from torch_geometric.nn import NNConv, MLP, global_add_pool
from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder
from tqdm import tqdm
import pandas as pd
import torch
from torch_geometric.data import (
    Data,
    InMemoryDataset,
    download_url,
)
from ogb.utils import smiles2graph
from deepchem.splits import RandomSplitter
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_add_pool
from deepchem.feat import RDKitDescriptors
from sklearn.feature_selection import VarianceThreshold
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV
import wandb
df = pd.read_csv('https://raw.githubusercontent.com/NinaB99/AI-for-Chemistry/main/Data/Bioavailibility.csv')
smiles = df['Updated SMILES'].values.tolist()
# Here, we use molecular descriptors from RDKit, like molecular weight, number of valence electrons, maximum and minimum partial charge, etc.
featurizer = RDKitDescriptors()
features = featurizer.featurize(smiles)
print(f"Number of generated molecular descriptors: {features.shape[1]}")

# Drop the features containing invalid values
features = features[:, ~np.isnan(features).any(axis=0)]
print(f"Number of molecular descriptors without invalid values: {features.shape[1]}")
y = df['logK(%F)'].values
# Here, we removed all zero-variance features, i.e. features that have the same value in all samples.

selector = VarianceThreshold(threshold=0.0)
features = selector.fit_transform(features)
print(f"Number of molecular descriptors after removing zero-variance features: {features.shape[1]}")
#df_features = pd.DataFrame(features)

splitter = RandomSplitter()
molecular_features = torch.tensor(features, dtype=torch.float32)
# train_idx, valid_idx, test_idx = splitter.split(molecular_features, frac_train=0.7, frac_valid=0.1, frac_test=0.2)
# train_dataset_features = molecular_features[train_idx]
# valid_dataset_features = molecular_features[valid_idx]
# test_dataset_features = molecular_features[test_idx]

#print(f"Train dataset shape: {train_dataset_features.shape}")
#print(f"Valid dataset shape: {valid_dataset_features.shape}")
#print(f"Test dataset shape: {test_dataset_features.shape}")

# Normalize target to mean = 0 and std = 1.
mean = y.mean()
std = y.std()
y = (y - mean) / std
mean, std = mean.item(), std.item()
class Graph_NN(pl.LightningModule):
    def __init__(self, hidden_dim, out_dim,
                 train_data, valid_data, test_data,
                 std, batch_size=32, lr=1e-3):
        super().__init__()
        self.std = std  # std of data's target
        self.train_data = train_data
        self.valid_data = valid_data
        self.test_data = test_data
        self.batch_size = batch_size
        self.lr = lr
        # Initial layers
        self.atom_emb = AtomEncoder(emb_dim=hidden_dim)
        self.bond_emb = BondEncoder(emb_dim=hidden_dim)
        # Message passing layers
        nn = MLP([hidden_dim, hidden_dim*2, hidden_dim*hidden_dim])
        self.conv = NNConv(hidden_dim, hidden_dim, nn, aggr='mean')
        self.gru = GRU(hidden_dim, hidden_dim)
        # Readout layers
        self.mlp = MLP([hidden_dim, int(hidden_dim/2), out_dim])

    def forward(self, data, mode="train"):

        # Initialization
        x = self.atom_emb(data.x)
        h = x.unsqueeze(0)
        edge_attr = self.bond_emb(data.edge_attr)

        # Message passing
        for i in range(3):
            m = F.relu(self.conv(x, data.edge_index, edge_attr))  # send message and aggregation
            x, h = self.gru(m.unsqueeze(0), h)  # node update
            x = x.squeeze(0)

        # Readout
        x = global_add_pool(x, data.batch)
        x = self.mlp(x)

        return x.view(-1)

    def training_step(self, batch, batch_idx):
        # Here we define the train loop.
        out = self.forward(batch, mode="train")
        loss = F.mse_loss(out, batch.y)
        self.log("Train loss", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        # Define validation step. At the end of every epoch, this will be executed
        out = self.forward(batch, mode="valid")
        loss = F.mse_loss(out * self.std, batch.y * self.std)  # report MSE
        self.log("Valid MSE", loss)

    def test_step(self, batch, batch_idx):
        # What to do in test
        out = self.forward(batch, mode="test")
        loss = F.mse_loss(out * self.std, batch.y * self.std)  # report MSE
        self.log("Test MSE", loss)

    def configure_optimizers(self):
        # Here we configure the optimization algorithm.
        optimizer = torch.optim.Adam(
            self.parameters(),
            lr=self.lr
        )
        return optimizer

    def train_dataloader(self):
        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)

    def val_dataloader(self):
        return DataLoader(self.valid_data, batch_size=self.batch_size, shuffle=False)

    def test_dataloader(self):
        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)
class csvGraphData(InMemoryDataset):
    """The solubility graph dataset using PyG
    """
    # bioavailability dataset download link
    raw_url = 'https://raw.githubusercontent.com/NinaB99/AI-for-Chemistry/main/Data/Bioavailibility.csv'

    def __init__(self, root, transform=None):
        super().__init__(root, transform)
        self.data, self.slices = torch.load(self.processed_paths[0])

    @property
    def raw_file_names(self):
        return ['Bioavailibility.csv']

    @property
    def processed_file_names(self):
        return ['data.pt']

    def download(self):
        print('Downloading dataset...')
        file_path = download_url(self.raw_url, self.raw_dir)

    def process(self):
        # load raw data from a csv file
        print(self.raw_paths)
        df = pd.read_csv(self.raw_paths[0],sep=',')
        smiles = df['Updated SMILES'].values.tolist()
#        mol_weight = df['Mol_Weight'].values.tolist()
 #       LogP_predicted = df['Predicted_LogP'].values.tolist()
  #      H_bond_donors = df['number_H_bond_donors'].values.tolist()
   #     H_bond_acceptors = df['number_H_bond_acceptors'].values.tolist()
        target = df['logK(%F)'].values.tolist()

        # Convert SMILES into graph data
        print('Converting SMILES strings into graphs...')
        data_list = []
        for i, smi in enumerate(tqdm(smiles)):

            # get graph data from SMILES
            graph = smiles2graph(smi)

            # convert to tensor and pyg data
            x = torch.tensor(graph['node_feat'], dtype=torch.long)
            edge_index = torch.tensor(graph['edge_index'], dtype=torch.long)
            edge_attr = torch.tensor(graph['edge_feat'], dtype=torch.long)
            y = torch.tensor([target[i]], dtype=torch.float)
            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y) #mol_weight=Mol_Weight, LogP=Predicted_LogP, H_bond_donors=number_H_bond_donors, H_bond_acceptors=number_H_bond_acceptors)
            data_list.append(data)

        # save data
        torch.save(self.collate(data_list), self.processed_paths[0])
# create dataset
dataset = csvGraphData('./data_pyg').shuffle()

# Normalize target to mean = 0 and std = 1.
mean = dataset.data.y.mean()
std = dataset.data.y.std()
dataset.data.y = (dataset.data.y - mean) / std
mean, std = mean.item(), std.item()

# split data
# splitter = RandomSplitter()
train_idx, valid_idx, test_idx = splitter.split(dataset, frac_train=0.7, frac_valid=0.1, frac_test=0.2)
train_dataset_graph = dataset[train_idx]
valid_dataset_graph = dataset[valid_idx]
test_dataset_graph = dataset[test_idx]
#print(f"Train dataset shape: {train_dataset_graph.shape}")
#print(f"Valid dataset shape: {valid_dataset_graph.shape}")
#print(f"Test dataset shape: {test_dataset_graph.shape}")
wandb.init(project="gnn-bioavailibility",
           config={
               "batch_size": 48,
               "learning_rate": 0.01,
               "hidden_size": 64,
               "max_epochs": 50
           })

graph_branch = Graph_NN(
     hidden_dim = wandb.config["hidden_size"],
     out_dim = 1,
     std = std,
     train_data = train_dataset_graph,
     valid_data = valid_dataset_graph,
     test_data = test_dataset_graph,
     lr=wandb.config["learning_rate"],
     batch_size=wandb.config["batch_size"]
)
wandb_logger = WandbLogger()
trainer = pl.Trainer(
    max_epochs = wandb.config["max_epochs"],
    logger = wandb_logger
)

# Finally! Training a model :)
trainer.fit(
    model=graph_branch,
)

# Now run test
graph_results = trainer.test(ckpt_path="best")
wandb.finish()
# Test RMSE
graph_test_mse = graph_results[0]["Test MSE"]
graph_test_rmse = graph_test_mse ** 0.5
print(f"\Graph neural network model performance: RMSE on test set = {graph_test_rmse:.4f}.\n")
class FeaturesNeuralNetwork(pl.LightningModule):
    def __init__(self, input_sz, hidden_sz, train_data, valid_data, test_data, batch_size=254, lr=1e-3):
        super().__init__()
        self.lr = lr
        self.train_data = train_data
        self.valid_data = valid_data
        self.test_data = test_data
        self.batch_size = batch_size
        
        # Define all the components
        self.model = nn.Sequential(
            nn.Linear(input_sz, hidden_sz),
            nn.ReLU(),
            nn.Linear(hidden_sz, hidden_sz),
            nn.ReLU(),
            nn.Linear(hidden_sz, 1)
        )
        
    def training_step(self, batch, batch_idx):
        # Here we define the train loop.
        x, y = batch
        z = self.model(x)
        loss = F.mse_loss(z, y)
        self.log("Train loss", loss)
        # print("end training step")
        return loss
    
    def validation_step(self, batch, batch_idx):
        # Define validation step. At the end of every epoch, this will be executed
        # print(batch)
        x, y = batch
        z = self.model(x)
        loss = F.mse_loss(z, y)  # report MSE
        self.log("Valid MSE", loss)
        
    def test_step(self, batch, batch_idx):
        # What to do in test
        x, y = batch
        z = self.model(x)
        loss = F.mse_loss(z, y)  # report MSE
        self.log("Test MSE", loss)

    def configure_optimizers(self):
        # Here we configure the optimization algorithm.
        optimizer = torch.optim.Adam(
            self.parameters(),
            lr=self.lr
        )
        return optimizer
    
    def forward(self, x):
        # Here we define what the NN does with its parts
        return self.model(x).flatten()
    
    def train_dataloader(self):
        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)
    
    def val_dataloader(self):
        return DataLoader(self.valid_data, batch_size=self.batch_size, shuffle=False)
    
    def test_dataloader(self):
        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)
from torch.utils.data import Dataset
#Creating features dataset
class Features_dataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y

    def __len__(self):
        return self.X.shape[0]

    def __getitem__(self, idx):
        
        if torch.is_tensor(idx):
            idx = idx.tolist()
        # X_ = torch.as_tensor(self.X[idx].astype(np.float32))
        X_ = self.X[idx]
        # y_ = torch.as_tensor(self.y[idx].astype(np.float32).reshape(-1))
        y_ = self.y[idx]

        return X_, y_
Formatted_dataset = Features_dataset(features,y)
#train_data = ESOLDataset(X_train, y_train)
#valid_data = ESOLDataset(X_valid, y_valid)
#test_data = ESOLDataset(X_test, y_test)
#molecular_features_tensor = torch.tensor(Formatted_dataset, dtype=torch.float32)
train_idx_NN, valid_idx_NN, test_idx_NN = splitter.split(Formatted_dataset, frac_train=0.7, frac_valid=0.1, frac_test=0.2)
train_dataset_features_NN_x = molecular_features[train_idx_NN]
train_y_NN = y[train_idx_NN]
train_dataset_features_NN = Features_dataset(train_dataset_features_NN_x,train_y_NN)

valid_dataset_features_NN_x = molecular_features[valid_idx_NN]
valid_y_NN = y[valid_idx_NN]
valid_dataset_features_NN = Features_dataset(valid_dataset_features_NN_x,valid_y_NN)

test_dataset_features_NN_x = molecular_features[test_idx_NN]
test_y_NN = y[test_idx_NN]
test_dataset_features_NN = Features_dataset(test_dataset_features_NN_x,test_y_NN)
# This will ask you to login to your wandb account

wandb.init(project="nn-bioavailibility",
           config={
               "batch_size": 32,
               "learning_rate": 0.001,
               "hidden_size": 512,
               "max_epochs": 100
           })
# Here we create an instance of our neural network.
# Play around with the hyperparameters!
features_nn_model = FeaturesNeuralNetwork(
    input_sz = train_dataset_features_NN.X.shape[1],
    hidden_sz = wandb.config["hidden_size"],
    train_data = train_dataset_features_NN,
    valid_data = valid_dataset_features_NN,
    test_data = test_dataset_features_NN,
    lr = wandb.config["learning_rate"],
    batch_size=wandb.config["batch_size"]
)

# Define trainer: How we want to train the model
wandb_logger = WandbLogger()
trainer = pl.Trainer(
    max_epochs = wandb.config["max_epochs"],
    logger = wandb_logger
)

# Finally! Training a model :)
trainer.fit(
    model=features_nn_model,
)

# Now run test
results = trainer.test(ckpt_path="best")
wandb.finish()
X = features
# training data size : test data size = 0.8 : 0.2
# fixed seed using the random_state parameter, so it always has the same split.
X_train, X_test, y_train, y_test = train_test_split(
    X, y, train_size=0.8, random_state=0)

scaler = MinMaxScaler()
scaler.fit(X_train)

# save original X
X_train_ori = X_train
X_test_ori = X_test
# transform data
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

# random forest regressor, and the default criterion is mean squared error (MSE)

ranf_reg = RandomForestRegressor(n_estimators=10, random_state=0)  # using 10 trees and seed=0

# XGBoost regressor

xgb_reg = XGBRegressor(n_estimators=10, random_state=0)  # using 10 trees and seed=0
def train_test_model(model, X_train, y_train, X_test, y_test):
    """
    Function that trains a model, and tests it.
    Inputs: sklearn model, train_data, test_data
    """
    # Train model
    model.fit(X_train, y_train)
    
    # Calculate RMSE on training
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    model_train_mse = mean_squared_error(y_train, y_pred_train)
    model_test_mse = mean_squared_error(y_test, y_pred_test)
    model_train_rmse = model_train_mse ** 0.5
    model_test_rmse = model_test_mse ** 0.5
    print(f"RMSE on train set: {model_train_rmse:.3f}, and test set: {model_test_rmse:.3f}.\n")


# Train and test the random forest model
print("Evaluating Random Forest Model.")
train_test_model(ranf_reg, X_train, y_train, X_test, y_test)

# Train and test XGBoost model
print("Evaluating XGBoost model.")
train_test_model(xgb_reg, X_train, y_train, X_test, y_test)
param_grid = {
    'n_estimators': [50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],
    'max_depth': [5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],
}

# use 5-folds cross validation during grid searching
grid_search_RF = GridSearchCV(
    RandomForestRegressor(random_state=0),
    param_grid,
    cv=5
)
grid_search_RF.fit(X_train, y_train)

# re-train a model using best hyperparameters
rf_gs = RandomForestRegressor(**grid_search_RF.best_params_, random_state=0)

print('Best paramters(Random Forest): ', grid_search_RF.best_params_)
print('Random forests performance after hyperparamter optimization:')
train_test_model(rf_gs, X_train, y_train, X_test, y_test)

# use 5-folds cross validation during grid searching
grid_search_XBG= GridSearchCV(
    XGBRegressor(random_state=0),
    param_grid,
    cv=5
)
grid_search_XBG.fit(X_train, y_train)

# re-train a model using best hyperparameters
xgb_gs = XGBRegressor(**grid_search_XBG.best_params_, random_state=0)

print('Best paramters(XGBoost): ', grid_search_XBG.best_params_)
print('XGBoost performance after hyperparamter optimization:')
train_test_model(xgb_gs, X_train, y_train, X_test, y_test)
class FinalNeuralNetwork(pl.LightningModule):
    def __init__(self, input_sz, hidden_sz, train_data, valid_data, test_data, batch_size=254, lr=1e-3):
        super().__init__()
        self.lr = lr
        self.train_data = train_data
        self.valid_data = valid_data
        self.test_data = test_data
        self.batch_size = batch_size
        
        # Define all the components
        self.model = nn.Sequential(
            nn.Linear(input_sz, hidden_sz),
            nn.ReLU(),
            nn.Linear(hidden_sz, hidden_sz),
            nn.ReLU(),
            nn.Linear(hidden_sz, 1)
        )
        
    def training_step(self, batch, batch_idx):
        # Here we define the train loop.
        x, y = batch
        z = self.model(x)
        loss = F.mse_loss(z, y)
        self.log("Train loss", loss)
        # print("end training step")
        return loss
    
    def validation_step(self, batch, batch_idx):
        # Define validation step. At the end of every epoch, this will be executed
        # print(batch)
        x, y = batch
        z = self.model(x)
        loss = F.mse_loss(z, y)  # report MSE
        self.log("Valid MSE", loss)
        
    def test_step(self, batch, batch_idx):
        # What to do in test
        x, y = batch
        z = self.model(x)
        loss = F.mse_loss(z, y)  # report MSE
        self.log("Test MSE", loss)

    def configure_optimizers(self):
        # Here we configure the optimization algorithm.
        optimizer = torch.optim.Adam(
            self.parameters(),
            lr=self.lr
        )
        return optimizer
    
    def forward(self, x):
        # Here we define what the NN does with its parts
        return self.model(x).flatten()
    
    def train_dataloader(self):
        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)
    
    def val_dataloader(self):
        return DataLoader(self.valid_data, batch_size=self.batch_size, shuffle=False)
    
    def test_dataloader(self):
        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)
from torch.utils.data import Dataset
#Creating features dataset
class Final_dataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y

    def __len__(self):
        return self.X.shape[0]

    def __getitem__(self, idx):
        
        if torch.is_tensor(idx):
            idx = idx.tolist()
        # X_ = torch.as_tensor(self.X[idx].astype(np.float32))
        X_ = self.X[idx]
        # y_ = torch.as_tensor(self.y[idx].astype(np.float32).reshape(-1))
        y_ = self.y[idx]

        return X_, y_
#collating outputs from the other models
features_predictions = FeaturesNeuralNetwork.predict(smiles, verbose=0)
graph_predictions = Graph_NN.predict(molecular_features, verbose=0)
RF_predictions = rf_gs.predict(molecular_features)
XGB_predictions = xgb_gs.predict(molecular_features)
outputs = [features_predictions, graph_predictions, RF_predictions, XGB_predictions]
outputs_tensor = torch.tensor(outputs, dtype=torch.float32)
final_dataset = Final_dataset(outputs,y)

#train_data = ESOLDataset(X_train, y_train)
#valid_data = ESOLDataset(X_valid, y_valid)
#test_data = ESOLDataset(X_test, y_test)
#molecular_features_tensor = torch.tensor(final_dataset, dtype=torch.float32)
final_train_idx_NN, final_valid_idx_NN, final_test_idx_NN = splitter.split(final_dataset, frac_train=0.7, frac_valid=0.1, frac_test=0.2)
train_dataset_final_NN_x = outputs_tensor[final_train_idx_NN]
train_y_NN_final = y[final_train_idx_NN]
train_dataset_final_NN = Final_dataset(train_dataset_final_NN_x,train_y_NN_final)

valid_dataset_final_NN_x = outputs_tensor[final_]
valid_y_NN_final = y[final_]
valid_dataset_final_NN = Final_dataset(valid_dataset_final_NN_x,valid_y_NN_final)

test_dataset_final_NN_x = outputs_tensor[final_test_idx_NN]
test_y_NN_final = y[final_test_idx_NN]
test_dataset_final_NN = Final_dataset(test_dataset_final_NN_x,test_y_NN_final)
# This will ask you to login to your wandb account

wandb.init(project="nn-bioavailibility",
           config={
               "batch_size": 32,
               "learning_rate": 0.001,
               "hidden_size": 512,
               "max_epochs": 100
           })
# Here we create an instance of our neural network.
# Play around with the hyperparameters!
final_nn_model = FinalNeuralNetwork(
    input_sz = train_dataset_final_NN.X.shape[1],
    hidden_sz = wandb.config["hidden_size"],
    train_data = train_dataset_final_NN,
    valid_data = valid_dataset_final_NN,
    test_data = test_dataset_final_NN,
    lr = wandb.config["learning_rate"],
    batch_size=wandb.config["batch_size"]
)

# Define trainer: How we want to train the model
wandb_logger = WandbLogger()
trainer = pl.Trainer(
    max_epochs = wandb.config["max_epochs"],
    logger = wandb_logger
)

# Finally! Training a model :)
trainer.fit(
    model=final_nn_model,
)

# Now run test
results = trainer.test(ckpt_path="best")
wandb.finish()