{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import statements\n",
    "! pip install chemprop\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "! pip install pytorch-lightning wandb rdkit ogb deepchem\n",
    "import torch\n",
    "VERSION = torch.__version__\n",
    "! pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-{VERSION}.html\n",
    "! pip install torch-geometric\n",
    "! mkdir data/\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "IPythonConsole.ipython_useSVG = True  # < use SVGs instead of PNGs\n",
    "IPythonConsole.drawOptions.addAtomIndices = True  # adding indices for atoms\n",
    "IPythonConsole.drawOptions.addBondIndices = False  # not adding indices for bonds\n",
    "IPythonConsole.molSize = 200, 200\n",
    "# Random Seeds and Reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "! pip install deepchem.data\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import GRU\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import NNConv, MLP, global_add_pool\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    download_url,\n",
    ")\n",
    "from ogb.utils import smiles2graph\n",
    "from deepchem.splits import RandomSplitter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_add_pool\n",
    "from deepchem.feat import RDKitDescriptors\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import wandb\n",
    "! pip install mordred\n",
    "from mordred import Calculator, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### importing data and normalising target\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/NinaB99/AI-for-Chemistry/main/Data/Bioavailibility.csv')\n",
    "smiles = df['SMILES'].values.tolist()\n",
    "category = df['Category'].values.tolist()\n",
    "y = df['logK(%F)'].values\n",
    "# Here, we removed all zero-variance features, i.e. features that have the same value in all samples.\n",
    "\n",
    "splitter = RandomSplitter()\n",
    "\n",
    "# Normalize target to mean = 0 and std = 1.\n",
    "mean = y.mean()\n",
    "std = y.std()\n",
    "y = (y - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using the Mordred calculator and preprocessing the data\n",
    "calc = Calculator(descriptors, ignore_3D=True)\n",
    "\n",
    "#extracting the SMILES from the imported data and calculating\n",
    "mols = [MolFromSmiles(smi) for smi in smiles]\n",
    "Mordred_features_df =  calc.pandas(mols)\n",
    "Mordred_features = Mordred_features_df.to_numpy()\n",
    "# print(type(Mordred_features)) # use for debugging\n",
    "Mordred_features = Mordred_features.astype(float)\n",
    "\n",
    "# removing invalid values\n",
    "Mordred_features = Mordred_features[:, ~pd.isnull(Mordred_features).any(axis=0)]\n",
    "print(f\"Number of molecular descriptors without invalid values: {Mordred_features.shape[1]}\")\n",
    "\n",
    "#Removing zero variance features\n",
    "selector = VarianceThreshold(threshold=0.0)\n",
    "Mordred_features = selector.fit_transform(Mordred_features)\n",
    "print(f\"Number of molecular descriptors after removing zero-variance features: {Mordred_features.shape[1]}\")\n",
    "print(Mordred_features[0,:])\n",
    "\n",
    "# convert features to tensor for processing\n",
    "Mordred_molecular_features = torch.tensor(Mordred_features, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating features NN\n",
    "class FeaturesNeuralNetwork(pl.LightningModule):\n",
    "    '''Neural network that has three hidden layers and uses Sigmoid activation function.\n",
    "    Inputs are presplit data and the hyperparameters\n",
    "    '''\n",
    "    def __init__(self, input_sz, hidden_sz, output_sz,\n",
    "                 train_data, valid_data, test_data,\n",
    "                 batch_size=254, dropout = 0.2, lr=1e-3, dropout_prob = 0.5):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.input_sz = input_sz\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.out_sz = output_sz\n",
    "        self.train_data = train_data\n",
    "        self.valid_data = valid_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout_prob\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        # three hidden layers with Sigmoid activation (best choice) and dropout\n",
    "        self.model = nn.Sequential(nn.Linear(input_sz, hidden_sz),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_sz, hidden_sz//2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_sz//2, 1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Here we define the train loop.\n",
    "        x, y = batch\n",
    "        z = self.model(x)\n",
    "        loss = F.mse_loss(z, y)\n",
    "        self.log(\"Train loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Define validation step. At the end of every epoch, this will be executed\n",
    "        x, y = batch\n",
    "        z = self.model(x)\n",
    "        loss = F.mse_loss(z, y)  # report MSE\n",
    "        self.log(\"Valid MSE\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # What to do in test\n",
    "        x, y = batch\n",
    "        z = self.model(x)\n",
    "        loss = F.mse_loss(z, y)  # report MSE\n",
    "        self.log(\"Test MSE\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Here we configure the optimization algorithm.\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating features dataset for features NN\n",
    "class Features_dataset(Dataset):\n",
    "    '''Processes data for input into the NN\n",
    "    Inputs: X = training data. y = known bioavailability targets\n",
    "    '''\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        # X_ = torch.as_tensor(self.X[idx].astype(np.float32))\n",
    "        X_ = self.X[idx]\n",
    "        # y_ = torch.as_tensor(self.y[idx].astype(np.float32).reshape(-1))\n",
    "        y_ = self.y[idx]\n",
    "\n",
    "        return X_, y_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting of data and preprocessing. Presprocessing after splitting to prevent information leakage\n",
    "Formatted_dataset = Features_dataset(Mordred_features,y)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# splitting data\n",
    "train_idx_NN, valid_idx_NN, test_idx_NN = splitter.split(Formatted_dataset, frac_train=0.7, frac_valid=0.1, frac_test=0.2)\n",
    "\n",
    "# preprocessing training data\n",
    "train_dataset_features_NN_x = Mordred_molecular_features[train_idx_NN]\n",
    "scaler.fit(train_dataset_features_NN_x)\n",
    "train_dataset_features_NN_x = scaler.transform(train_dataset_features_NN_x)\n",
    "train_dataset_features_NN_x = torch.tensor(train_dataset_features_NN_x, dtype=torch.float32)\n",
    "train_y_NN = y[train_idx_NN]\n",
    "train_dataset_features_NN = Features_dataset(train_dataset_features_NN_x,train_y_NN)\n",
    "\n",
    "# preprocessing validation data using scaler from test data\n",
    "valid_dataset_features_NN_x = Mordred_molecular_features[valid_idx_NN]\n",
    "train_dataset_features_NN_x = scaler.transform(train_dataset_features_NN_x)\n",
    "train_dataset_features_NN_x = torch.tensor(train_dataset_features_NN_x, dtype=torch.float32)\n",
    "valid_y_NN = y[valid_idx_NN]\n",
    "valid_dataset_features_NN = Features_dataset(valid_dataset_features_NN_x,valid_y_NN)\n",
    "\n",
    "# preprocessing test data using scaler from test data\n",
    "test_dataset_features_NN_x = Mordred_molecular_features[test_idx_NN]\n",
    "test_dataset_features_NN_x = scaler.transform(test_dataset_features_NN_x)\n",
    "test_dataset_features_NN_x = torch.tensor(test_dataset_features_NN_x, dtype=torch.float32)\n",
    "test_y_NN = y[test_idx_NN]\n",
    "test_dataset_features_NN = Features_dataset(test_dataset_features_NN_x,test_y_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating an instance of the features NN\n",
    "# This will ask you to login to your wandb account\n",
    "\n",
    "wandb.init(project=\"nn-bioavailibility\",\n",
    "           config={\n",
    "               \"batch_size\": 48,\n",
    "               \"learning_rate\": 0.001,\n",
    "               \"hidden_size\": 500,\n",
    "               \"max_epochs\": 500\n",
    "           })\n",
    "\n",
    "# Here we create an instance of our neural network.\n",
    "# Opimised hyperparameters\n",
    "features_nn_model = FeaturesNeuralNetwork(\n",
    "    input_sz = train_dataset_features_NN.X.shape[1],\n",
    "    hidden_sz = wandb.config[\"hidden_size\"],\n",
    "    output_sz = 1,\n",
    "    train_data = train_dataset_features_NN,\n",
    "    valid_data = valid_dataset_features_NN,\n",
    "    test_data = test_dataset_features_NN,\n",
    "    lr = wandb.config[\"learning_rate\"],\n",
    "    batch_size=wandb.config[\"batch_size\"],\n",
    "    dropout_prob = 0.8\n",
    ")\n",
    "\n",
    "# Define trainer: How we want to train the model\n",
    "wandb_logger = WandbLogger()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs = wandb.config[\"max_epochs\"],\n",
    "    logger = wandb_logger\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer.fit(\n",
    "    model=features_nn_model,\n",
    ")\n",
    "\n",
    "# running test\n",
    "results = trainer.test(ckpt_path=\"best\")\n",
    "wandb.finish()\n",
    "\n",
    "# saving the neural network\n",
    "torch.save(features_nn_model.state_dict(), 'Mordred_featuriser_nn_model2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluating succes of the model\n",
    "# loading the instance of the NN, not necessary if just run\n",
    "features_nn_model.load_state_dict(torch.load('Mordred_featuriser_nn_model.pth'))\n",
    "# set the models to evaluate mode\n",
    "features_nn_model.eval()\n",
    "\n",
    "# Perform the forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    test_predictions_tensor_features = features_nn_model(Mordred_molecular_features)\n",
    "\n",
    "# Convert predictions back to NumPy array\n",
    "features_predictions = test_predictions_tensor_features.numpy().flatten()\n",
    "\n",
    "# Create outputs DataFrame containing predictions from GNN and the categorised real values\n",
    "outputs_df = pd.DataFrame([features_predictions,\n",
    "                          category])\n",
    "\n",
    "outputs_df = outputs_df.transpose()\n",
    "outputs_df = outputs_df.reset_index(drop=True)\n",
    "count_total = len(outputs_df[0])\n",
    "\n",
    "# classifying the predictions as high or low and comparing to literature values\n",
    "features_count_correct = 0\n",
    "for i in range(count_total):\n",
    "    if outputs_df[0][i] > 0.5 and outputs_df[1][i] == 1:\n",
    "        features_count_correct += 1\n",
    "    elif outputs_df[0][i] < 0.5 and outputs_df[1][i] == 0:\n",
    "        features_count_correct += 1\n",
    "print(features_count_correct/count_total*100, \"%\")\n",
    "\n",
    "# calculating False/True positives and negatives, optional print statements added\n",
    "features_NN_TP = ((outputs_df[0] > 0.5) & (outputs_df[1] == 1)).sum()\n",
    "# print(features_NN_TP)\n",
    "features_NN_FN = ((outputs_df[0] < 0.5) & (outputs_df[1] == 1)).sum()\n",
    "# print(features_NN_FN)\n",
    "features_NN_TN = ((outputs_df[0] < 0.5) & (outputs_df[1] == 0)).sum()\n",
    "# print(features_NN_TN)\n",
    "features_NN_FP = ((outputs_df[0] > 0.5) & (outputs_df[1] == 0)).sum()\n",
    "# print(features_NN_FP)\n",
    "\n",
    "# calculating evaluation metrics for the models\n",
    "features_NN_sensitivity = features_NN_TP/(features_NN_TP+features_NN_FN)\n",
    "print(f\"Sensitivity of featuriser neural network: {features_NN_sensitivity}\")\n",
    "\n",
    "features_NN_specificity = features_NN_TN/(features_NN_TN+features_NN_FP)\n",
    "print(f\"Specificity of featuriser neural network: {features_NN_specificity}\")\n",
    "\n",
    "features_NN_CCR = ((features_NN_sensitivity+features_NN_specificity)/2)*100\n",
    "print(f\"Correct classfication rate of featuriser neural network: {features_NN_CCR}\")\n",
    "\n",
    "print(outputs_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
