{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import statements\n",
    "! pip install chemprop\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "! pip install pytorch-lightning wandb rdkit ogb deepchem\n",
    "import torch\n",
    "VERSION = torch.__version__\n",
    "! pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-{VERSION}.html\n",
    "! pip install torch-geometric\n",
    "! mkdir data/\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "IPythonConsole.ipython_useSVG = True  # < use SVGs instead of PNGs\n",
    "IPythonConsole.drawOptions.addAtomIndices = True  # adding indices for atoms\n",
    "IPythonConsole.drawOptions.addBondIndices = False  # not adding indices for bonds\n",
    "IPythonConsole.molSize = 200, 200\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "! pip install deepchem.data\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import GRU\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import NNConv, MLP, global_add_pool\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    download_url,\n",
    ")\n",
    "from ogb.utils import smiles2graph\n",
    "from deepchem.splits import RandomSplitter\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, global_add_pool\n",
    "from deepchem.feat import RDKitDescriptors\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### downloading data and assigning key columns from the dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/NinaB99/AI-for-Chemistry/main/Data/Bioavailibility.csv')\n",
    "smiles = df['SMILES'].values.tolist()\n",
    "y = df['logK(%F)'].values\n",
    "category = df['Category'].values.tolist()\n",
    "\n",
    "# normalising the bioavailability targets\n",
    "mean = y.mean()\n",
    "std = y.std()\n",
    "y = (y - mean) / std\n",
    "mean, std = mean.item(), std.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### classes for GNN and its MLP\n",
    "class CustomMLP(nn.Module):\n",
    "    '''a custom function for MLP that allows for dropout\n",
    "    '''\n",
    "    def __init__(self, layer_sizes, dropout_rate=0.5):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            if i < len(layer_sizes) - 2:  # No activation or dropout on the output layer\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class Graph_NN(pl.LightningModule):\n",
    "    '''neural network class. Hyperparameters and split data are inputs. Trained using the MSE.\n",
    "    '''\n",
    "    def __init__(self, hidden_dim, out_dim,\n",
    "                 train_data, valid_data, test_data,\n",
    "                 std, dropout_rate=0.5, batch_size=32, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.std = std  # std of data's target\n",
    "        self.train_data = train_data\n",
    "        self.valid_data = valid_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        # Initial layers\n",
    "        self.atom_emb = AtomEncoder(emb_dim=hidden_dim)\n",
    "        self.bond_emb = BondEncoder(emb_dim=hidden_dim)\n",
    "        # Message passing layers\n",
    "        nn = CustomMLP([hidden_dim, hidden_dim * 2, hidden_dim * hidden_dim], dropout_rate)\n",
    "        self.conv = NNConv(hidden_dim, hidden_dim, nn, aggr='mean')\n",
    "        self.gru = GRU(hidden_dim, hidden_dim)\n",
    "        # Readout layers\n",
    "        self.mlp = CustomMLP([hidden_dim, int(hidden_dim / 2), out_dim], dropout_rate)\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "\n",
    "        # Initialization\n",
    "        x = self.atom_emb(data.x)\n",
    "        h = x.unsqueeze(0)\n",
    "        edge_attr = self.bond_emb(data.edge_attr)\n",
    "\n",
    "        # Message passing\n",
    "        for i in range(3):\n",
    "            m = F.relu(self.conv(x, data.edge_index, edge_attr))  # send message and aggregation\n",
    "            x, h = self.gru(m.unsqueeze(0), h)  # node update\n",
    "            x = x.squeeze(0)\n",
    "\n",
    "        # Readout\n",
    "        x = global_add_pool(x, data.batch)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x.view(-1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Here we define the train loop.\n",
    "        out = self.forward(batch, mode=\"train\")\n",
    "        loss = F.mse_loss(out, batch.y)\n",
    "        self.log(\"Train loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Define validation step. At the end of every epoch, this will be executed\n",
    "        out = self.forward(batch, mode=\"valid\")\n",
    "        loss = F.mse_loss(out * self.std, batch.y * self.std)  # report MSE\n",
    "        self.log(\"Valid MSE\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # What to do in test\n",
    "        out = self.forward(batch, mode=\"test\")\n",
    "        loss = F.mse_loss(out * self.std, batch.y * self.std)  # report MSE\n",
    "        self.log(\"Test MSE\", loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Here we configure the optimization algorithm.\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.lr\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### creating data class for the GNN\n",
    "class csvGraphData(InMemoryDataset):\n",
    "    \"\"\"The solubility graph dataset using PyG\n",
    "    \"\"\"\n",
    "    # bioavailability dataset download link\n",
    "    raw_url = 'https://raw.githubusercontent.com/NinaB99/AI-for-Chemistry/main/Data/Bioavailibility.csv'\n",
    "\n",
    "    def __init__(self, root, transform=None):\n",
    "        super().__init__(root, transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['Bioavailibility.csv']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        print('Downloading dataset...')\n",
    "        file_path = download_url(self.raw_url, self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        # load raw data from a csv file\n",
    "        print(self.raw_paths)\n",
    "        df = pd.read_csv(self.raw_paths[0],sep=',')\n",
    "        smiles = df['SMILES'].values.tolist()\n",
    "        target = df['logK(%F)'].values.tolist()\n",
    "\n",
    "        # Convert SMILES into graph data\n",
    "        print('Converting SMILES strings into graphs...')\n",
    "        data_list = []\n",
    "        for i, smi in enumerate(tqdm(smiles)):\n",
    "\n",
    "            # get graph data from SMILES\n",
    "            graph = smiles2graph(smi)\n",
    "\n",
    "            # convert to tensor and pyg data\n",
    "            x = torch.tensor(graph['node_feat'], dtype=torch.long)\n",
    "            edge_index = torch.tensor(graph['edge_index'], dtype=torch.long)\n",
    "            edge_attr = torch.tensor(graph['edge_feat'], dtype=torch.long)\n",
    "            y = torch.tensor([target[i]], dtype=torch.float)\n",
    "            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "            data_list.append(data)\n",
    "\n",
    "        # save data\n",
    "        torch.save(self.collate(data_list), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### preparing data and splitting\n",
    "# create dataset\n",
    "dataset = csvGraphData('./data_pyg').shuffle()\n",
    "\n",
    "# Normalize target to mean = 0 and std = 1.\n",
    "mean = dataset.data.y.mean()\n",
    "std = dataset.data.y.std()\n",
    "dataset.data.y = (dataset.data.y - mean) / std\n",
    "mean, std = mean.item(), std.item()\n",
    "\n",
    "split data\n",
    "splitter = RandomSplitter()\n",
    "train_idx, valid_idx, test_idx = splitter.split(dataset, frac_train=0.7, frac_valid=0.1, frac_test=0.2)\n",
    "train_dataset_graph = dataset[train_idx]\n",
    "valid_dataset_graph = dataset[valid_idx]\n",
    "test_dataset_graph = dataset[test_idx]\n",
    "# optional print statements for testing:\n",
    "# print(f\"Train dataset shape: {train_dataset_graph.shape}\")\n",
    "# print(f\"Valid dataset shape: {valid_dataset_graph.shape}\")\n",
    "# print(f\"Test dataset shape: {test_dataset_graph.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### running an instance of the GNN\n",
    "wandb.init(project=\"gnn-bioavailibility\",\n",
    "           config={\n",
    "               \"batch_size\": 24,\n",
    "               \"learning_rate\": 0.001,\n",
    "               \"hidden_size\": 64,\n",
    "               \"max_epochs\": 150\n",
    "           })\n",
    "\n",
    "#creating instance of GNN and setting hyperparameters \n",
    "graph_branch = Graph_NN(\n",
    "     hidden_dim = wandb.config[\"hidden_size\"],\n",
    "     out_dim = 1,\n",
    "     std = std,\n",
    "     train_data = train_dataset_graph,\n",
    "     valid_data = valid_dataset_graph,\n",
    "     test_data = test_dataset_graph,\n",
    "     lr=wandb.config[\"learning_rate\"],\n",
    "     batch_size=wandb.config[\"batch_size\"]\n",
    ")\n",
    "wandb_logger = WandbLogger()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs = wandb.config[\"max_epochs\"],\n",
    "    logger = wandb_logger\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "trainer.fit(\n",
    "    model=graph_branch,\n",
    ")\n",
    "\n",
    "# Running test\n",
    "graph_results = trainer.test(ckpt_path=\"best\")\n",
    "wandb.finish()\n",
    "# Test RMSE\n",
    "graph_test_mse = graph_results[0][\"Test MSE\"]\n",
    "graph_test_rmse = graph_test_mse ** 0.5\n",
    "print(f\"\\Graph neural network model performance: RMSE on test set = {graph_test_rmse:.4f}.\\n\")\n",
    "# save this instance of the GNN\n",
    "torch.save(graph_branch.state_dict(), 'graph_nn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluating the predictions of the GNN\n",
    "# change mode of GNN\n",
    "graph_branch.eval()\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "test_loader = DataLoader(dataset, batch_size=24, shuffle=False)\n",
    "\n",
    "# Perform the forward pass to get predictions\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        predictions = graph_branch(batch)\n",
    "        test_predictions.append(predictions)\n",
    "\n",
    "# Concatenate all predictions\n",
    "test_predictions_tensor_graph = torch.cat(test_predictions)\n",
    "\n",
    "# Convert predictions back to NumPy array\n",
    "graph_predictions = test_predictions_tensor_graph.numpy().flatten()\n",
    "\n",
    "# Create outputs DataFrame containing predictions from GNN and the categorised real values\n",
    "outputs_df = pd.DataFrame({\n",
    "    \"Predictions\": graph_predictions,\n",
    "    \"Category\": category\n",
    "})\n",
    "\n",
    "outputs_df = outputs_df.reset_index(drop=True)\n",
    "count_total = len(outputs_df)\n",
    "\n",
    "# classifying the predictions as high or low and comparing to literature values\n",
    "graph_count_correct = 0\n",
    "for i in range(count_total):\n",
    "    if outputs_df[\"Predictions\"][i] > 0.5 and outputs_df[\"Category\"][i] == 1:\n",
    "        graph_count_correct += 1\n",
    "    elif outputs_df[\"Predictions\"][i] < 0.5 and outputs_df[\"Category\"][i] == 0:\n",
    "        graph_count_correct += 1\n",
    "print(graph_count_correct/count_total*100, \"%\")\n",
    "\n",
    "# checking the accuracy of the predictions\n",
    "graph_NN_TP = ((outputs_df[\"Predictions\"] > 0.5) & (outputs_df[\"Category\"] == 1)).sum()\n",
    "graph_NN_FN = ((outputs_df[\"Predictions\"] < 0.5) & (outputs_df[\"Category\"] == 1)).sum()\n",
    "graph_NN_TN = ((outputs_df[\"Predictions\"] < 0.5) & (outputs_df[\"Category\"] == 0)).sum()\n",
    "graph_NN_FP = ((outputs_df[\"Predictions\"] > 0.5) & (outputs_df[\"Category\"] == 0)).sum()\n",
    "\n",
    "# calculating evaluation metrics for the models\n",
    "graph_NN_sensitivity = graph_NN_TP / (graph_NN_TP + graph_NN_FN)\n",
    "print(f\"Sensitivity of graph neural network: {graph_NN_sensitivity}\")\n",
    "\n",
    "graph_NN_specificity = graph_NN_TN / (graph_NN_TN + graph_NN_FP)\n",
    "print(f\"Specificity of graph neural network: {graph_NN_specificity}\")\n",
    "\n",
    "graph_NN_CCR = ((graph_NN_sensitivity + graph_NN_specificity) / 2) * 100\n",
    "print(f\"Correct classification rate of graph neural network: {graph_NN_CCR}\")\n",
    "\n",
    "print(outputs_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
