{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article from Philippe:\n",
    "https://practicalcheminformatics.blogspot.com/2023/06/getting-real-with-molecular-property.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for running in colab:\n",
    "\n",
    "# !pip install rdkit\n",
    "# !pip install deepchem\n",
    "\n",
    "\n",
    "# !wget https://raw.githubusercontent.com/NinaB99/AI-for-Chemistry/main/Data/ADME_public_set_3521.csv\n",
    "# !wget https://raw.githubusercontent.com/NinaB99/AI-for-Chemistry/main/Data/11095_2013_1222_MOESM2_ESM.csv\n",
    "\n",
    "# load biogen data\n",
    "# import pandas as pd\n",
    "\n",
    "# biogen_data=pd.read_csv(\"ADME_public_set_3521.csv\")\n",
    "# print(biogen_data.columns)\n",
    "\n",
    "# #load bioavailabity data\n",
    "# bio_avail_data = pd.read_csv(\"11095_2013_1222_MOESM2_ESM.csv\",sep=\";\")\n",
    "# print(bio_avail_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'Name', 'InChI', 'InChIKey', 'SMILES', 'Solubility', 'SD',\n",
      "       'Ocurrences', 'Group', 'MolWt', 'MolLogP', 'MolMR', 'HeavyAtomCount',\n",
      "       'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumRotatableBonds',\n",
      "       'NumValenceElectrons', 'NumAromaticRings', 'NumSaturatedRings',\n",
      "       'NumAliphaticRings', 'RingCount', 'TPSA', 'LabuteASA', 'BalabanJ',\n",
      "       'BertzCT', 'MolW(Da)', 'LogP', 'Lipinski_rule'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# load biogen data\n",
    "import pandas as pd\n",
    "\n",
    "biogen_data=pd.read_csv(\"../Data/Biogen.csv\")\n",
    "#print(biogen_data.columns)\n",
    "\n",
    "#new data\n",
    "curated_data = pd.read_csv(\"../Data/CuratedSol.csv\")\n",
    "print(curated_data.columns)\n",
    "\n",
    "#load bioavailabity data\n",
    "bio_avail_data = pd.read_csv(\"../Data/Bioavailibility.csv\")\n",
    "#print(bio_avail_data.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use feed-forward NN using pytorch.\n",
    "train on solubility from dataset 1 first.\n",
    "then optimize for bioavailability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "from deepchem.feat import RDKitDescriptors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize(Dataframe: pd.DataFrame):\n",
    "    \n",
    "    \"\"\"Canonicalizes the SMILES from Dataframe. A column called 'SMILES' is requiered\n",
    "\n",
    "    Args: Dataframe with 'SMILES' column contaning smiles. \n",
    "    \"\"\"\n",
    "    \n",
    "    Dataframe['SMILES'] = Dataframe['SMILES'].apply(lambda x: Chem.MolToSmiles(Chem.MolFromSmiles(x))) #canonicalize smiles from a Dataframe                                          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "canonicalize(biogen_data)\n",
    "canonicalize(curated_data)\n",
    "\n",
    "#make sure to remove overlaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:03:19] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:19] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:19] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:21] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:21] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:21] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:21] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:21] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:21] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:24] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:24] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:24] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:29] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:29] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:29] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:43] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:45] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:45] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:45] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:46] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:46] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:46] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:50] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:50] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:50] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:50] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:50] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:50] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:51] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:52] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:59] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:59] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:03:59] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:00] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:00] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:00] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:02] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:02] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:02] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:02] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:02] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:02] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:03] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:03] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:03] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:12] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:12] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:12] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:17] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:17] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:17] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:17] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:17] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:17] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:17] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:17] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:17] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:19] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:19] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:19] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:24] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:24] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:24] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:30] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:30] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:30] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:39] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:39] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:39] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:39] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:39] WARNING: not removing hydrogen atom without neighbors\n",
      "[18:04:39] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### DATA PREPARATION ####\n",
    "# try with new data\n",
    "\n",
    "# Function to generate features from SMILES strings using RDKit descriptors\n",
    "def generate_features(smiles_list):\n",
    "    featurizer = RDKitDescriptors()\n",
    "    features = featurizer.featurize(smiles_list)\n",
    "    # Drop features containing invalid values\n",
    "    features = features[:, ~np.isnan(features).any(axis=0)]\n",
    "    return features\n",
    "\n",
    "#remove nan values from data\n",
    "data = data.dropna(subset=['Solubility'])\n",
    "\n",
    "#get x and y data (x is the molecular descriptors, y is the solubility)\n",
    "y_data = data[\"Solubility\"]\n",
    "\n",
    "print(len(y_data))\n",
    "\n",
    "# Generate features from SMILES data (get smiles from df)\n",
    "smiles = data[\"SMILES\"]\n",
    "X_data = generate_features(smiles)\n",
    "\n",
    "#split data into training and validation using train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "# Convert y pandas Series to NumPy array\n",
    "y_train = y_train.values.astype(np.float32)\n",
    "y_test = y_test.values.astype(np.float32)\n",
    "\n",
    "#scale x values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#convert data to pytorch tensors (like numpy arrays but for pytorch)\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Reshape the target tensor to match the shape of the output tensor\n",
    "y_train_tensor = y_train_tensor.view(-1, 1)\n",
    "# Reshape the target tensor to match the shape of the output tensor\n",
    "y_test_tensor = y_test_tensor.view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.368035824195333"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best Hyperparameters: {'activation': 'relu', 'hidden_layer_sizes': (512, 512, 512), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "Test Score (R^2): 0.1923752586047588\n"
     ]
    }
   ],
   "source": [
    "#### CREATING MODEL ####\n",
    "\n",
    "#haven't tried it with the newest settings\n",
    "\n",
    "# Define the neural network architecture\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_data.shape[1]  # Number of molecular descriptors. 201 different ones\n",
    "hidden_dim = 256   #best according to grid search\n",
    "output_dim = 1\n",
    "learning_rate = 0.001 #gets overruled by the grid search\n",
    "num_epochs = 800\n",
    "batch_size = 32\n",
    "\n",
    "#create dataloader for batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#initialize model, loss function (nn.MSELoss) and optimizer\n",
    "model = SolubilityPredictor(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "##### Grid-search for best hyperparameters #####\n",
    "# Define your hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(256,256,256),(512,512,512)],  # Number of neurons in the hidden layer(s)\n",
    "    'activation': ['relu', 'tanh'],  # Activation function\n",
    "    'solver': ['adam', 'sgd'],  # Optimization algorithm\n",
    "    'learning_rate': ['constant', 'adaptive'],  # Learning rate schedule\n",
    "}\n",
    "\n",
    "# Create an MLPRegressor object\n",
    "mlp = MLPRegressor(max_iter=num_epochs, batch_size=batch_size)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_score = model.score(X_test, y_test)\n",
    "print(\"Test Score (R^2):\", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.3812762971349799\n",
      "Test RMSE: 0.6174757461916864\n",
      "Test MAE: 0.4129440122100839\n",
      "Test R^2: 0.1923752586047588\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the test set\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate additional metrics\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500, Loss: 629.7532318234444\n",
      "Epoch 2/1500, Loss: 349.3753622174263\n",
      "Epoch 3/1500, Loss: 300.51816365122795\n",
      "Epoch 4/1500, Loss: 271.09655803442\n",
      "Epoch 5/1500, Loss: 251.77361357212067\n",
      "Epoch 6/1500, Loss: 244.4482697546482\n",
      "Epoch 7/1500, Loss: 219.4488686323166\n",
      "Epoch 8/1500, Loss: 209.87260562181473\n",
      "Epoch 9/1500, Loss: 199.35788887739182\n",
      "Epoch 10/1500, Loss: 187.55638401210308\n",
      "Epoch 11/1500, Loss: 181.75955553352833\n",
      "Epoch 12/1500, Loss: 177.62774194777012\n",
      "Epoch 13/1500, Loss: 173.41815902292728\n",
      "Epoch 14/1500, Loss: 157.04992344975471\n",
      "Epoch 15/1500, Loss: 152.091867223382\n",
      "Epoch 16/1500, Loss: 144.3432312309742\n",
      "Epoch 17/1500, Loss: 136.0918851941824\n",
      "Epoch 18/1500, Loss: 140.43613086640835\n",
      "Epoch 19/1500, Loss: 141.20106229186058\n",
      "Epoch 20/1500, Loss: 128.21031394600868\n",
      "Epoch 21/1500, Loss: 127.69747073948383\n",
      "Epoch 22/1500, Loss: 118.70110666751862\n",
      "Epoch 23/1500, Loss: 118.87316009402275\n",
      "Epoch 24/1500, Loss: 119.48442097008228\n",
      "Epoch 25/1500, Loss: 107.0220463052392\n",
      "Epoch 26/1500, Loss: 103.89200886338949\n",
      "Epoch 27/1500, Loss: 103.50911761820316\n",
      "Epoch 28/1500, Loss: 102.36915802955627\n",
      "Epoch 29/1500, Loss: 102.90956442058086\n",
      "Epoch 30/1500, Loss: 91.29093676805496\n",
      "Epoch 31/1500, Loss: 93.96592818945646\n",
      "Epoch 32/1500, Loss: 87.78699734061956\n",
      "Epoch 33/1500, Loss: 88.37679363787174\n",
      "Epoch 34/1500, Loss: 86.06840731948614\n",
      "Epoch 35/1500, Loss: 78.26518175005913\n",
      "Epoch 36/1500, Loss: 81.45724297314882\n",
      "Epoch 37/1500, Loss: 82.6868957914412\n",
      "Epoch 38/1500, Loss: 81.87960606068373\n",
      "Epoch 39/1500, Loss: 77.26003575325012\n",
      "Epoch 40/1500, Loss: 72.98079778254032\n",
      "Epoch 41/1500, Loss: 77.1298865750432\n",
      "Epoch 42/1500, Loss: 76.10341779887676\n",
      "Epoch 43/1500, Loss: 70.69422896206379\n",
      "Epoch 44/1500, Loss: 65.36492148041725\n",
      "Epoch 45/1500, Loss: 67.47261980548501\n",
      "Epoch 46/1500, Loss: 66.10487832129002\n",
      "Epoch 47/1500, Loss: 65.58355270326138\n",
      "Epoch 48/1500, Loss: 63.37973391637206\n",
      "Epoch 49/1500, Loss: 56.7341475635767\n",
      "Epoch 50/1500, Loss: 62.665827073156834\n",
      "Epoch 51/1500, Loss: 61.027587074786425\n",
      "Epoch 52/1500, Loss: 66.49667370319366\n",
      "Epoch 53/1500, Loss: 61.850411769002676\n",
      "Epoch 54/1500, Loss: 58.87076824530959\n",
      "Epoch 55/1500, Loss: 56.28142819926143\n",
      "Epoch 56/1500, Loss: 56.28165243938565\n",
      "Epoch 57/1500, Loss: 57.28562208265066\n",
      "Epoch 58/1500, Loss: 51.10926112532616\n",
      "Epoch 59/1500, Loss: 54.62640484422445\n",
      "Epoch 60/1500, Loss: 53.96449826657772\n",
      "Epoch 61/1500, Loss: 51.13225880265236\n",
      "Epoch 62/1500, Loss: 52.07942136377096\n",
      "Epoch 63/1500, Loss: 49.59358285740018\n",
      "Epoch 64/1500, Loss: 49.7467822432518\n",
      "Epoch 65/1500, Loss: 49.12137168645859\n",
      "Epoch 66/1500, Loss: 51.14538963139057\n",
      "Epoch 67/1500, Loss: 50.351494040340185\n",
      "Epoch 68/1500, Loss: 47.86403153836727\n",
      "Epoch 69/1500, Loss: 48.45769784972072\n",
      "Epoch 70/1500, Loss: 48.82363141141832\n",
      "Epoch 71/1500, Loss: 44.27365554496646\n",
      "Epoch 72/1500, Loss: 43.34363870322704\n",
      "Epoch 73/1500, Loss: 45.732816342264414\n",
      "Epoch 74/1500, Loss: 44.924517437815666\n",
      "Epoch 75/1500, Loss: 44.87527311965823\n",
      "Epoch 76/1500, Loss: 43.75206508114934\n",
      "Epoch 77/1500, Loss: 44.253939773887396\n",
      "Epoch 78/1500, Loss: 45.231874741613865\n",
      "Epoch 79/1500, Loss: 43.950086671859026\n",
      "Epoch 80/1500, Loss: 40.554028891026974\n",
      "Epoch 81/1500, Loss: 40.0171340405941\n",
      "Epoch 82/1500, Loss: 39.07324984297156\n",
      "Epoch 83/1500, Loss: 39.75317393988371\n",
      "Epoch 84/1500, Loss: 41.85429810732603\n",
      "Epoch 85/1500, Loss: 37.37375436350703\n",
      "Epoch 86/1500, Loss: 35.127072378993034\n",
      "Epoch 87/1500, Loss: 36.919119607657194\n",
      "Epoch 88/1500, Loss: 36.10051163099706\n",
      "Epoch 89/1500, Loss: 39.96607096493244\n",
      "Epoch 90/1500, Loss: 37.81076941639185\n",
      "Epoch 91/1500, Loss: 34.11335752904415\n",
      "Epoch 92/1500, Loss: 31.557937441393733\n",
      "Epoch 93/1500, Loss: 35.86505272611976\n",
      "Epoch 94/1500, Loss: 33.78745614737272\n",
      "Epoch 95/1500, Loss: 32.95774741657078\n",
      "Epoch 96/1500, Loss: 33.01073056086898\n",
      "Epoch 97/1500, Loss: 42.71268523298204\n",
      "Epoch 98/1500, Loss: 38.2953275218606\n",
      "Epoch 99/1500, Loss: 33.493637977167964\n",
      "Epoch 100/1500, Loss: 32.01330788247287\n",
      "Epoch 101/1500, Loss: 29.49416621774435\n",
      "Epoch 102/1500, Loss: 31.76509278640151\n",
      "Epoch 103/1500, Loss: 34.78964191302657\n",
      "Epoch 104/1500, Loss: 31.011593967676163\n",
      "Epoch 105/1500, Loss: 29.17618766054511\n",
      "Epoch 106/1500, Loss: 32.84764939546585\n",
      "Epoch 107/1500, Loss: 35.462799897417426\n",
      "Epoch 108/1500, Loss: 28.885404163971543\n",
      "Epoch 109/1500, Loss: 29.21932421065867\n",
      "Epoch 110/1500, Loss: 27.669890644028783\n",
      "Epoch 111/1500, Loss: 35.27916968613863\n",
      "Epoch 112/1500, Loss: 30.108414815738797\n",
      "Epoch 113/1500, Loss: 30.047499867156148\n",
      "Epoch 114/1500, Loss: 28.327559495344758\n",
      "Epoch 115/1500, Loss: 30.53013599291444\n",
      "Epoch 116/1500, Loss: 28.038461662828922\n",
      "Epoch 117/1500, Loss: 32.9137005507946\n",
      "Epoch 118/1500, Loss: 29.10505697131157\n",
      "Epoch 119/1500, Loss: 27.18159531056881\n",
      "Epoch 120/1500, Loss: 27.06933457404375\n",
      "Epoch 121/1500, Loss: 25.59908668883145\n",
      "Epoch 122/1500, Loss: 25.317852176725864\n",
      "Epoch 123/1500, Loss: 25.468191092833877\n",
      "Epoch 124/1500, Loss: 27.997702945023775\n",
      "Epoch 125/1500, Loss: 27.62583322264254\n",
      "Epoch 126/1500, Loss: 26.486458649858832\n",
      "Epoch 127/1500, Loss: 31.25384747982025\n",
      "Epoch 128/1500, Loss: 28.03016698732972\n",
      "Epoch 129/1500, Loss: 24.770069427788258\n",
      "Epoch 130/1500, Loss: 24.715159002691507\n",
      "Epoch 131/1500, Loss: 25.30430125631392\n",
      "Epoch 132/1500, Loss: 26.48483430966735\n",
      "Epoch 133/1500, Loss: 32.00869821757078\n",
      "Epoch 134/1500, Loss: 26.942257598042488\n",
      "Epoch 135/1500, Loss: 25.52785506658256\n",
      "Epoch 136/1500, Loss: 30.57137723825872\n",
      "Epoch 137/1500, Loss: 28.926968332380056\n",
      "Epoch 138/1500, Loss: 23.7226075604558\n",
      "Epoch 139/1500, Loss: 22.908641850575805\n",
      "Epoch 140/1500, Loss: 21.58191116154194\n",
      "Epoch 141/1500, Loss: 23.011726807802916\n",
      "Epoch 142/1500, Loss: 21.588544810190797\n",
      "Epoch 143/1500, Loss: 24.421804847195745\n",
      "Epoch 144/1500, Loss: 23.294754289090633\n",
      "Epoch 145/1500, Loss: 24.107679780572653\n",
      "Epoch 146/1500, Loss: 23.60374520532787\n",
      "Epoch 147/1500, Loss: 23.57172010280192\n",
      "Epoch 148/1500, Loss: 29.907097086310387\n",
      "Epoch 149/1500, Loss: 29.821838442236185\n",
      "Epoch 150/1500, Loss: 24.32277063280344\n",
      "Epoch 151/1500, Loss: 21.433174115605652\n",
      "Epoch 152/1500, Loss: 21.579885696060956\n",
      "Epoch 153/1500, Loss: 21.69822696968913\n",
      "Epoch 154/1500, Loss: 21.84204711765051\n",
      "Epoch 155/1500, Loss: 24.244503438472748\n",
      "Epoch 156/1500, Loss: 22.185353312641382\n",
      "Epoch 157/1500, Loss: 23.928932461887598\n",
      "Epoch 158/1500, Loss: 22.695289511233568\n",
      "Epoch 159/1500, Loss: 25.451716603711247\n",
      "Epoch 160/1500, Loss: 28.894106309860945\n",
      "Epoch 161/1500, Loss: 25.224403619766235\n",
      "Epoch 162/1500, Loss: 24.33382213488221\n",
      "Epoch 163/1500, Loss: 20.869623750448227\n",
      "Epoch 164/1500, Loss: 20.694503128528595\n",
      "Epoch 165/1500, Loss: 22.28197759948671\n",
      "Epoch 166/1500, Loss: 19.882884170860052\n",
      "Epoch 167/1500, Loss: 20.91467460989952\n",
      "Epoch 168/1500, Loss: 22.45335130766034\n",
      "Epoch 169/1500, Loss: 28.0777616109699\n",
      "Epoch 170/1500, Loss: 29.787947345525026\n",
      "Epoch 171/1500, Loss: 20.956332948058844\n",
      "Epoch 172/1500, Loss: 18.800789098255336\n",
      "Epoch 173/1500, Loss: 20.183290218934417\n",
      "Epoch 174/1500, Loss: 20.829557359218597\n",
      "Epoch 175/1500, Loss: 19.800100988708436\n",
      "Epoch 176/1500, Loss: 19.16227979399264\n",
      "Epoch 177/1500, Loss: 20.228313355706632\n",
      "Epoch 178/1500, Loss: 19.677381055429578\n",
      "Epoch 179/1500, Loss: 18.71085705049336\n",
      "Epoch 180/1500, Loss: 18.881988676264882\n",
      "Epoch 181/1500, Loss: 22.231788461096585\n",
      "Epoch 182/1500, Loss: 20.627709859982133\n",
      "Epoch 183/1500, Loss: 22.560214817523956\n",
      "Epoch 184/1500, Loss: 24.053499177098274\n",
      "Epoch 185/1500, Loss: 22.50174753367901\n",
      "Epoch 186/1500, Loss: 20.849635606631637\n",
      "Epoch 187/1500, Loss: 19.624038197100163\n",
      "Epoch 188/1500, Loss: 22.615004274062812\n",
      "Epoch 189/1500, Loss: 19.591086268424988\n",
      "Epoch 190/1500, Loss: 20.026907613500953\n",
      "Epoch 191/1500, Loss: 18.57590371184051\n",
      "Epoch 192/1500, Loss: 22.028075038455427\n",
      "Epoch 193/1500, Loss: 18.481713478453457\n",
      "Epoch 194/1500, Loss: 20.11507753189653\n",
      "Epoch 195/1500, Loss: 24.8693039547652\n",
      "Epoch 196/1500, Loss: 23.412266126833856\n",
      "Epoch 197/1500, Loss: 17.779205065220594\n",
      "Epoch 198/1500, Loss: 17.22172538470477\n",
      "Epoch 199/1500, Loss: 16.51122168265283\n",
      "Epoch 200/1500, Loss: 17.59143339470029\n",
      "Epoch 201/1500, Loss: 17.097007615491748\n",
      "Epoch 202/1500, Loss: 19.04802234005183\n",
      "Epoch 203/1500, Loss: 18.13355749566108\n",
      "Epoch 204/1500, Loss: 17.532992511987686\n",
      "Epoch 205/1500, Loss: 21.180441591888666\n",
      "Epoch 206/1500, Loss: 18.219240956939757\n",
      "Epoch 207/1500, Loss: 18.786530889570713\n",
      "Epoch 208/1500, Loss: 18.036921455524862\n",
      "Epoch 209/1500, Loss: 19.023009032011032\n",
      "Epoch 210/1500, Loss: 18.84749636799097\n",
      "Epoch 211/1500, Loss: 17.78946771286428\n",
      "Epoch 212/1500, Loss: 17.15840866509825\n",
      "Epoch 213/1500, Loss: 16.57715025637299\n",
      "Epoch 214/1500, Loss: 16.993732936680317\n",
      "Epoch 215/1500, Loss: 18.37366726435721\n",
      "Epoch 216/1500, Loss: 17.511993929743767\n",
      "Epoch 217/1500, Loss: 19.349541772156954\n",
      "Epoch 218/1500, Loss: 24.667746383696795\n",
      "Epoch 219/1500, Loss: 21.52328203152865\n",
      "Epoch 220/1500, Loss: 21.411923924461007\n",
      "Epoch 221/1500, Loss: 15.75462558772415\n",
      "Epoch 222/1500, Loss: 20.293219778686762\n",
      "Epoch 223/1500, Loss: 16.92708166129887\n",
      "Epoch 224/1500, Loss: 18.24144798051566\n",
      "Epoch 225/1500, Loss: 28.62763554044068\n",
      "Epoch 226/1500, Loss: 17.781741383485496\n",
      "Epoch 227/1500, Loss: 16.626652392093092\n",
      "Epoch 228/1500, Loss: 18.908492056652904\n",
      "Epoch 229/1500, Loss: 16.070246285758913\n",
      "Epoch 230/1500, Loss: 17.230244039557874\n",
      "Epoch 231/1500, Loss: 16.53441507741809\n",
      "Epoch 232/1500, Loss: 15.864058124832809\n",
      "Epoch 233/1500, Loss: 15.414019706659019\n",
      "Epoch 234/1500, Loss: 15.416281745769083\n",
      "Epoch 235/1500, Loss: 15.973343206569552\n",
      "Epoch 236/1500, Loss: 18.343302633613348\n",
      "Epoch 237/1500, Loss: 17.70719520840794\n",
      "Epoch 238/1500, Loss: 16.99820731114596\n",
      "Epoch 239/1500, Loss: 15.91496009286493\n",
      "Epoch 240/1500, Loss: 15.382197224535048\n",
      "Epoch 241/1500, Loss: 15.42710599116981\n",
      "Epoch 242/1500, Loss: 15.705119264312088\n",
      "Epoch 243/1500, Loss: 16.75483092200011\n",
      "Epoch 244/1500, Loss: 15.723406867589802\n",
      "Epoch 245/1500, Loss: 16.460873105563223\n",
      "Epoch 246/1500, Loss: 16.346260577440262\n",
      "Epoch 247/1500, Loss: 15.388490119948983\n",
      "Epoch 248/1500, Loss: 15.723456506617367\n",
      "Epoch 249/1500, Loss: 16.48023009672761\n",
      "Epoch 250/1500, Loss: 14.423002048395574\n",
      "Epoch 251/1500, Loss: 15.399427788332105\n",
      "Epoch 252/1500, Loss: 15.067958968691528\n",
      "Epoch 253/1500, Loss: 15.505523047409952\n",
      "Epoch 254/1500, Loss: 15.833424653857946\n",
      "Epoch 255/1500, Loss: 16.84201435931027\n",
      "Epoch 256/1500, Loss: 15.914649642072618\n",
      "Epoch 257/1500, Loss: 16.05146094178781\n",
      "Epoch 258/1500, Loss: 14.776097333990037\n",
      "Epoch 259/1500, Loss: 14.306365843862295\n",
      "Epoch 260/1500, Loss: 14.781680611893535\n",
      "Epoch 261/1500, Loss: 15.077704658731818\n",
      "Epoch 262/1500, Loss: 15.108352199196815\n",
      "Epoch 263/1500, Loss: 16.226621369831264\n",
      "Epoch 264/1500, Loss: 15.018923153169453\n",
      "Epoch 265/1500, Loss: 16.346040475182235\n",
      "Epoch 266/1500, Loss: 19.39998573809862\n",
      "Epoch 267/1500, Loss: 16.74178002215922\n",
      "Epoch 268/1500, Loss: 14.32414901163429\n",
      "Epoch 269/1500, Loss: 14.937900758348405\n",
      "Epoch 270/1500, Loss: 14.96403413079679\n",
      "Epoch 271/1500, Loss: 13.395884544122964\n",
      "Epoch 272/1500, Loss: 15.288513885810971\n",
      "Epoch 273/1500, Loss: 17.376709254458547\n",
      "Epoch 274/1500, Loss: 15.345515266992152\n",
      "Epoch 275/1500, Loss: 13.795351691544056\n",
      "Epoch 276/1500, Loss: 14.11777025833726\n",
      "Epoch 277/1500, Loss: 13.340593971777707\n",
      "Epoch 278/1500, Loss: 14.406002508476377\n",
      "Epoch 279/1500, Loss: 14.043702230788767\n",
      "Epoch 280/1500, Loss: 14.09211881738156\n",
      "Epoch 281/1500, Loss: 14.670005052350461\n",
      "Epoch 282/1500, Loss: 14.96114624850452\n",
      "Epoch 283/1500, Loss: 14.807649916037917\n",
      "Epoch 284/1500, Loss: 14.729844007641077\n",
      "Epoch 285/1500, Loss: 15.809261200018227\n",
      "Epoch 286/1500, Loss: 14.788653925061226\n",
      "Epoch 287/1500, Loss: 22.4229650599882\n",
      "Epoch 288/1500, Loss: 17.739014165475965\n",
      "Epoch 289/1500, Loss: 15.932546718977392\n",
      "Epoch 290/1500, Loss: 17.082482832483947\n",
      "Epoch 291/1500, Loss: 65.83943709172308\n",
      "Epoch 292/1500, Loss: 17.610070953145623\n",
      "Epoch 293/1500, Loss: 15.22931208787486\n",
      "Epoch 294/1500, Loss: 14.402863058261573\n",
      "Epoch 295/1500, Loss: 13.515415528789163\n",
      "Epoch 296/1500, Loss: 12.817762132268399\n",
      "Epoch 297/1500, Loss: 12.126935448963195\n",
      "Epoch 298/1500, Loss: 12.596523540094495\n",
      "Epoch 299/1500, Loss: 13.420251448638737\n",
      "Epoch 300/1500, Loss: 12.70795467402786\n",
      "Epoch 301/1500, Loss: 13.232201936654747\n",
      "Epoch 302/1500, Loss: 13.74675817321986\n",
      "Epoch 303/1500, Loss: 14.87235114723444\n",
      "Epoch 304/1500, Loss: 15.084485850296915\n",
      "Epoch 305/1500, Loss: 15.176437392830849\n",
      "Epoch 306/1500, Loss: 14.476099603809416\n",
      "Epoch 307/1500, Loss: 14.683565316721797\n",
      "Epoch 308/1500, Loss: 15.449730835855007\n",
      "Epoch 309/1500, Loss: 14.001369730569422\n",
      "Epoch 310/1500, Loss: 14.175157936289907\n",
      "Epoch 311/1500, Loss: 14.315790307708085\n",
      "Epoch 312/1500, Loss: 13.83468209952116\n",
      "Epoch 313/1500, Loss: 16.152043990790844\n",
      "Epoch 314/1500, Loss: 13.851380127016455\n",
      "Epoch 315/1500, Loss: 13.159521861933172\n",
      "Epoch 316/1500, Loss: 12.338747047353536\n",
      "Epoch 317/1500, Loss: 11.736757796723396\n",
      "Epoch 318/1500, Loss: 12.782328535802662\n",
      "Epoch 319/1500, Loss: 12.589886995498091\n",
      "Epoch 320/1500, Loss: 13.881319773383439\n",
      "Epoch 321/1500, Loss: 15.137236035428941\n",
      "Epoch 322/1500, Loss: 13.785110175609589\n",
      "Epoch 323/1500, Loss: 13.564859974198043\n",
      "Epoch 324/1500, Loss: 13.650057499296963\n",
      "Epoch 325/1500, Loss: 13.374345545191318\n",
      "Epoch 326/1500, Loss: 12.467716565355659\n",
      "Epoch 327/1500, Loss: 12.287846938706934\n",
      "Epoch 328/1500, Loss: 13.092798225581646\n",
      "Epoch 329/1500, Loss: 12.556177464313805\n",
      "Epoch 330/1500, Loss: 13.773415951058269\n",
      "Epoch 331/1500, Loss: 13.345141794532537\n",
      "Epoch 332/1500, Loss: 15.959878087975085\n",
      "Epoch 333/1500, Loss: 14.234814080409706\n",
      "Epoch 334/1500, Loss: 15.907321913167834\n",
      "Epoch 335/1500, Loss: 14.973950029350817\n",
      "Epoch 336/1500, Loss: 14.67246084753424\n",
      "Epoch 337/1500, Loss: 20.086876736022532\n",
      "Epoch 338/1500, Loss: 15.326164693571627\n",
      "Epoch 339/1500, Loss: 20.575548957102\n",
      "Epoch 340/1500, Loss: 15.799832601100206\n",
      "Epoch 341/1500, Loss: 12.915752768982202\n",
      "Epoch 342/1500, Loss: 11.136401093099266\n",
      "Epoch 343/1500, Loss: 12.305361358914524\n",
      "Epoch 344/1500, Loss: 11.525699069257826\n",
      "Epoch 345/1500, Loss: 12.558588488027453\n",
      "Epoch 346/1500, Loss: 12.182690675370395\n",
      "Epoch 347/1500, Loss: 11.186949193943292\n",
      "Epoch 348/1500, Loss: 15.19016607105732\n",
      "Epoch 349/1500, Loss: 12.476380417589098\n",
      "Epoch 350/1500, Loss: 12.30397941917181\n",
      "Epoch 351/1500, Loss: 13.224719799123704\n",
      "Epoch 352/1500, Loss: 12.165401035919785\n",
      "Epoch 353/1500, Loss: 12.36050972621888\n",
      "Epoch 354/1500, Loss: 13.533170985989273\n",
      "Epoch 355/1500, Loss: 14.85607129149139\n",
      "Epoch 356/1500, Loss: 13.7267512595281\n",
      "Epoch 357/1500, Loss: 13.365344641264528\n",
      "Epoch 358/1500, Loss: 11.39703707024455\n",
      "Epoch 359/1500, Loss: 11.459056596271694\n",
      "Epoch 360/1500, Loss: 12.44706625584513\n",
      "Epoch 361/1500, Loss: 12.6228634044528\n",
      "Epoch 362/1500, Loss: 11.813390840310603\n",
      "Epoch 363/1500, Loss: 11.329894147347659\n",
      "Epoch 364/1500, Loss: 12.648636328056455\n",
      "Epoch 365/1500, Loss: 11.804960483219475\n",
      "Epoch 366/1500, Loss: 11.427559503354132\n",
      "Epoch 367/1500, Loss: 11.860634485725313\n",
      "Epoch 368/1500, Loss: 13.81059566885233\n",
      "Epoch 369/1500, Loss: 13.59926966484636\n",
      "Epoch 370/1500, Loss: 13.163162202574313\n",
      "Epoch 371/1500, Loss: 12.797484868671745\n",
      "Epoch 372/1500, Loss: 13.557794742286205\n",
      "Epoch 373/1500, Loss: 11.815973484888673\n",
      "Epoch 374/1500, Loss: 12.34948323853314\n",
      "Epoch 375/1500, Loss: 12.437363591045141\n",
      "Epoch 376/1500, Loss: 12.70933795394376\n",
      "Epoch 377/1500, Loss: 11.205723709892482\n",
      "Epoch 378/1500, Loss: 11.365540578495711\n",
      "Epoch 379/1500, Loss: 16.501363510265946\n",
      "Epoch 380/1500, Loss: 11.923045199364424\n",
      "Epoch 381/1500, Loss: 11.752763056196272\n",
      "Epoch 382/1500, Loss: 11.338154309429228\n",
      "Epoch 383/1500, Loss: 12.152764460537583\n",
      "Epoch 384/1500, Loss: 12.365036134142429\n",
      "Epoch 385/1500, Loss: 14.248148426413536\n",
      "Epoch 386/1500, Loss: 12.18976515950635\n",
      "Epoch 387/1500, Loss: 12.121812770143151\n",
      "Epoch 388/1500, Loss: 10.622108032926917\n",
      "Epoch 389/1500, Loss: 12.46750184521079\n",
      "Epoch 390/1500, Loss: 11.537550937850028\n",
      "Epoch 391/1500, Loss: 11.624001600779593\n",
      "Epoch 392/1500, Loss: 11.906581752933562\n",
      "Epoch 393/1500, Loss: 12.033223340753466\n",
      "Epoch 394/1500, Loss: 11.929085875861347\n",
      "Epoch 395/1500, Loss: 11.112459927797318\n",
      "Epoch 396/1500, Loss: 11.000538995489478\n",
      "Epoch 397/1500, Loss: 11.641628910787404\n",
      "Epoch 398/1500, Loss: 13.503378787543625\n",
      "Epoch 399/1500, Loss: 14.027048591524363\n",
      "Epoch 400/1500, Loss: 13.35034884698689\n",
      "Epoch 401/1500, Loss: 13.6792205106467\n",
      "Epoch 402/1500, Loss: 10.808360972441733\n",
      "Epoch 403/1500, Loss: 10.049126426689327\n",
      "Epoch 404/1500, Loss: 10.93216300290078\n",
      "Epoch 405/1500, Loss: 10.373327300418168\n",
      "Epoch 406/1500, Loss: 10.532646554522216\n",
      "Epoch 407/1500, Loss: 11.59342962084338\n",
      "Epoch 408/1500, Loss: 13.986910294741392\n",
      "Epoch 409/1500, Loss: 12.80837070569396\n",
      "Epoch 410/1500, Loss: 12.751497089862823\n",
      "Epoch 411/1500, Loss: 11.91226978553459\n",
      "Epoch 412/1500, Loss: 10.79509987635538\n",
      "Epoch 413/1500, Loss: 15.464652949944139\n",
      "Epoch 414/1500, Loss: 12.029345024842769\n",
      "Epoch 415/1500, Loss: 10.1695940149948\n",
      "Epoch 416/1500, Loss: 12.569260532967746\n",
      "Epoch 417/1500, Loss: 11.722510055173188\n",
      "Epoch 418/1500, Loss: 10.119799613952637\n",
      "Epoch 419/1500, Loss: 9.764181670732796\n",
      "Epoch 420/1500, Loss: 11.3018122240901\n",
      "Epoch 421/1500, Loss: 11.046841876581311\n",
      "Epoch 422/1500, Loss: 11.244454769417644\n",
      "Epoch 423/1500, Loss: 12.469701515510678\n",
      "Epoch 424/1500, Loss: 12.924429521895945\n",
      "Epoch 425/1500, Loss: 13.848469308577478\n",
      "Epoch 426/1500, Loss: 12.669053842313588\n",
      "Epoch 427/1500, Loss: 13.124427924398333\n",
      "Epoch 428/1500, Loss: 10.665197614114732\n",
      "Epoch 429/1500, Loss: 12.17516293283552\n",
      "Epoch 430/1500, Loss: 10.755599403288215\n",
      "Epoch 431/1500, Loss: 10.689153639599681\n",
      "Epoch 432/1500, Loss: 10.82473672926426\n",
      "Epoch 433/1500, Loss: 10.955261513590813\n",
      "Epoch 434/1500, Loss: 10.031671585515141\n",
      "Epoch 435/1500, Loss: 10.93347305757925\n",
      "Epoch 436/1500, Loss: 11.4056246727705\n",
      "Epoch 437/1500, Loss: 11.439184247981757\n",
      "Epoch 438/1500, Loss: 10.799974120687693\n",
      "Epoch 439/1500, Loss: 10.364824892487377\n",
      "Epoch 440/1500, Loss: 10.605366153176874\n",
      "Epoch 441/1500, Loss: 10.832452160771936\n",
      "Epoch 442/1500, Loss: 10.497859152033925\n",
      "Epoch 443/1500, Loss: 10.341056826990098\n",
      "Epoch 444/1500, Loss: 10.675652035046369\n",
      "Epoch 445/1500, Loss: 11.889306855387986\n",
      "Epoch 446/1500, Loss: 10.840069754049182\n",
      "Epoch 447/1500, Loss: 11.404122424777597\n",
      "Epoch 448/1500, Loss: 11.520537071395665\n",
      "Epoch 449/1500, Loss: 11.231574756558985\n",
      "Epoch 450/1500, Loss: 10.910672680474818\n",
      "Epoch 451/1500, Loss: 10.754328286275268\n",
      "Epoch 452/1500, Loss: 11.128277835901827\n",
      "Epoch 453/1500, Loss: 10.717022818047553\n",
      "Epoch 454/1500, Loss: 11.872384087182581\n",
      "Epoch 455/1500, Loss: 12.996940505690873\n",
      "Epoch 456/1500, Loss: 10.82342415349558\n",
      "Epoch 457/1500, Loss: 10.10003136890009\n",
      "Epoch 458/1500, Loss: 10.546043267473578\n",
      "Epoch 459/1500, Loss: 10.560736414510757\n",
      "Epoch 460/1500, Loss: 11.859483161009848\n",
      "Epoch 461/1500, Loss: 10.581865738611668\n",
      "Epoch 462/1500, Loss: 10.539933071937412\n",
      "Epoch 463/1500, Loss: 10.23414698895067\n",
      "Epoch 464/1500, Loss: 9.895609444472939\n",
      "Epoch 465/1500, Loss: 9.795186017174274\n",
      "Epoch 466/1500, Loss: 10.899643101729453\n",
      "Epoch 467/1500, Loss: 11.283544931095093\n",
      "Epoch 468/1500, Loss: 11.51417980529368\n",
      "Epoch 469/1500, Loss: 11.858228761237115\n",
      "Epoch 470/1500, Loss: 12.142456554342061\n",
      "Epoch 471/1500, Loss: 11.284741769079119\n",
      "Epoch 472/1500, Loss: 10.93020104477182\n",
      "Epoch 473/1500, Loss: 10.642114378046244\n",
      "Epoch 474/1500, Loss: 9.441697128582746\n",
      "Epoch 475/1500, Loss: 9.297204139176756\n",
      "Epoch 476/1500, Loss: 9.60666936589405\n",
      "Epoch 477/1500, Loss: 9.222936515696347\n",
      "Epoch 478/1500, Loss: 9.315845222212374\n",
      "Epoch 479/1500, Loss: 10.76902576861903\n",
      "Epoch 480/1500, Loss: 10.943486054427922\n",
      "Epoch 481/1500, Loss: 11.066659552510828\n",
      "Epoch 482/1500, Loss: 9.785359274130315\n",
      "Epoch 483/1500, Loss: 9.407328034751117\n",
      "Epoch 484/1500, Loss: 9.469429036136717\n",
      "Epoch 485/1500, Loss: 9.54368921648711\n",
      "Epoch 486/1500, Loss: 9.817266282159835\n",
      "Epoch 487/1500, Loss: 10.515266780275851\n",
      "Epoch 488/1500, Loss: 10.099156144075096\n",
      "Epoch 489/1500, Loss: 11.97157817427069\n",
      "Epoch 490/1500, Loss: 11.506313314661384\n",
      "Epoch 491/1500, Loss: 11.772513531148434\n",
      "Epoch 492/1500, Loss: 10.472616453189403\n",
      "Epoch 493/1500, Loss: 10.771720582153648\n",
      "Epoch 494/1500, Loss: 10.049933726433665\n",
      "Epoch 495/1500, Loss: 9.420886897481978\n",
      "Epoch 496/1500, Loss: 8.929465762805194\n",
      "Epoch 497/1500, Loss: 8.933529163245112\n",
      "Epoch 498/1500, Loss: 10.664855195442215\n",
      "Epoch 499/1500, Loss: 10.683534253854305\n",
      "Epoch 500/1500, Loss: 10.817042582202703\n",
      "Epoch 501/1500, Loss: 10.403502164408565\n",
      "Epoch 502/1500, Loss: 11.047116927336901\n",
      "Epoch 503/1500, Loss: 10.990214076358825\n",
      "Epoch 504/1500, Loss: 9.913282661233097\n",
      "Epoch 505/1500, Loss: 9.454584969207644\n",
      "Epoch 506/1500, Loss: 9.902511021122336\n",
      "Epoch 507/1500, Loss: 9.847430370282382\n",
      "Epoch 508/1500, Loss: 9.604194635525346\n",
      "Epoch 509/1500, Loss: 10.367530685849488\n",
      "Epoch 510/1500, Loss: 10.384657174814492\n",
      "Epoch 511/1500, Loss: 10.853498089127243\n",
      "Epoch 512/1500, Loss: 10.033699091058224\n",
      "Epoch 513/1500, Loss: 13.418359628412873\n",
      "Epoch 514/1500, Loss: 12.571621086448431\n",
      "Epoch 515/1500, Loss: 10.417155873030424\n",
      "Epoch 516/1500, Loss: 8.742738411296159\n",
      "Epoch 517/1500, Loss: 9.477309794630855\n",
      "Epoch 518/1500, Loss: 9.76867679366842\n",
      "Epoch 519/1500, Loss: 9.053426703438163\n",
      "Epoch 520/1500, Loss: 9.249279744923115\n",
      "Epoch 521/1500, Loss: 10.14820689568296\n",
      "Epoch 522/1500, Loss: 9.984160752501339\n",
      "Epoch 523/1500, Loss: 11.417644024826586\n",
      "Epoch 524/1500, Loss: 9.962003803811967\n",
      "Epoch 525/1500, Loss: 9.925742290448397\n",
      "Epoch 526/1500, Loss: 9.40003858320415\n",
      "Epoch 527/1500, Loss: 9.026161060668528\n",
      "Epoch 528/1500, Loss: 9.611109692137688\n",
      "Epoch 529/1500, Loss: 10.983514758758247\n",
      "Epoch 530/1500, Loss: 9.856364813633263\n",
      "Epoch 531/1500, Loss: 8.961067425087094\n",
      "Epoch 532/1500, Loss: 8.93258244683966\n",
      "Epoch 533/1500, Loss: 9.047326371073723\n",
      "Epoch 534/1500, Loss: 8.926649738103151\n",
      "Epoch 535/1500, Loss: 9.289702808018774\n",
      "Epoch 536/1500, Loss: 31.28214193554595\n",
      "Epoch 537/1500, Loss: 11.97296152729541\n",
      "Epoch 538/1500, Loss: 10.00175557890907\n",
      "Epoch 539/1500, Loss: 10.239982973784208\n",
      "Epoch 540/1500, Loss: 10.134221075568348\n",
      "Epoch 541/1500, Loss: 9.86793800117448\n",
      "Epoch 542/1500, Loss: 18.69554646825418\n",
      "Epoch 543/1500, Loss: 18.672004380263388\n",
      "Epoch 544/1500, Loss: 12.968278775922954\n",
      "Epoch 545/1500, Loss: 11.374680317938328\n",
      "Epoch 546/1500, Loss: 10.324823077768087\n",
      "Epoch 547/1500, Loss: 10.562538608442992\n",
      "Epoch 548/1500, Loss: 9.171025161864236\n",
      "Epoch 549/1500, Loss: 8.542520788498223\n",
      "Epoch 550/1500, Loss: 8.917886903043836\n",
      "Epoch 551/1500, Loss: 9.22391544166021\n",
      "Epoch 552/1500, Loss: 9.31241933023557\n",
      "Epoch 553/1500, Loss: 9.653736667241901\n",
      "Epoch 554/1500, Loss: 14.398742206860334\n",
      "Epoch 555/1500, Loss: 10.771620790008456\n",
      "Epoch 556/1500, Loss: 9.570641594007611\n",
      "Epoch 557/1500, Loss: 8.823328443104401\n",
      "Epoch 558/1500, Loss: 8.03468674980104\n",
      "Epoch 559/1500, Loss: 9.06750973383896\n",
      "Epoch 560/1500, Loss: 9.655455368105322\n",
      "Epoch 561/1500, Loss: 9.843301578890532\n",
      "Epoch 562/1500, Loss: 9.48284427402541\n",
      "Epoch 563/1500, Loss: 8.629979248158634\n",
      "Epoch 564/1500, Loss: 8.050093507394195\n",
      "Epoch 565/1500, Loss: 9.625450999941677\n",
      "Epoch 566/1500, Loss: 11.294104496017098\n",
      "Epoch 567/1500, Loss: 9.66147025115788\n",
      "Epoch 568/1500, Loss: 9.49971090327017\n",
      "Epoch 569/1500, Loss: 9.624149942304939\n",
      "Epoch 570/1500, Loss: 10.107846729457378\n",
      "Epoch 571/1500, Loss: 9.492776672821492\n",
      "Epoch 572/1500, Loss: 9.680035320110619\n",
      "Epoch 573/1500, Loss: 10.888826209120452\n",
      "Epoch 574/1500, Loss: 10.608990516513586\n",
      "Epoch 575/1500, Loss: 11.609589292667806\n",
      "Epoch 576/1500, Loss: 10.91973969899118\n",
      "Epoch 577/1500, Loss: 11.207070397213101\n",
      "Epoch 578/1500, Loss: 10.144943862222135\n",
      "Epoch 579/1500, Loss: 8.851328832563013\n",
      "Epoch 580/1500, Loss: 9.140521919354796\n",
      "Epoch 581/1500, Loss: 9.351399544626474\n",
      "Epoch 582/1500, Loss: 8.85770116886124\n",
      "Epoch 583/1500, Loss: 8.245614382904023\n",
      "Epoch 584/1500, Loss: 8.599192534573376\n",
      "Epoch 585/1500, Loss: 8.325760038103908\n",
      "Epoch 586/1500, Loss: 8.723642144817859\n",
      "Epoch 587/1500, Loss: 9.505040757358074\n",
      "Epoch 588/1500, Loss: 8.887196545954794\n",
      "Epoch 589/1500, Loss: 10.76707966811955\n",
      "Epoch 590/1500, Loss: 10.174933669622988\n",
      "Epoch 591/1500, Loss: 9.752049167174846\n",
      "Epoch 592/1500, Loss: 9.000037585850805\n",
      "Epoch 593/1500, Loss: 8.779878594912589\n",
      "Epoch 594/1500, Loss: 8.958805101225153\n",
      "Epoch 595/1500, Loss: 8.441955449525267\n",
      "Epoch 596/1500, Loss: 9.00379057181999\n",
      "Epoch 597/1500, Loss: 8.422387731261551\n",
      "Epoch 598/1500, Loss: 9.49707114417106\n",
      "Epoch 599/1500, Loss: 9.538106325548142\n",
      "Epoch 600/1500, Loss: 10.284535823389888\n",
      "Epoch 601/1500, Loss: 9.657786509487778\n",
      "Epoch 602/1500, Loss: 9.791464338079095\n",
      "Epoch 603/1500, Loss: 9.98467602301389\n",
      "Epoch 604/1500, Loss: 9.18981003947556\n",
      "Epoch 605/1500, Loss: 9.693811486009508\n",
      "Epoch 606/1500, Loss: 8.68316133087501\n",
      "Epoch 607/1500, Loss: 8.49537869822234\n",
      "Epoch 608/1500, Loss: 9.063079780898988\n",
      "Epoch 609/1500, Loss: 8.945612507872283\n",
      "Epoch 610/1500, Loss: 10.474790147738531\n",
      "Epoch 611/1500, Loss: 10.77038409980014\n",
      "Epoch 612/1500, Loss: 9.498466620687395\n",
      "Epoch 613/1500, Loss: 8.99395826458931\n",
      "Epoch 614/1500, Loss: 11.13519505225122\n",
      "Epoch 615/1500, Loss: 9.8356081857346\n",
      "Epoch 616/1500, Loss: 8.665060137864202\n",
      "Epoch 617/1500, Loss: 8.018984399503097\n",
      "Epoch 618/1500, Loss: 8.159978522919118\n",
      "Epoch 619/1500, Loss: 8.388245078735054\n",
      "Epoch 620/1500, Loss: 21.774799483828247\n",
      "Epoch 621/1500, Loss: 9.871078321244568\n",
      "Epoch 622/1500, Loss: 8.599503099918365\n",
      "Epoch 623/1500, Loss: 9.211512977257371\n",
      "Epoch 624/1500, Loss: 8.702139330096543\n",
      "Epoch 625/1500, Loss: 8.528564100852236\n",
      "Epoch 626/1500, Loss: 9.502879360225052\n",
      "Epoch 627/1500, Loss: 8.87455632025376\n",
      "Epoch 628/1500, Loss: 9.40835673105903\n",
      "Epoch 629/1500, Loss: 11.115257517434657\n",
      "Epoch 630/1500, Loss: 11.881154699716717\n",
      "Epoch 631/1500, Loss: 9.675326159223914\n",
      "Epoch 632/1500, Loss: 8.95347363082692\n",
      "Epoch 633/1500, Loss: 9.10353760770522\n",
      "Epoch 634/1500, Loss: 9.297788614872843\n",
      "Epoch 635/1500, Loss: 9.07628700695932\n",
      "Epoch 636/1500, Loss: 8.566783377435058\n",
      "Epoch 637/1500, Loss: 8.190706911031157\n",
      "Epoch 638/1500, Loss: 8.00455835647881\n",
      "Epoch 639/1500, Loss: 8.112531524617225\n",
      "Epoch 640/1500, Loss: 9.194525595055893\n",
      "Epoch 641/1500, Loss: 8.712266699410975\n",
      "Epoch 642/1500, Loss: 8.712894522584975\n",
      "Epoch 643/1500, Loss: 8.897377443034202\n",
      "Epoch 644/1500, Loss: 9.404933697544038\n",
      "Epoch 645/1500, Loss: 9.695050504989922\n",
      "Epoch 646/1500, Loss: 9.584280632436275\n",
      "Epoch 647/1500, Loss: 9.289797991514206\n",
      "Epoch 648/1500, Loss: 9.157079760916531\n",
      "Epoch 649/1500, Loss: 9.894193860236555\n",
      "Epoch 650/1500, Loss: 8.967162432149053\n",
      "Epoch 651/1500, Loss: 8.373934452887625\n",
      "Epoch 652/1500, Loss: 8.675199827179313\n",
      "Epoch 653/1500, Loss: 8.289118681568652\n",
      "Epoch 654/1500, Loss: 7.938402669969946\n",
      "Epoch 655/1500, Loss: 8.673614622093737\n",
      "Epoch 656/1500, Loss: 8.879124682862312\n",
      "Epoch 657/1500, Loss: 8.9867810793221\n",
      "Epoch 658/1500, Loss: 9.107878327835351\n",
      "Epoch 659/1500, Loss: 9.281713113654405\n",
      "Epoch 660/1500, Loss: 9.518374697770923\n",
      "Epoch 661/1500, Loss: 8.599028734490275\n",
      "Epoch 662/1500, Loss: 8.762748278211802\n",
      "Epoch 663/1500, Loss: 8.973673895467073\n",
      "Epoch 664/1500, Loss: 8.8112337293569\n",
      "Epoch 665/1500, Loss: 8.242592143360525\n",
      "Epoch 666/1500, Loss: 8.237709545297548\n",
      "Epoch 667/1500, Loss: 8.589408429572359\n",
      "Epoch 668/1500, Loss: 8.093418386764824\n",
      "Epoch 669/1500, Loss: 8.307830224744976\n",
      "Epoch 670/1500, Loss: 9.12409926368855\n",
      "Epoch 671/1500, Loss: 9.511439715977758\n",
      "Epoch 672/1500, Loss: 9.563151018694043\n",
      "Epoch 673/1500, Loss: 9.25447730673477\n",
      "Epoch 674/1500, Loss: 9.102129864040762\n",
      "Epoch 675/1500, Loss: 8.780970901949331\n",
      "Epoch 676/1500, Loss: 8.146554578328505\n",
      "Epoch 677/1500, Loss: 8.144134192494676\n",
      "Epoch 678/1500, Loss: 8.295337011339143\n",
      "Epoch 679/1500, Loss: 7.941717503359541\n",
      "Epoch 680/1500, Loss: 8.074049655348063\n",
      "Epoch 681/1500, Loss: 8.224696036893874\n",
      "Epoch 682/1500, Loss: 9.207152207847685\n",
      "Epoch 683/1500, Loss: 10.006261934991926\n",
      "Epoch 684/1500, Loss: 9.021279744571075\n",
      "Epoch 685/1500, Loss: 8.26157797081396\n",
      "Epoch 686/1500, Loss: 8.10902851098217\n",
      "Epoch 687/1500, Loss: 8.20842121168971\n",
      "Epoch 688/1500, Loss: 7.250115451402962\n",
      "Epoch 689/1500, Loss: 8.070848554838449\n",
      "Epoch 690/1500, Loss: 9.196623178198934\n",
      "Epoch 691/1500, Loss: 9.245818480383605\n",
      "Epoch 692/1500, Loss: 8.746905525680631\n",
      "Epoch 693/1500, Loss: 10.786644545849413\n",
      "Epoch 694/1500, Loss: 9.239852970466018\n",
      "Epoch 695/1500, Loss: 11.949279454536736\n",
      "Epoch 696/1500, Loss: 9.676112196873873\n",
      "Epoch 697/1500, Loss: 8.345783845288679\n",
      "Epoch 698/1500, Loss: 8.677303808741271\n",
      "Epoch 699/1500, Loss: 9.386751341633499\n",
      "Epoch 700/1500, Loss: 8.569167397916317\n",
      "Epoch 701/1500, Loss: 8.400450143963099\n",
      "Epoch 702/1500, Loss: 8.023334504337981\n",
      "Epoch 703/1500, Loss: 8.449481339659542\n",
      "Epoch 704/1500, Loss: 8.64167933864519\n",
      "Epoch 705/1500, Loss: 10.205903912894428\n",
      "Epoch 706/1500, Loss: 10.20862730499357\n",
      "Epoch 707/1500, Loss: 9.939823800697923\n",
      "Epoch 708/1500, Loss: 9.587582012405619\n",
      "Epoch 709/1500, Loss: 8.142772372579202\n",
      "Epoch 710/1500, Loss: 8.999825459672138\n",
      "Epoch 711/1500, Loss: 7.807859471067786\n",
      "Epoch 712/1500, Loss: 9.390535757411271\n",
      "Epoch 713/1500, Loss: 7.591914591146633\n",
      "Epoch 714/1500, Loss: 8.060943687334657\n",
      "Epoch 715/1500, Loss: 7.4895712931174785\n",
      "Epoch 716/1500, Loss: 8.017695728456602\n",
      "Epoch 717/1500, Loss: 8.713925062445924\n",
      "Epoch 718/1500, Loss: 8.799172852188349\n",
      "Epoch 719/1500, Loss: 8.930082044564188\n",
      "Epoch 720/1500, Loss: 8.681999520864338\n",
      "Epoch 721/1500, Loss: 8.75104459747672\n",
      "Epoch 722/1500, Loss: 8.854388434905559\n",
      "Epoch 723/1500, Loss: 7.937230830313638\n",
      "Epoch 724/1500, Loss: 7.989927621558309\n",
      "Epoch 725/1500, Loss: 8.65982325002551\n",
      "Epoch 726/1500, Loss: 8.792014316190034\n",
      "Epoch 727/1500, Loss: 10.019214505329728\n",
      "Epoch 728/1500, Loss: 9.428941737860441\n",
      "Epoch 729/1500, Loss: 10.446809583809227\n",
      "Epoch 730/1500, Loss: 11.406178509816527\n",
      "Epoch 731/1500, Loss: 9.319748205132782\n",
      "Epoch 732/1500, Loss: 7.975223129382357\n",
      "Epoch 733/1500, Loss: 8.2474025641568\n",
      "Epoch 734/1500, Loss: 8.156344467541203\n",
      "Epoch 735/1500, Loss: 8.15801703510806\n",
      "Epoch 736/1500, Loss: 8.945259901229292\n",
      "Epoch 737/1500, Loss: 8.600822330219671\n",
      "Epoch 738/1500, Loss: 8.119133688509464\n",
      "Epoch 739/1500, Loss: 8.617355796974152\n",
      "Epoch 740/1500, Loss: 8.20308127766475\n",
      "Epoch 741/1500, Loss: 8.499803390586749\n",
      "Epoch 742/1500, Loss: 8.571795555762947\n",
      "Epoch 743/1500, Loss: 8.508143624756485\n",
      "Epoch 744/1500, Loss: 8.146419037599117\n",
      "Epoch 745/1500, Loss: 8.416442416608334\n",
      "Epoch 746/1500, Loss: 7.293689347570762\n",
      "Epoch 747/1500, Loss: 7.321659856243059\n",
      "Epoch 748/1500, Loss: 7.558060354087502\n",
      "Epoch 749/1500, Loss: 7.889464831911027\n",
      "Epoch 750/1500, Loss: 10.39602895732969\n",
      "Epoch 751/1500, Loss: 9.994556655175984\n",
      "Epoch 752/1500, Loss: 9.060844311024994\n",
      "Epoch 753/1500, Loss: 9.714804574847221\n",
      "Epoch 754/1500, Loss: 8.748170801671222\n",
      "Epoch 755/1500, Loss: 9.112126136664301\n",
      "Epoch 756/1500, Loss: 8.172628518659621\n",
      "Epoch 757/1500, Loss: 8.154456414515153\n",
      "Epoch 758/1500, Loss: 8.456615812843665\n",
      "Epoch 759/1500, Loss: 7.96515633398667\n",
      "Epoch 760/1500, Loss: 7.280067806132138\n",
      "Epoch 761/1500, Loss: 7.5180882648564875\n",
      "Epoch 762/1500, Loss: 7.96690774592571\n",
      "Epoch 763/1500, Loss: 9.47943046502769\n",
      "Epoch 764/1500, Loss: 11.39562273863703\n",
      "Epoch 765/1500, Loss: 8.307880068430677\n",
      "Epoch 766/1500, Loss: 8.437270724447444\n",
      "Epoch 767/1500, Loss: 7.722453366033733\n",
      "Epoch 768/1500, Loss: 6.864122941857204\n",
      "Epoch 769/1500, Loss: 7.470800427254289\n",
      "Epoch 770/1500, Loss: 7.882446296513081\n",
      "Epoch 771/1500, Loss: 8.619209139375016\n",
      "Epoch 772/1500, Loss: 8.46235667122528\n",
      "Epoch 773/1500, Loss: 7.392038085963577\n",
      "Epoch 774/1500, Loss: 8.182208721991628\n",
      "Epoch 775/1500, Loss: 7.436109133064747\n",
      "Epoch 776/1500, Loss: 9.191469942219555\n",
      "Epoch 777/1500, Loss: 8.554799293866381\n",
      "Epoch 778/1500, Loss: 8.586756980512291\n",
      "Epoch 779/1500, Loss: 8.231582734268159\n",
      "Epoch 780/1500, Loss: 8.80586706311442\n",
      "Epoch 781/1500, Loss: 8.140414894092828\n",
      "Epoch 782/1500, Loss: 7.989811363397166\n",
      "Epoch 783/1500, Loss: 7.470316099002957\n",
      "Epoch 784/1500, Loss: 7.6166425824631006\n",
      "Epoch 785/1500, Loss: 7.749084603739902\n",
      "Epoch 786/1500, Loss: 8.507303616497666\n",
      "Epoch 787/1500, Loss: 8.001106179319322\n",
      "Epoch 788/1500, Loss: 7.961546133970842\n",
      "Epoch 789/1500, Loss: 7.7512201284989715\n",
      "Epoch 790/1500, Loss: 8.162305980920792\n",
      "Epoch 791/1500, Loss: 8.206379184732214\n",
      "Epoch 792/1500, Loss: 8.159440823132172\n",
      "Epoch 793/1500, Loss: 8.755661335773766\n",
      "Epoch 794/1500, Loss: 10.484628786332905\n",
      "Epoch 795/1500, Loss: 10.475166039541364\n",
      "Epoch 796/1500, Loss: 9.197795255109668\n",
      "Epoch 797/1500, Loss: 8.402724187122658\n",
      "Epoch 798/1500, Loss: 8.273934952216223\n",
      "Epoch 799/1500, Loss: 7.1874931862112135\n",
      "Epoch 800/1500, Loss: 7.365248720860109\n",
      "Epoch 801/1500, Loss: 7.164369823876768\n",
      "Epoch 802/1500, Loss: 7.700439189327881\n",
      "Epoch 803/1500, Loss: 7.927920235786587\n",
      "Epoch 804/1500, Loss: 8.750411033863202\n",
      "Epoch 805/1500, Loss: 10.025401850463822\n",
      "Epoch 806/1500, Loss: 8.635295039974153\n",
      "Epoch 807/1500, Loss: 7.873874802840874\n",
      "Epoch 808/1500, Loss: 9.044865497853607\n",
      "Epoch 809/1500, Loss: 8.965937065891922\n",
      "Epoch 810/1500, Loss: 10.801922108978033\n",
      "Epoch 811/1500, Loss: 8.711656894534826\n",
      "Epoch 812/1500, Loss: 8.739530524006113\n",
      "Epoch 813/1500, Loss: 9.023290494922549\n",
      "Epoch 814/1500, Loss: 10.190912764053792\n",
      "Epoch 815/1500, Loss: 9.34084078669548\n",
      "Epoch 816/1500, Loss: 8.019059563521296\n",
      "Epoch 817/1500, Loss: 8.584570973413065\n",
      "Epoch 818/1500, Loss: 8.590414182515815\n",
      "Epoch 819/1500, Loss: 8.068416528403759\n",
      "Epoch 820/1500, Loss: 7.769309451337904\n",
      "Epoch 821/1500, Loss: 7.200329999672249\n",
      "Epoch 822/1500, Loss: 7.315760186407715\n",
      "Epoch 823/1500, Loss: 8.208093171240762\n",
      "Epoch 824/1500, Loss: 8.38856755476445\n",
      "Epoch 825/1500, Loss: 8.296967748785391\n",
      "Epoch 826/1500, Loss: 7.84916827455163\n",
      "Epoch 827/1500, Loss: 7.50626188213937\n",
      "Epoch 828/1500, Loss: 7.606121065095067\n",
      "Epoch 829/1500, Loss: 7.79995058523491\n",
      "Epoch 830/1500, Loss: 7.700375332031399\n",
      "Epoch 831/1500, Loss: 9.063041471410543\n",
      "Epoch 832/1500, Loss: 8.466238031862304\n",
      "Epoch 833/1500, Loss: 7.76152296946384\n",
      "Epoch 834/1500, Loss: 8.196278030052781\n",
      "Epoch 835/1500, Loss: 8.641375749837607\n",
      "Epoch 836/1500, Loss: 7.6325252167880535\n",
      "Epoch 837/1500, Loss: 7.257775017526001\n",
      "Epoch 838/1500, Loss: 7.724246725905687\n",
      "Epoch 839/1500, Loss: 7.847980039194226\n",
      "Epoch 840/1500, Loss: 7.983036324614659\n",
      "Epoch 841/1500, Loss: 8.041774184443057\n",
      "Epoch 842/1500, Loss: 8.692175490316004\n",
      "Epoch 843/1500, Loss: 9.481753513216972\n",
      "Epoch 844/1500, Loss: 7.956990145146847\n",
      "Epoch 845/1500, Loss: 7.782510852906853\n",
      "Epoch 846/1500, Loss: 8.143267328385264\n",
      "Epoch 847/1500, Loss: 7.574297959450632\n",
      "Epoch 848/1500, Loss: 7.208897039759904\n",
      "Epoch 849/1500, Loss: 7.054740317631513\n",
      "Epoch 850/1500, Loss: 8.368112345924601\n",
      "Epoch 851/1500, Loss: 7.225367689039558\n",
      "Epoch 852/1500, Loss: 7.30369226471521\n",
      "Epoch 853/1500, Loss: 7.559041919419542\n",
      "Epoch 854/1500, Loss: 8.70212861103937\n",
      "Epoch 855/1500, Loss: 8.035368269775063\n",
      "Epoch 856/1500, Loss: 8.4524716346059\n",
      "Epoch 857/1500, Loss: 7.967974642757326\n",
      "Epoch 858/1500, Loss: 7.245546600781381\n",
      "Epoch 859/1500, Loss: 7.373523143818602\n",
      "Epoch 860/1500, Loss: 8.36409047502093\n",
      "Epoch 861/1500, Loss: 8.041773641481996\n",
      "Epoch 862/1500, Loss: 8.239235088927671\n",
      "Epoch 863/1500, Loss: 9.321928745135665\n",
      "Epoch 864/1500, Loss: 8.31186074949801\n",
      "Epoch 865/1500, Loss: 8.368637580890208\n",
      "Epoch 866/1500, Loss: 7.876215752679855\n",
      "Epoch 867/1500, Loss: 7.139614213258028\n",
      "Epoch 868/1500, Loss: 6.874395122518763\n",
      "Epoch 869/1500, Loss: 6.6624594130553305\n",
      "Epoch 870/1500, Loss: 7.175621273927391\n",
      "Epoch 871/1500, Loss: 8.306935243308544\n",
      "Epoch 872/1500, Loss: 7.922450829762965\n",
      "Epoch 873/1500, Loss: 7.369344366947189\n",
      "Epoch 874/1500, Loss: 7.942549854516983\n",
      "Epoch 875/1500, Loss: 7.443733303109184\n",
      "Epoch 876/1500, Loss: 11.350485337432474\n",
      "Epoch 877/1500, Loss: 10.745269988663495\n",
      "Epoch 878/1500, Loss: 9.03645687410608\n",
      "Epoch 879/1500, Loss: 8.189460604917258\n",
      "Epoch 880/1500, Loss: 7.781060326611623\n",
      "Epoch 881/1500, Loss: 7.7028601539786905\n",
      "Epoch 882/1500, Loss: 7.952571796486154\n",
      "Epoch 883/1500, Loss: 7.710028638364747\n",
      "Epoch 884/1500, Loss: 8.046531581319869\n",
      "Epoch 885/1500, Loss: 8.117929085157812\n",
      "Epoch 886/1500, Loss: 7.22142482874915\n",
      "Epoch 887/1500, Loss: 7.640556770842522\n",
      "Epoch 888/1500, Loss: 8.923113135620952\n",
      "Epoch 889/1500, Loss: 8.07566087320447\n",
      "Epoch 890/1500, Loss: 9.733661813661456\n",
      "Epoch 891/1500, Loss: 7.969111023005098\n",
      "Epoch 892/1500, Loss: 7.318161715986207\n",
      "Epoch 893/1500, Loss: 7.050325880758464\n",
      "Epoch 894/1500, Loss: 7.250852861441672\n",
      "Epoch 895/1500, Loss: 7.2687519991304725\n",
      "Epoch 896/1500, Loss: 8.146549647208303\n",
      "Epoch 897/1500, Loss: 8.27858209144324\n",
      "Epoch 898/1500, Loss: 8.29824364790693\n",
      "Epoch 899/1500, Loss: 8.64795536431484\n",
      "Epoch 900/1500, Loss: 8.141232391120866\n",
      "Epoch 901/1500, Loss: 9.65993294911459\n",
      "Epoch 902/1500, Loss: 7.522455511149019\n",
      "Epoch 903/1500, Loss: 9.689397156704217\n",
      "Epoch 904/1500, Loss: 9.599226455204189\n",
      "Epoch 905/1500, Loss: 8.22112575150095\n",
      "Epoch 906/1500, Loss: 6.67506739590317\n",
      "Epoch 907/1500, Loss: 6.38758793589659\n",
      "Epoch 908/1500, Loss: 7.406106271548197\n",
      "Epoch 909/1500, Loss: 7.911305231275037\n",
      "Epoch 910/1500, Loss: 7.863932440057397\n",
      "Epoch 911/1500, Loss: 6.960757669061422\n",
      "Epoch 912/1500, Loss: 8.03329257760197\n",
      "Epoch 913/1500, Loss: 7.577834298601374\n",
      "Epoch 914/1500, Loss: 7.4014469012618065\n",
      "Epoch 915/1500, Loss: 8.720425016712397\n",
      "Epoch 916/1500, Loss: 7.885657541221008\n",
      "Epoch 917/1500, Loss: 7.123718299437314\n",
      "Epoch 918/1500, Loss: 7.553643523948267\n",
      "Epoch 919/1500, Loss: 7.354405698599294\n",
      "Epoch 920/1500, Loss: 7.516276404261589\n",
      "Epoch 921/1500, Loss: 7.317304591299035\n",
      "Epoch 922/1500, Loss: 8.296865845564753\n",
      "Epoch 923/1500, Loss: 8.71443046675995\n",
      "Epoch 924/1500, Loss: 8.13432337064296\n",
      "Epoch 925/1500, Loss: 8.226825064513832\n",
      "Epoch 926/1500, Loss: 7.603316274937242\n",
      "Epoch 927/1500, Loss: 7.614833934232593\n",
      "Epoch 928/1500, Loss: 8.090249794069678\n",
      "Epoch 929/1500, Loss: 7.399632050655782\n",
      "Epoch 930/1500, Loss: 8.033288552891463\n",
      "Epoch 931/1500, Loss: 8.718181394506246\n",
      "Epoch 932/1500, Loss: 7.605702105443925\n",
      "Epoch 933/1500, Loss: 8.812092447187752\n",
      "Epoch 934/1500, Loss: 7.9199296166189015\n",
      "Epoch 935/1500, Loss: 7.903021888574585\n",
      "Epoch 936/1500, Loss: 6.758597422391176\n",
      "Epoch 937/1500, Loss: 7.124404163798317\n",
      "Epoch 938/1500, Loss: 7.450060711475089\n",
      "Epoch 939/1500, Loss: 8.679997806902975\n",
      "Epoch 940/1500, Loss: 7.60759236337617\n",
      "Epoch 941/1500, Loss: 7.402042671805248\n",
      "Epoch 942/1500, Loss: 6.841910304035991\n",
      "Epoch 943/1500, Loss: 7.418691952712834\n",
      "Epoch 944/1500, Loss: 7.430477254092693\n",
      "Epoch 945/1500, Loss: 7.582768993917853\n",
      "Epoch 946/1500, Loss: 8.12170982756652\n",
      "Epoch 947/1500, Loss: 8.061370510840788\n",
      "Epoch 948/1500, Loss: 7.573017691960558\n",
      "Epoch 949/1500, Loss: 8.023166470229626\n",
      "Epoch 950/1500, Loss: 8.465241889934987\n",
      "Epoch 951/1500, Loss: 13.867455143481493\n",
      "Epoch 952/1500, Loss: 9.293927849968895\n",
      "Epoch 953/1500, Loss: 7.771127617452294\n",
      "Epoch 954/1500, Loss: 6.951872137608007\n",
      "Epoch 955/1500, Loss: 6.362230055266991\n",
      "Epoch 956/1500, Loss: 6.651033787056804\n",
      "Epoch 957/1500, Loss: 7.162947870558128\n",
      "Epoch 958/1500, Loss: 7.46717511350289\n",
      "Epoch 959/1500, Loss: 7.0875466510187835\n",
      "Epoch 960/1500, Loss: 7.214675043243915\n",
      "Epoch 961/1500, Loss: 7.518234331160784\n",
      "Epoch 962/1500, Loss: 7.87035861890763\n",
      "Epoch 963/1500, Loss: 7.865066290600225\n",
      "Epoch 964/1500, Loss: 7.55904951132834\n",
      "Epoch 965/1500, Loss: 8.62188819097355\n",
      "Epoch 966/1500, Loss: 8.635122166015208\n",
      "Epoch 967/1500, Loss: 8.121614751638845\n",
      "Epoch 968/1500, Loss: 7.42652950133197\n",
      "Epoch 969/1500, Loss: 7.681508903391659\n",
      "Epoch 970/1500, Loss: 7.431311886291951\n",
      "Epoch 971/1500, Loss: 6.946384459268302\n",
      "Epoch 972/1500, Loss: 6.773326326627284\n",
      "Epoch 973/1500, Loss: 6.697757293004543\n",
      "Epoch 974/1500, Loss: 8.27089204848744\n",
      "Epoch 975/1500, Loss: 7.062919684220105\n",
      "Epoch 976/1500, Loss: 7.491467904066667\n",
      "Epoch 977/1500, Loss: 7.509820889448747\n",
      "Epoch 978/1500, Loss: 7.240014950511977\n",
      "Epoch 979/1500, Loss: 7.009705198695883\n",
      "Epoch 980/1500, Loss: 7.3406802045647055\n",
      "Epoch 981/1500, Loss: 8.319476006319746\n",
      "Epoch 982/1500, Loss: 8.384351112879813\n",
      "Epoch 983/1500, Loss: 8.377211493439972\n",
      "Epoch 984/1500, Loss: 7.404420793987811\n",
      "Epoch 985/1500, Loss: 7.621065751183778\n",
      "Epoch 986/1500, Loss: 7.687878160504624\n",
      "Epoch 987/1500, Loss: 7.891378849511966\n",
      "Epoch 988/1500, Loss: 7.8275464156176895\n",
      "Epoch 989/1500, Loss: 8.933364546159282\n",
      "Epoch 990/1500, Loss: 8.882437987718731\n",
      "Epoch 991/1500, Loss: 7.401131224818528\n",
      "Epoch 992/1500, Loss: 7.980265272781253\n",
      "Epoch 993/1500, Loss: 7.456564945867285\n",
      "Epoch 994/1500, Loss: 6.583992847474292\n",
      "Epoch 995/1500, Loss: 7.08743177074939\n",
      "Epoch 996/1500, Loss: 7.01622624299489\n",
      "Epoch 997/1500, Loss: 8.084536425303668\n",
      "Epoch 998/1500, Loss: 10.11865114280954\n",
      "Epoch 999/1500, Loss: 9.286249521188438\n",
      "Epoch 1000/1500, Loss: 8.023871403187513\n",
      "Epoch 1001/1500, Loss: 7.537715863902122\n",
      "Epoch 1002/1500, Loss: 6.745836340589449\n",
      "Epoch 1003/1500, Loss: 7.453680625418201\n",
      "Epoch 1004/1500, Loss: 6.971754307858646\n",
      "Epoch 1005/1500, Loss: 6.688918600673787\n",
      "Epoch 1006/1500, Loss: 6.226747799664736\n",
      "Epoch 1007/1500, Loss: 7.9600482631940395\n",
      "Epoch 1008/1500, Loss: 7.871632536873221\n",
      "Epoch 1009/1500, Loss: 7.149830318754539\n",
      "Epoch 1010/1500, Loss: 8.195097106974572\n",
      "Epoch 1011/1500, Loss: 6.999901915434748\n",
      "Epoch 1012/1500, Loss: 7.2607401316054165\n",
      "Epoch 1013/1500, Loss: 8.024369815830141\n",
      "Epoch 1014/1500, Loss: 7.5711246754508466\n",
      "Epoch 1015/1500, Loss: 7.073498698649928\n",
      "Epoch 1016/1500, Loss: 7.024765745969489\n",
      "Epoch 1017/1500, Loss: 7.200602168450132\n",
      "Epoch 1018/1500, Loss: 7.749461570754647\n",
      "Epoch 1019/1500, Loss: 7.518380734138191\n",
      "Epoch 1020/1500, Loss: 6.965757653582841\n",
      "Epoch 1021/1500, Loss: 7.456528775859624\n",
      "Epoch 1022/1500, Loss: 30.87629702920094\n",
      "Epoch 1023/1500, Loss: 9.824489039834589\n",
      "Epoch 1024/1500, Loss: 8.807392782298848\n",
      "Epoch 1025/1500, Loss: 7.469148545525968\n",
      "Epoch 1026/1500, Loss: 7.651261621387675\n",
      "Epoch 1027/1500, Loss: 7.119782263180241\n",
      "Epoch 1028/1500, Loss: 6.923098978353664\n",
      "Epoch 1029/1500, Loss: 6.851507375948131\n",
      "Epoch 1030/1500, Loss: 6.928964275633916\n",
      "Epoch 1031/1500, Loss: 7.116977418889292\n",
      "Epoch 1032/1500, Loss: 6.9335972256958485\n",
      "Epoch 1033/1500, Loss: 6.877847352065146\n",
      "Epoch 1034/1500, Loss: 6.804921901319176\n",
      "Epoch 1035/1500, Loss: 7.018046205863357\n",
      "Epoch 1036/1500, Loss: 6.900383859407157\n",
      "Epoch 1037/1500, Loss: 6.811822487972677\n",
      "Epoch 1038/1500, Loss: 7.220728057669476\n",
      "Epoch 1039/1500, Loss: 7.352238629478961\n",
      "Epoch 1040/1500, Loss: 8.046729616820812\n",
      "Epoch 1041/1500, Loss: 8.160979571053758\n",
      "Epoch 1042/1500, Loss: 7.9290834094863385\n",
      "Epoch 1043/1500, Loss: 7.1235308416653425\n",
      "Epoch 1044/1500, Loss: 7.487690781475976\n",
      "Epoch 1045/1500, Loss: 7.640028183348477\n",
      "Epoch 1046/1500, Loss: 8.082331512589008\n",
      "Epoch 1047/1500, Loss: 8.265848472947255\n",
      "Epoch 1048/1500, Loss: 8.652802181430161\n",
      "Epoch 1049/1500, Loss: 7.17717008292675\n",
      "Epoch 1050/1500, Loss: 7.202517403522506\n",
      "Epoch 1051/1500, Loss: 6.70708052162081\n",
      "Epoch 1052/1500, Loss: 7.721655077300966\n",
      "Epoch 1053/1500, Loss: 7.000619539059699\n",
      "Epoch 1054/1500, Loss: 6.998788473196328\n",
      "Epoch 1055/1500, Loss: 7.070728627266362\n",
      "Epoch 1056/1500, Loss: 7.879857499618083\n",
      "Epoch 1057/1500, Loss: 7.893625232391059\n",
      "Epoch 1058/1500, Loss: 7.759338988224044\n",
      "Epoch 1059/1500, Loss: 7.909053772920743\n",
      "Epoch 1060/1500, Loss: 7.481170646380633\n",
      "Epoch 1061/1500, Loss: 7.064351185690612\n",
      "Epoch 1062/1500, Loss: 6.674457061337307\n",
      "Epoch 1063/1500, Loss: 6.529201799072325\n",
      "Epoch 1064/1500, Loss: 8.540131884627044\n",
      "Epoch 1065/1500, Loss: 7.5124383920338005\n",
      "Epoch 1066/1500, Loss: 7.479359182761982\n",
      "Epoch 1067/1500, Loss: 6.976050378521904\n",
      "Epoch 1068/1500, Loss: 7.152169140288606\n",
      "Epoch 1069/1500, Loss: 6.94701536372304\n",
      "Epoch 1070/1500, Loss: 8.393903763499111\n",
      "Epoch 1071/1500, Loss: 7.263872403651476\n",
      "Epoch 1072/1500, Loss: 6.932169027859345\n",
      "Epoch 1073/1500, Loss: 7.0753343985415995\n",
      "Epoch 1074/1500, Loss: 7.1837609896901995\n",
      "Epoch 1075/1500, Loss: 7.879737104289234\n",
      "Epoch 1076/1500, Loss: 7.393222952727228\n",
      "Epoch 1077/1500, Loss: 8.386136075016111\n",
      "Epoch 1078/1500, Loss: 7.697861480759457\n",
      "Epoch 1079/1500, Loss: 6.7277285368181765\n",
      "Epoch 1080/1500, Loss: 7.396253020036966\n",
      "Epoch 1081/1500, Loss: 7.109931462211534\n",
      "Epoch 1082/1500, Loss: 6.920071460772306\n",
      "Epoch 1083/1500, Loss: 6.20785970822908\n",
      "Epoch 1084/1500, Loss: 6.624624795978889\n",
      "Epoch 1085/1500, Loss: 7.455026963027194\n",
      "Epoch 1086/1500, Loss: 8.760751536348835\n",
      "Epoch 1087/1500, Loss: 8.082950835581869\n",
      "Epoch 1088/1500, Loss: 7.411978811491281\n",
      "Epoch 1089/1500, Loss: 6.596152798039839\n",
      "Epoch 1090/1500, Loss: 6.916314176982269\n",
      "Epoch 1091/1500, Loss: 6.585195401450619\n",
      "Epoch 1092/1500, Loss: 7.03245203755796\n",
      "Epoch 1093/1500, Loss: 8.927390071097761\n",
      "Epoch 1094/1500, Loss: 7.54130701511167\n",
      "Epoch 1095/1500, Loss: 6.757986235665157\n",
      "Epoch 1096/1500, Loss: 7.024759937543422\n",
      "Epoch 1097/1500, Loss: 7.397778291488066\n",
      "Epoch 1098/1500, Loss: 7.091632443247363\n",
      "Epoch 1099/1500, Loss: 7.097731109708548\n",
      "Epoch 1100/1500, Loss: 7.107912355102599\n",
      "Epoch 1101/1500, Loss: 7.439611588837579\n",
      "Epoch 1102/1500, Loss: 7.7029626108706\n",
      "Epoch 1103/1500, Loss: 7.768055158434436\n",
      "Epoch 1104/1500, Loss: 7.382222269428894\n",
      "Epoch 1105/1500, Loss: 6.896645578788593\n",
      "Epoch 1106/1500, Loss: 6.6338734473101795\n",
      "Epoch 1107/1500, Loss: 7.115751371020451\n",
      "Epoch 1108/1500, Loss: 6.849313494050875\n",
      "Epoch 1109/1500, Loss: 7.424035530537367\n",
      "Epoch 1110/1500, Loss: 7.067895184038207\n",
      "Epoch 1111/1500, Loss: 7.916787143796682\n",
      "Epoch 1112/1500, Loss: 7.633959253784269\n",
      "Epoch 1113/1500, Loss: 7.408896672306582\n",
      "Epoch 1114/1500, Loss: 7.237198046874255\n",
      "Epoch 1115/1500, Loss: 6.971278329147026\n",
      "Epoch 1116/1500, Loss: 7.159761824645102\n",
      "Epoch 1117/1500, Loss: 8.615314387017861\n",
      "Epoch 1118/1500, Loss: 7.042736670468003\n",
      "Epoch 1119/1500, Loss: 6.317631304264069\n",
      "Epoch 1120/1500, Loss: 6.800421722698957\n",
      "Epoch 1121/1500, Loss: 8.484121716814116\n",
      "Epoch 1122/1500, Loss: 8.623365902109072\n",
      "Epoch 1123/1500, Loss: 7.074432182125747\n",
      "Epoch 1124/1500, Loss: 7.706128700636327\n",
      "Epoch 1125/1500, Loss: 8.27068110043183\n",
      "Epoch 1126/1500, Loss: 6.580073958495632\n",
      "Epoch 1127/1500, Loss: 6.56675702217035\n",
      "Epoch 1128/1500, Loss: 6.918041484197602\n",
      "Epoch 1129/1500, Loss: 6.725408683996648\n",
      "Epoch 1130/1500, Loss: 6.700766008347273\n",
      "Epoch 1131/1500, Loss: 7.210480245761573\n",
      "Epoch 1132/1500, Loss: 7.0057888340670615\n",
      "Epoch 1133/1500, Loss: 7.096521191764623\n",
      "Epoch 1134/1500, Loss: 8.059147940017283\n",
      "Epoch 1135/1500, Loss: 7.430775285931304\n",
      "Epoch 1136/1500, Loss: 6.559585445560515\n",
      "Epoch 1137/1500, Loss: 7.275090656476095\n",
      "Epoch 1138/1500, Loss: 6.882603436009958\n",
      "Epoch 1139/1500, Loss: 6.354459398658946\n",
      "Epoch 1140/1500, Loss: 6.902218459406868\n",
      "Epoch 1141/1500, Loss: 6.675908650737256\n",
      "Epoch 1142/1500, Loss: 7.030528512084857\n",
      "Epoch 1143/1500, Loss: 7.3801869777962565\n",
      "Epoch 1144/1500, Loss: 8.125219468027353\n",
      "Epoch 1145/1500, Loss: 8.504713964182884\n",
      "Epoch 1146/1500, Loss: 8.190315872896463\n",
      "Epoch 1147/1500, Loss: 6.883847678313032\n",
      "Epoch 1148/1500, Loss: 6.675557117210701\n",
      "Epoch 1149/1500, Loss: 7.76524755731225\n",
      "Epoch 1150/1500, Loss: 7.601851920597255\n",
      "Epoch 1151/1500, Loss: 7.066099754534662\n",
      "Epoch 1152/1500, Loss: 7.399994569597766\n",
      "Epoch 1153/1500, Loss: 6.381324747111648\n",
      "Epoch 1154/1500, Loss: 6.198988222167827\n",
      "Epoch 1155/1500, Loss: 6.040520744165406\n",
      "Epoch 1156/1500, Loss: 6.700984908035025\n",
      "Epoch 1157/1500, Loss: 7.109943529823795\n",
      "Epoch 1158/1500, Loss: 6.616300356574357\n",
      "Epoch 1159/1500, Loss: 7.2930695412214845\n",
      "Epoch 1160/1500, Loss: 6.799124255310744\n",
      "Epoch 1161/1500, Loss: 6.763833153760061\n",
      "Epoch 1162/1500, Loss: 8.035879866220057\n",
      "Epoch 1163/1500, Loss: 8.13118544453755\n",
      "Epoch 1164/1500, Loss: 7.742061915574595\n",
      "Epoch 1165/1500, Loss: 7.960038892226294\n",
      "Epoch 1166/1500, Loss: 7.0182640054263175\n",
      "Epoch 1167/1500, Loss: 7.213104402646422\n",
      "Epoch 1168/1500, Loss: 6.83209682861343\n",
      "Epoch 1169/1500, Loss: 6.398528932710178\n",
      "Epoch 1170/1500, Loss: 6.278554523829371\n",
      "Epoch 1171/1500, Loss: 6.686866386095062\n",
      "Epoch 1172/1500, Loss: 7.469310258049518\n",
      "Epoch 1173/1500, Loss: 7.186697605066001\n",
      "Epoch 1174/1500, Loss: 6.829689794452861\n",
      "Epoch 1175/1500, Loss: 6.742569223744795\n",
      "Epoch 1176/1500, Loss: 6.752435554983094\n",
      "Epoch 1177/1500, Loss: 7.303002185886726\n",
      "Epoch 1178/1500, Loss: 6.942570780636743\n",
      "Epoch 1179/1500, Loss: 8.970105561427772\n",
      "Epoch 1180/1500, Loss: 7.97303511807695\n",
      "Epoch 1181/1500, Loss: 7.712465523276478\n",
      "Epoch 1182/1500, Loss: 7.046560344286263\n",
      "Epoch 1183/1500, Loss: 6.611697997432202\n",
      "Epoch 1184/1500, Loss: 8.478964952286333\n",
      "Epoch 1185/1500, Loss: 7.056974072707817\n",
      "Epoch 1186/1500, Loss: 6.999962487025186\n",
      "Epoch 1187/1500, Loss: 6.4163575624115765\n",
      "Epoch 1188/1500, Loss: 8.468299182830378\n",
      "Epoch 1189/1500, Loss: 8.529786148574203\n",
      "Epoch 1190/1500, Loss: 6.92966856341809\n",
      "Epoch 1191/1500, Loss: 6.176778701832518\n",
      "Epoch 1192/1500, Loss: 7.085675215115771\n",
      "Epoch 1193/1500, Loss: 7.214841489680111\n",
      "Epoch 1194/1500, Loss: 7.383050802629441\n",
      "Epoch 1195/1500, Loss: 6.197386920917779\n",
      "Epoch 1196/1500, Loss: 7.4343042459804565\n",
      "Epoch 1197/1500, Loss: 6.486162697663531\n",
      "Epoch 1198/1500, Loss: 7.368275861255825\n",
      "Epoch 1199/1500, Loss: 8.01371846254915\n",
      "Epoch 1200/1500, Loss: 8.250747831305489\n",
      "Epoch 1201/1500, Loss: 6.952957136556506\n",
      "Epoch 1202/1500, Loss: 6.500141359400004\n",
      "Epoch 1203/1500, Loss: 6.9291293795686215\n",
      "Epoch 1204/1500, Loss: 7.656638321932405\n",
      "Epoch 1205/1500, Loss: 7.816627888008952\n",
      "Epoch 1206/1500, Loss: 7.025619448162615\n",
      "Epoch 1207/1500, Loss: 7.88334286538884\n",
      "Epoch 1208/1500, Loss: 6.68066567950882\n",
      "Epoch 1209/1500, Loss: 6.81058103707619\n",
      "Epoch 1210/1500, Loss: 6.466806422453374\n",
      "Epoch 1211/1500, Loss: 6.581829658476636\n",
      "Epoch 1212/1500, Loss: 7.114180139964446\n",
      "Epoch 1213/1500, Loss: 8.331368903396651\n",
      "Epoch 1214/1500, Loss: 7.701561615802348\n",
      "Epoch 1215/1500, Loss: 7.014494428876787\n",
      "Epoch 1216/1500, Loss: 7.568844095570967\n",
      "Epoch 1217/1500, Loss: 7.523712869500741\n",
      "Epoch 1218/1500, Loss: 7.167109201662242\n",
      "Epoch 1219/1500, Loss: 6.458882744424045\n",
      "Epoch 1220/1500, Loss: 7.121089313644916\n",
      "Epoch 1221/1500, Loss: 6.536037498619407\n",
      "Epoch 1222/1500, Loss: 7.531017536530271\n",
      "Epoch 1223/1500, Loss: 9.48264839593321\n",
      "Epoch 1224/1500, Loss: 7.651100091636181\n",
      "Epoch 1225/1500, Loss: 6.515061346581206\n",
      "Epoch 1226/1500, Loss: 6.368558540940285\n",
      "Epoch 1227/1500, Loss: 6.3700498576508835\n",
      "Epoch 1228/1500, Loss: 6.1259603928774595\n",
      "Epoch 1229/1500, Loss: 7.735440149437636\n",
      "Epoch 1230/1500, Loss: 6.9232663796283305\n",
      "Epoch 1231/1500, Loss: 6.979232483776286\n",
      "Epoch 1232/1500, Loss: 6.516215142328292\n",
      "Epoch 1233/1500, Loss: 6.525483122095466\n",
      "Epoch 1234/1500, Loss: 6.184994226554409\n",
      "Epoch 1235/1500, Loss: 6.925078179687262\n",
      "Epoch 1236/1500, Loss: 6.930232037557289\n",
      "Epoch 1237/1500, Loss: 7.714242685819045\n",
      "Epoch 1238/1500, Loss: 7.946975329425186\n",
      "Epoch 1239/1500, Loss: 7.5560865150764585\n",
      "Epoch 1240/1500, Loss: 7.083218550309539\n",
      "Epoch 1241/1500, Loss: 6.74138994095847\n",
      "Epoch 1242/1500, Loss: 8.035429863724858\n",
      "Epoch 1243/1500, Loss: 7.1333338127005845\n",
      "Epoch 1244/1500, Loss: 8.327609615633264\n",
      "Epoch 1245/1500, Loss: 7.045045694336295\n",
      "Epoch 1246/1500, Loss: 10.402250783750787\n",
      "Epoch 1247/1500, Loss: 7.629879620624706\n",
      "Epoch 1248/1500, Loss: 7.229396641720086\n",
      "Epoch 1249/1500, Loss: 6.644970371387899\n",
      "Epoch 1250/1500, Loss: 6.528440451365896\n",
      "Epoch 1251/1500, Loss: 8.20760665065609\n",
      "Epoch 1252/1500, Loss: 7.970987147884443\n",
      "Epoch 1253/1500, Loss: 7.448391601443291\n",
      "Epoch 1254/1500, Loss: 8.433272570837289\n",
      "Epoch 1255/1500, Loss: 7.729847612790763\n",
      "Epoch 1256/1500, Loss: 6.762140477541834\n",
      "Epoch 1257/1500, Loss: 7.42166344774887\n",
      "Epoch 1258/1500, Loss: 7.151518395869061\n",
      "Epoch 1259/1500, Loss: 7.218048233771697\n",
      "Epoch 1260/1500, Loss: 7.283981705782935\n",
      "Epoch 1261/1500, Loss: 7.352524955756962\n",
      "Epoch 1262/1500, Loss: 7.028047961648554\n",
      "Epoch 1263/1500, Loss: 8.195479243062437\n",
      "Epoch 1264/1500, Loss: 7.300294705666602\n",
      "Epoch 1265/1500, Loss: 6.998785314150155\n",
      "Epoch 1266/1500, Loss: 7.226552804931998\n",
      "Epoch 1267/1500, Loss: 7.999692931305617\n",
      "Epoch 1268/1500, Loss: 7.53656715946272\n",
      "Epoch 1269/1500, Loss: 7.635896963998675\n",
      "Epoch 1270/1500, Loss: 6.992414075648412\n",
      "Epoch 1271/1500, Loss: 7.886884733801708\n",
      "Epoch 1272/1500, Loss: 6.994133884552866\n",
      "Epoch 1273/1500, Loss: 7.892197945155203\n",
      "Epoch 1274/1500, Loss: 8.990759145002812\n",
      "Epoch 1275/1500, Loss: 8.621021806495264\n",
      "Epoch 1276/1500, Loss: 6.488207045011222\n",
      "Epoch 1277/1500, Loss: 6.038370446534827\n",
      "Epoch 1278/1500, Loss: 6.194070050725713\n",
      "Epoch 1279/1500, Loss: 6.316972367698327\n",
      "Epoch 1280/1500, Loss: 7.3182764942757785\n",
      "Epoch 1281/1500, Loss: 6.538249756209552\n",
      "Epoch 1282/1500, Loss: 6.75391247170046\n",
      "Epoch 1283/1500, Loss: 6.256043959176168\n",
      "Epoch 1284/1500, Loss: 7.324414354283363\n",
      "Epoch 1285/1500, Loss: 7.302193952025846\n",
      "Epoch 1286/1500, Loss: 7.076243251794949\n",
      "Epoch 1287/1500, Loss: 6.717050813836977\n",
      "Epoch 1288/1500, Loss: 6.57655630283989\n",
      "Epoch 1289/1500, Loss: 6.791297264862806\n",
      "Epoch 1290/1500, Loss: 7.293969247490168\n",
      "Epoch 1291/1500, Loss: 6.3642831111792475\n",
      "Epoch 1292/1500, Loss: 6.814812057418749\n",
      "Epoch 1293/1500, Loss: 6.672433509957045\n",
      "Epoch 1294/1500, Loss: 7.01111182034947\n",
      "Epoch 1295/1500, Loss: 6.690907481126487\n",
      "Epoch 1296/1500, Loss: 6.787939277244732\n",
      "Epoch 1297/1500, Loss: 7.5494013163261116\n",
      "Epoch 1298/1500, Loss: 6.95083894720301\n",
      "Epoch 1299/1500, Loss: 6.518086635856889\n",
      "Epoch 1300/1500, Loss: 7.19032914377749\n",
      "Epoch 1301/1500, Loss: 7.364938776940107\n",
      "Epoch 1302/1500, Loss: 6.863929274491966\n",
      "Epoch 1303/1500, Loss: 7.121883672429249\n",
      "Epoch 1304/1500, Loss: 7.428038701647893\n",
      "Epoch 1305/1500, Loss: 6.901358142262325\n",
      "Epoch 1306/1500, Loss: 7.8800419410690665\n",
      "Epoch 1307/1500, Loss: 6.678150741150603\n",
      "Epoch 1308/1500, Loss: 6.817205430706963\n",
      "Epoch 1309/1500, Loss: 6.674421210307628\n",
      "Epoch 1310/1500, Loss: 6.387124703032896\n",
      "Epoch 1311/1500, Loss: 6.627749762963504\n",
      "Epoch 1312/1500, Loss: 7.199403313919902\n",
      "Epoch 1313/1500, Loss: 7.841660380130634\n",
      "Epoch 1314/1500, Loss: 7.092067140387371\n",
      "Epoch 1315/1500, Loss: 6.243825799552724\n",
      "Epoch 1316/1500, Loss: 7.137366945622489\n",
      "Epoch 1317/1500, Loss: 6.43025353574194\n",
      "Epoch 1318/1500, Loss: 6.617170468205586\n",
      "Epoch 1319/1500, Loss: 6.884300349280238\n",
      "Epoch 1320/1500, Loss: 6.743150108726695\n",
      "Epoch 1321/1500, Loss: 6.119482122478075\n",
      "Epoch 1322/1500, Loss: 6.53925865306519\n",
      "Epoch 1323/1500, Loss: 6.032268489710987\n",
      "Epoch 1324/1500, Loss: 7.4305683865677565\n",
      "Epoch 1325/1500, Loss: 7.391023093368858\n",
      "Epoch 1326/1500, Loss: 8.127213695086539\n",
      "Epoch 1327/1500, Loss: 6.5482619339600205\n",
      "Epoch 1328/1500, Loss: 7.415003010304645\n",
      "Epoch 1329/1500, Loss: 6.793602207675576\n",
      "Epoch 1330/1500, Loss: 8.15339081408456\n",
      "Epoch 1331/1500, Loss: 8.398483444470912\n",
      "Epoch 1332/1500, Loss: 8.234173000790179\n",
      "Epoch 1333/1500, Loss: 7.778621859382838\n",
      "Epoch 1334/1500, Loss: 6.512303392635658\n",
      "Epoch 1335/1500, Loss: 6.2012227366212755\n",
      "Epoch 1336/1500, Loss: 6.203839934663847\n",
      "Epoch 1337/1500, Loss: 6.49727564281784\n",
      "Epoch 1338/1500, Loss: 6.361949637066573\n",
      "Epoch 1339/1500, Loss: 6.56735239806585\n",
      "Epoch 1340/1500, Loss: 7.779545207042247\n",
      "Epoch 1341/1500, Loss: 6.767130346735939\n",
      "Epoch 1342/1500, Loss: 6.540796163259074\n",
      "Epoch 1343/1500, Loss: 6.840122167021036\n",
      "Epoch 1344/1500, Loss: 7.490621917648241\n",
      "Epoch 1345/1500, Loss: 6.7271700070705265\n",
      "Epoch 1346/1500, Loss: 6.40165603812784\n",
      "Epoch 1347/1500, Loss: 6.710128827020526\n",
      "Epoch 1348/1500, Loss: 6.818780807079747\n",
      "Epoch 1349/1500, Loss: 6.684360075742006\n",
      "Epoch 1350/1500, Loss: 6.774754784302786\n",
      "Epoch 1351/1500, Loss: 7.025307526462711\n",
      "Epoch 1352/1500, Loss: 6.260000061476603\n",
      "Epoch 1353/1500, Loss: 6.661857198225334\n",
      "Epoch 1354/1500, Loss: 6.620094228535891\n",
      "Epoch 1355/1500, Loss: 7.091015880228952\n",
      "Epoch 1356/1500, Loss: 6.92012666980736\n",
      "Epoch 1357/1500, Loss: 7.24917673273012\n",
      "Epoch 1358/1500, Loss: 7.143747201422229\n",
      "Epoch 1359/1500, Loss: 6.316754255443811\n",
      "Epoch 1360/1500, Loss: 6.953766220016405\n",
      "Epoch 1361/1500, Loss: 6.7620388781651855\n",
      "Epoch 1362/1500, Loss: 6.1409653960727155\n",
      "Epoch 1363/1500, Loss: 6.520812128903344\n",
      "Epoch 1364/1500, Loss: 6.0611508982256055\n",
      "Epoch 1365/1500, Loss: 6.019439057447016\n",
      "Epoch 1366/1500, Loss: 6.501828779699281\n",
      "Epoch 1367/1500, Loss: 6.06838160357438\n",
      "Epoch 1368/1500, Loss: 6.109118018066511\n",
      "Epoch 1369/1500, Loss: 6.512644710950553\n",
      "Epoch 1370/1500, Loss: 6.724407786503434\n",
      "Epoch 1371/1500, Loss: 7.512861827621236\n",
      "Epoch 1372/1500, Loss: 6.929143401328474\n",
      "Epoch 1373/1500, Loss: 7.047179718501866\n",
      "Epoch 1374/1500, Loss: 7.312170230783522\n",
      "Epoch 1375/1500, Loss: 8.10486571653746\n",
      "Epoch 1376/1500, Loss: 6.6567014718893915\n",
      "Epoch 1377/1500, Loss: 7.571801334852353\n",
      "Epoch 1378/1500, Loss: 6.604455241002142\n",
      "Epoch 1379/1500, Loss: 6.484390391968191\n",
      "Epoch 1380/1500, Loss: 6.228465903550386\n",
      "Epoch 1381/1500, Loss: 6.446202175342478\n",
      "Epoch 1382/1500, Loss: 6.701219344977289\n",
      "Epoch 1383/1500, Loss: 6.210461899871007\n",
      "Epoch 1384/1500, Loss: 6.9929380242247134\n",
      "Epoch 1385/1500, Loss: 6.99498199718073\n",
      "Epoch 1386/1500, Loss: 6.824308800976723\n",
      "Epoch 1387/1500, Loss: 6.65524560213089\n",
      "Epoch 1388/1500, Loss: 7.235945458058268\n",
      "Epoch 1389/1500, Loss: 6.654173834482208\n",
      "Epoch 1390/1500, Loss: 5.908103979192674\n",
      "Epoch 1391/1500, Loss: 6.500553113874048\n",
      "Epoch 1392/1500, Loss: 7.023303902475163\n",
      "Epoch 1393/1500, Loss: 6.722780339187011\n",
      "Epoch 1394/1500, Loss: 6.319887885823846\n",
      "Epoch 1395/1500, Loss: 6.665470265084878\n",
      "Epoch 1396/1500, Loss: 6.965355009073392\n",
      "Epoch 1397/1500, Loss: 6.603937865002081\n",
      "Epoch 1398/1500, Loss: 7.1992382577154785\n",
      "Epoch 1399/1500, Loss: 6.788104876410216\n",
      "Epoch 1400/1500, Loss: 7.433893681736663\n",
      "Epoch 1401/1500, Loss: 6.454876647330821\n",
      "Epoch 1402/1500, Loss: 7.8864709716290236\n",
      "Epoch 1403/1500, Loss: 7.3561104636173695\n",
      "Epoch 1404/1500, Loss: 6.873774766223505\n",
      "Epoch 1405/1500, Loss: 6.261597225675359\n",
      "Epoch 1406/1500, Loss: 7.115867591463029\n",
      "Epoch 1407/1500, Loss: 6.354550461983308\n",
      "Epoch 1408/1500, Loss: 6.4013973388355225\n",
      "Epoch 1409/1500, Loss: 7.363386457785964\n",
      "Epoch 1410/1500, Loss: 6.917126069543883\n",
      "Epoch 1411/1500, Loss: 6.90471218759194\n",
      "Epoch 1412/1500, Loss: 7.077539643505588\n",
      "Epoch 1413/1500, Loss: 6.331012368667871\n",
      "Epoch 1414/1500, Loss: 6.963606475619599\n",
      "Epoch 1415/1500, Loss: 6.659214386716485\n",
      "Epoch 1416/1500, Loss: 6.409986127633601\n",
      "Epoch 1417/1500, Loss: 6.752881878288463\n",
      "Epoch 1418/1500, Loss: 6.514104278059676\n",
      "Epoch 1419/1500, Loss: 6.41858986299485\n",
      "Epoch 1420/1500, Loss: 6.269636487122625\n",
      "Epoch 1421/1500, Loss: 315.91551186703146\n",
      "Epoch 1422/1500, Loss: 9.56414598505944\n",
      "Epoch 1423/1500, Loss: 7.897137549705803\n",
      "Epoch 1424/1500, Loss: 6.32645130995661\n",
      "Epoch 1425/1500, Loss: 6.029832911910489\n",
      "Epoch 1426/1500, Loss: 5.646427788306028\n",
      "Epoch 1427/1500, Loss: 5.894992615678348\n",
      "Epoch 1428/1500, Loss: 5.8552070705918595\n",
      "Epoch 1429/1500, Loss: 6.514167799148709\n",
      "Epoch 1430/1500, Loss: 6.078211800195277\n",
      "Epoch 1431/1500, Loss: 6.337288877228275\n",
      "Epoch 1432/1500, Loss: 6.090196268167347\n",
      "Epoch 1433/1500, Loss: 6.404844640986994\n",
      "Epoch 1434/1500, Loss: 5.943320540711284\n",
      "Epoch 1435/1500, Loss: 6.5062128668650985\n",
      "Epoch 1436/1500, Loss: 6.975374155910686\n",
      "Epoch 1437/1500, Loss: 6.719021260039881\n",
      "Epoch 1438/1500, Loss: 6.339957895223051\n",
      "Epoch 1439/1500, Loss: 6.1515409195562825\n",
      "Epoch 1440/1500, Loss: 6.184781806077808\n",
      "Epoch 1441/1500, Loss: 5.889451704220846\n",
      "Epoch 1442/1500, Loss: 6.007388345664367\n",
      "Epoch 1443/1500, Loss: 6.69109901599586\n",
      "Epoch 1444/1500, Loss: 6.3476275315042585\n",
      "Epoch 1445/1500, Loss: 6.133351306896657\n",
      "Epoch 1446/1500, Loss: 6.782287989277393\n",
      "Epoch 1447/1500, Loss: 7.131110273534432\n",
      "Epoch 1448/1500, Loss: 6.218698396347463\n",
      "Epoch 1449/1500, Loss: 6.4401050203014165\n",
      "Epoch 1450/1500, Loss: 7.994321386096999\n",
      "Epoch 1451/1500, Loss: 9.115215972531587\n",
      "Epoch 1452/1500, Loss: 7.125663660466671\n",
      "Epoch 1453/1500, Loss: 6.324861561879516\n",
      "Epoch 1454/1500, Loss: 6.447118813404813\n",
      "Epoch 1455/1500, Loss: 6.942205668892711\n",
      "Epoch 1456/1500, Loss: 6.435527432244271\n",
      "Epoch 1457/1500, Loss: 6.105704959016293\n",
      "Epoch 1458/1500, Loss: 5.952962084673345\n",
      "Epoch 1459/1500, Loss: 6.401442021364346\n",
      "Epoch 1460/1500, Loss: 6.217948050238192\n",
      "Epoch 1461/1500, Loss: 6.914762792410329\n",
      "Epoch 1462/1500, Loss: 7.2215874183457345\n",
      "Epoch 1463/1500, Loss: 6.99675841582939\n",
      "Epoch 1464/1500, Loss: 6.800593012711033\n",
      "Epoch 1465/1500, Loss: 6.442549071041867\n",
      "Epoch 1466/1500, Loss: 6.204230376286432\n",
      "Epoch 1467/1500, Loss: 6.242354811052792\n",
      "Epoch 1468/1500, Loss: 6.19192799506709\n",
      "Epoch 1469/1500, Loss: 6.231517966836691\n",
      "Epoch 1470/1500, Loss: 6.54784205975011\n",
      "Epoch 1471/1500, Loss: 6.3523375110235065\n",
      "Epoch 1472/1500, Loss: 6.6804624162614346\n",
      "Epoch 1473/1500, Loss: 7.113288598135114\n",
      "Epoch 1474/1500, Loss: 6.653430072590709\n",
      "Epoch 1475/1500, Loss: 6.166880278964527\n",
      "Epoch 1476/1500, Loss: 6.25526975421235\n",
      "Epoch 1477/1500, Loss: 6.660686936113052\n",
      "Epoch 1478/1500, Loss: 6.837480803253129\n",
      "Epoch 1479/1500, Loss: 6.4510380127467215\n",
      "Epoch 1480/1500, Loss: 6.31776767084375\n",
      "Epoch 1481/1500, Loss: 6.140196776250377\n",
      "Epoch 1482/1500, Loss: 6.8462044859770685\n",
      "Epoch 1483/1500, Loss: 6.540769714396447\n",
      "Epoch 1484/1500, Loss: 6.160871694097295\n",
      "Epoch 1485/1500, Loss: 6.84840834280476\n",
      "Epoch 1486/1500, Loss: 6.298400888219476\n",
      "Epoch 1487/1500, Loss: 5.9666789972689\n",
      "Epoch 1488/1500, Loss: 6.2759499995736405\n",
      "Epoch 1489/1500, Loss: 7.239327298244461\n",
      "Epoch 1490/1500, Loss: 7.303621314466\n",
      "Epoch 1491/1500, Loss: 6.545616498915479\n",
      "Epoch 1492/1500, Loss: 6.114287606673315\n",
      "Epoch 1493/1500, Loss: 6.9589928889181465\n",
      "Epoch 1494/1500, Loss: 6.669731519185007\n",
      "Epoch 1495/1500, Loss: 6.242234460543841\n",
      "Epoch 1496/1500, Loss: 6.550213138340041\n",
      "Epoch 1497/1500, Loss: 6.716074391966686\n",
      "Epoch 1498/1500, Loss: 6.584756683907472\n",
      "Epoch 1499/1500, Loss: 5.882576720323414\n",
      "Epoch 1500/1500, Loss: 6.393674159888178\n",
      "Test Score (R^2): 0.7743698322046302\n"
     ]
    }
   ],
   "source": [
    "#### CREATING MODEL ####\n",
    "\n",
    "# Define the neural network architecture with multiple hidden layers\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        layers = []\n",
    "        previous_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(previous_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            previous_dim = hidden_dim\n",
    "        layers.append(nn.Linear(previous_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.activation = nn.ReLU() #best according to grid search\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_data.shape[1]  # Number of molecular descriptors\n",
    "hidden_dim = [512,512,512]  # (512,512,512) Best according to grid search\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function (nn.MSELoss) and optimizer\n",
    "model = SolubilityPredictor(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "test_score = r2_score(y_test, y_pred)\n",
    "print(\"Test Score (R^2):\", test_score)\n",
    "\n",
    "#maybe make plots of training and validation loss to see how it is learning\n",
    "#can help identify overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 1.2239636182785034\n",
      "Test RMSE: 1.1063288450241089\n",
      "Test MAE: 0.7110916376113892\n",
      "Test R^2: 0.7743698322046302\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Perform the forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    test_predictions_tensor = model(X_test_tensor)\n",
    "\n",
    "# Convert predictions back to NumPy array\n",
    "test_predictions = test_predictions_tensor.numpy().flatten()\n",
    "\n",
    "# Calculate additional metrics\n",
    "test_mse = mean_squared_error(y_test, test_predictions)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, test_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With dropout, weight decay\n",
    "try dropout rates between 0.2 and 0.5. start with 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 681.7832328081131\n",
      "Epoch 101/1000, Loss: 111.76737007498741\n",
      "Epoch 201/1000, Loss: 74.74509523808956\n",
      "Epoch 301/1000, Loss: 64.90365134179592\n",
      "Epoch 401/1000, Loss: 56.54144813865423\n",
      "Epoch 501/1000, Loss: 55.897232234478\n",
      "Epoch 601/1000, Loss: 51.54749147221446\n",
      "Epoch 701/1000, Loss: 52.32152093201876\n",
      "Epoch 801/1000, Loss: 46.192381370812654\n",
      "Epoch 901/1000, Loss: 48.13287205994129\n",
      "Test MSE: 1.2193962335586548\n",
      "Test RMSE: 1.1042627096176147\n",
      "Test MAE: 0.695110559463501\n",
      "Test R^2: 0.7752118032844304\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network architecture with multiple hidden layers and dropout\n",
    "class SolubilityPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(SolubilityPredictor, self).__init__()\n",
    "        layers = []\n",
    "        previous_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(previous_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))  # Adding dropout\n",
    "            previous_dim = hidden_dim\n",
    "        layers.append(nn.Linear(previous_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_dim = X_data.shape[1]  # Number of molecular descriptors\n",
    "hidden_dims = [512, 512, 512]  # Best according to grid search\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function (nn.MSELoss) and optimizer with weight decay (L2 regularization)\n",
    "model = SolubilityPredictor(input_dim, hidden_dims, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy().flatten()\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test R^2: {test_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'solubility_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
